[{"id":"e311fa0f331a6c92d156d7ca54026af7","publish_timestamp":1614790813,"title":"Boosting the performance of virtual machines with Jump-Start","blogName":"Facebook","image":"https://engineering.fb.com/wp-content/uploads/2021/03/RiB_LightNavy.jpg","categories":["Developer Tools","Web"],"description":"What the research is: Jump-Start is a new approach for improving the performance of virtual machines at scale. Virtual machines are a modern and popular design to implement programming languages used to build applications in general, including large-scale websites like Facebook and Instagram. However, virtual machines incur well-known performance overhead in terms of the amount [...]\nRead More...\nThe post Boosting the performance of virtual machines with Jump-Start appeared first on Facebook Engineering.\n","publish_date":"2021-03-03 17:00:13","link":"https://engineering.fb.com/2021/03/03/developer-tools/hhvm-jump-start/","blog":{"id":"facebook","link":"https://code.facebook.com/posts/","name":"Facebook","rssFeed":"https://code.fb.com/feed","type":"company"},"blogType":"company"},{"id":"e633a4c4685421e07c2741d4b45e28eb","publish_timestamp":1614099611,"title":"Mitigating the effects of silent data corruption at scale","blogName":"Facebook","image":"https://engineering.fb.com/wp-content/uploads/2021/02/RiB_DarkBlue.jpg","categories":["Data Infrastructure","Research in Brief"],"description":"What the research is:  Silent data corruption, or data errors that go undetected by the larger system, is a widespread problem for large-scale infrastructure systems. This type of corruption can propagate across the stack and manifest as application-level problems. It can also result in data loss and require months to debug and resolve. This work [...]\nRead More...\nThe post Mitigating the effects of silent data corruption at scale appeared first on Facebook Engineering.\n","publish_date":"2021-02-23 17:00:11","link":"https://engineering.fb.com/2021/02/23/data-infrastructure/silent-data-corruption/","blog":{"id":"facebook","link":"https://code.facebook.com/posts/","name":"Facebook","rssFeed":"https://code.fb.com/feed","type":"company"},"blogType":"company"},{"id":"e94c8aec3a45c607dce5088d19f3b531","publish_timestamp":1614013238,"title":"FOQS: Scaling a distributed priority queue","blogName":"Facebook","image":"https://engineering.fb.com/wp-content/uploads/2021/02/CD21_047_ENGBlog_FOQS_HERO_FINAL.png","categories":["Production Engineering","Scale","Facebook Ordered Queueing Service","FOQS"],"description":"We will be hosting a talk about our work on Scaling a Distributed Priority Queue during our virtual Systems @Scale event at 11 am PT on Wednesday, February 24, followed by a live Q&#38;A session. Please submit any questions to systemsatscale@fb.com before the event. The entire Facebook ecosystem is powered by thousands of distributed systems [...]\nRead More...\nThe post FOQS: Scaling a distributed priority queue appeared first on Facebook Engineering.\n","publish_date":"2021-02-22 17:00:38","link":"https://engineering.fb.com/2021/02/22/production-engineering/foqs-scaling-a-distributed-priority-queue/","blog":{"id":"facebook","link":"https://code.facebook.com/posts/","name":"Facebook","rssFeed":"https://code.fb.com/feed","type":"company"},"blogType":"company"},{"id":"78f6c718e59091ba1fc738a8f676977c","publish_timestamp":1613581164,"title":"Faster, more efficient systems for finding and fixing regressions","blogName":"Facebook","image":"https://engineering.fb.com/wp-content/uploads/2021/02/FixFast_HERO_FINAL.jpg","categories":["Developer Tools"],"description":"Every workday, Facebook engineers commit thousands of diffs (which is a change consisting of one or more files) into production. This code velocity allows us to rapidly ship new features, deliver bug fixes and optimizations, and run experiments. However, a natural downside to moving quickly in any industry is the risk of inadvertently causing regressions [...]\nRead More...\nThe post Faster, more efficient systems for finding and fixing regressions appeared first on Facebook Engineering.\n","publish_date":"2021-02-17 16:59:24","link":"https://engineering.fb.com/2021/02/17/developer-tools/fix-fast/","blog":{"id":"facebook","link":"https://code.facebook.com/posts/","name":"Facebook","rssFeed":"https://code.fb.com/feed","type":"company"},"blogType":"company"},{"id":"68f3d5f8568bedf0ee55c8d4ffa81319","publish_timestamp":1612890006,"title":"Minesweeper automates root cause analysis as a first-line defense against bugs","blogName":"Facebook","image":"https://engineering.fb.com/wp-content/uploads/2021/02/Minesweeper-Hero.png","categories":["Developer Tools","Networking  Traffic","Production Engineering","Security","Uncategorized"],"description":"Root cause analysis (RCA) is an important part of fixing any bug. After all, you can’t solve a problem without getting to the heart of it. But RCA isn’t always simple, especially at a scale like Facebook’s. When billions of people are using an app on a variety of platforms and devices, a single bug [...]\nRead More...\nThe post Minesweeper automates root cause analysis as a first-line defense against bugs appeared first on Facebook Engineering.\n","publish_date":"2021-02-09 17:00:06","link":"https://engineering.fb.com/2021/02/09/developer-tools/minesweeper/","blog":{"id":"facebook","link":"https://code.facebook.com/posts/","name":"Facebook","rssFeed":"https://code.fb.com/feed","type":"company"},"blogType":"company"},{"id":"1e2acd8068ed390fb0084f3b3435a54f","publish_timestamp":1612544368,"title":"Open-sourcing Thrift for Haskell","blogName":"Facebook","image":"https://engineering.fb.com/wp-content/uploads/2020/10/OSiB_RichTeal.jpg","categories":["Developer Tools","Open Source","Open Source in Brief"],"description":"What it is: Thrift is a serialization and remote procedure call (RPC) framework used for cross-service communication. Most services at Facebook communicate via Thrift because it provides a simple, language-agnostic protocol for communicating with structured data. Thrift can already be used in programming languages such as C++, Python, and Java using fbthrift. We are also [...]\nRead More...\nThe post Open-sourcing Thrift for Haskell appeared first on Facebook Engineering.\n","publish_date":"2021-02-05 16:59:28","link":"https://engineering.fb.com/2021/02/05/open-source/hsthrift/","blog":{"id":"facebook","link":"https://code.facebook.com/posts/","name":"Facebook","rssFeed":"https://code.fb.com/feed","type":"company"},"blogType":"company"},{"id":"df258fb804adf4ec7ffaf18bf332ebe7","publish_timestamp":1611680411,"title":"How machine learning powers Facebook’s News Feed ranking algorithm","blogName":"Facebook","image":"https://engineering.fb.com/wp-content/uploads/2021/01/Under-the-Hood_Hero_1920x1080_Final.jpg","categories":["Core Data","ML Applications"],"description":"Designing a personalized ranking system for more than 2 billion people (all with different interests) and a plethora of content to select from presents significant, complex challenges. This is something we tackle every day with News Feed ranking. Without machine learning (ML), people’s News Feeds could be flooded with content they don’t find as relevant [...]\nRead More...\nThe post How machine learning powers Facebook’s News Feed ranking algorithm appeared first on Facebook Engineering.\n","publish_date":"2021-01-26 17:00:11","link":"https://engineering.fb.com/2021/01/26/ml-applications/news-feed-ranking/","blog":{"id":"facebook","link":"https://code.facebook.com/posts/","name":"Facebook","rssFeed":"https://code.fb.com/feed","type":"company"},"blogType":"company"},{"id":"94efc09e4f37e9de90cf66ecc4735a7d","publish_timestamp":1609347618,"title":"2020 year in review: Connectivity innovations, faster apps, and progress toward net zero","blogName":"Facebook","image":"https://engineering.fb.com/wp-content/uploads/2020/12/CD20_796_ENGBlog_YearInReview_hero_FINAL.jpg","categories":["Connectivity","Data Center Engineering","Data Infrastructure","Developer Tools","Networking  Traffic","Open Source","Production Engineering"],"description":"It goes without saying that 2020 has been a challenging year, to put it lightly. But if anything, the COVID-19 pandemic has shined a light on our need to connect as people. For Facebook, that meant our work has become more important than ever. Whether it was finding new and innovative ways to expand internet [...]\nRead More...\nThe post 2020 year in review: Connectivity innovations, faster apps, and progress toward net zero appeared first on Facebook Engineering.\n","publish_date":"2020-12-30 17:00:18","link":"https://engineering.fb.com/2020/12/30/connectivity/2020-year-in-review/","blog":{"id":"facebook","link":"https://code.facebook.com/posts/","name":"Facebook","rssFeed":"https://code.fb.com/feed","type":"company"},"blogType":"company"},{"id":"6e59715d12983e2d8307b231303ecf57","publish_timestamp":1608570050,"title":"A smaller, faster video calling library for our apps","blogName":"Facebook","image":"https://engineering.fb.com/wp-content/uploads/2020/12/Rsys2.jpg","categories":["Video Engineering"],"description":"We are rolling out a new video calling library to all the relevant products across our apps and services, including Instagram, Messenger, Portal, Workplace chat, etc.  To create a library generic enough to support all these different use cases, we needed to rewrite our existing library from scratch using the latest version of the open [...]\nRead More...\nThe post A smaller, faster video calling library for our apps appeared first on Facebook Engineering.\n","publish_date":"2020-12-21 17:00:50","link":"https://engineering.fb.com/2020/12/21/video-engineering/rsys/","blog":{"id":"facebook","link":"https://code.facebook.com/posts/","name":"Facebook","rssFeed":"https://code.fb.com/feed","type":"company"},"blogType":"company"},{"id":"49eb2638245fdfc2acf5a9b118071df6","publish_timestamp":1615594083,"title":"Production Media Management: Transforming Media Workflows by leveraging the Cloud","blogName":"Netflix","image":"https://miro.medium.com/max/1200/1*VFyxg0H0Enodd9rvXGOJ5Q.png","categories":["movieproduction","mediaworkflows","contenthub","mediamanagement","productiontechnology"],"description":"Written by Anton Margoline, Avinash Dathathri, Devang Shah and Murthy Parthasarathi. Credit to Netflix Studio’s Product, Design, Content Hub Engineering teams along with all of the supporting partner and platform teams.In this post, we will share a behind-the-scenes look at how Netflix delivers technology and infrastructure to help production crews create and exchange media during production and post production stages. We’ll also cover how our Studio Engineering efforts are helping Netflix productions to spend less time on media logistics by utilizing our cloud based services.Lights, Camera, Media! Productions take on media managementIn a typical live action production, after media is offloaded from the camera and sound recorders on set, it is operated on as files on disk using various tools between departments, like Editorial, Sound and Music, Visual Effects (VFX), Picture Finishing and teams at Netflix. Increasingly, the teams are globally distributed, and each stage of the process generates many terabytes of data.Media exchanges between different departments constitute a media workflow, and no two productions share the same workflow, known in the industry by the term ‘snowflake workflow’. The stories demand different technical approaches to production, which is why a media workflow for a multi-camera show with visual effects such as Stranger Things, has a different workflow to Formula 1: Drive to Survive with an extensive amount of footage.Media workflows are always evolving and adapting; driven by changes in production technology (new cameras and formats), post production technology (tools used by Sound, Music, VFX, and Picture Finishing) and consumer technology (adoption of 4K, HDR, and Atmos). It would be impossible to describe all of the complexities and the history of the industry in a single post. For a more comprehensive overview, please refer to Scott Arundale and Tashi Trieu’s book, Modern Post: Workflows and Techniques for Digital Filmmakers.Workflows are louder than words. Technology empowering Netflix productions today!Now that we understand what media workflows are, let’s take a look at some of the workflows we’ve enabled.Collect Camera Media (On-Set/Near-Set)We enable camera and sound media imports via our partner API integrations or via Netflix media import UIs. Along with the files, metadata plays an important role in downstream workflows, so we make significant efforts to categorize all media into respective assets with the help of the metadata we collect from our partner API integrations as well as our internal video inspection services. Media Workflows:Content Hub (Netflix UI) Import: Imports footage media, which is inspected and, with the help of the metadata, categorized into assets.Partner API Import: We provide external APIs for our partners to exchange media files and metadata to and from the cloud. We have pilot integrations with media management tools including Colorfront’s Express Dailies, Light Iron and Fotokem’s Nextlab and we’re looking to extend this in the future.Iterate on a Movie Timeline (Editorial)We enable Editorial workflows to drive media interchange between Editorial and VFX, Sound &amp; Music, Picture Finishing facility and Netflix. Most of the workflows start with an Editor providing an edit decision list timeline with a playable reference (.mov file). Depending on the type of the workflow, this timeline can be shared as is, or transformed into alternative formats required by the tools used in other areas of production. Media Workflows:VFX Plate Generation &amp; Delivery: Editorial turns over an edit decision list timeline which is processed into media references and either matched to already uploaded VFX Plates (ACES EXR images + other files) or, if the plates are not available, they are transcoded from the raw camera media. At the end of the workflow, the VFX facility receives VFX Plates as a downloadable folder.Conform Pull: Editorial shares an edit decision list timeline, which upon processing is turned over to the Picture Finishing facility as a downloadable folder with original camera media trimmed to the parts used in the timeline.Studio Archival (Cut Turnover): As production iterates on the timeline, versions of the timeline (cuts) are shared (turned over) with other areas of production so they can begin their work. Major versions are known as “Locked Cuts”. Utilizing this workflow, an Editor uploads the aforementioned timeline with its related files. The media is transcoded onto different formats and, as required and permitted, shared with other departments downstream, such as dubbing, marketing or PR.Produce Visual Effects (VFX)We enable VFX via several media workflows, starting from the initial request from an Editorial department to facilitate the visual effects work, iterating on the produced VFX shots using Media Review workflows, delivering back the finished product by VFX Shot Delivery and, at the very end, archiving everything for safekeeping. Media Workflows:Media Review: VFX Shot delivery and review workflow, used by Editorial, show-side VFX and Netflix. Both Netflix and our productions frequently rely on 3rd party software to manage their VFX assets, which is why this workflow leverages integrations to sync media and metadata.VFX Shot Delivery: VFX Shots are delivered from VFX to Picture Finishing facility.Studio Archival (most VFX media): Shots and other media used to produce visual effects are delivered and archived for safekeeping.Picture Finishing (Picture Finishing Facility)We enable Picture Finishing facilities to get all of the ingredients needed to do the conform, where all media used in the timeline is verified and made available for color grading. If the facility also helps with media management on a given production, we have workflows where the Picture Finishing facility would manage VFX Plate delivery to the VFX facility. Media workflows: VFX Plate Delivery: Provides means to procure VFX Plates (ACES EXR images + other files) used by VFX in the process of creating visual effects.Sound, MusicWe enable Editorial to share their versions of the timeline (cuts) in the form of playable timeline references (.mov files) with Sound/Music. We also enable Sound/Music to deliver their final products as Stems and Mixes so they can be used further into the production cycle, such as for mixing, dubbing and safekeeping. Media Workflows: Studio Archival and its variants.Localization, Marketing/PR, Streaming (Netflix)We enable our production partners to deliver media from many different aspects of production, some of which are mentioned in the areas above, with many more. In addition to safekeeping the media, Studio Archival media workflows empower media used during production for Marketing, PR and other workflows. Media Workflows: Studio Archival and its variants.The magic is in the details. VFX Plate Generation &amp; Delivery workflowLets dive deeper into VFX Plate Generation &amp; Delivery media workflow to demonstrate the steps required within this media exchange. While describing the details we’ll use the opportunity to refer to how our technology infrastructure enables this workflow among many others.The VFX Plate Generation &amp; Delivery workflow is a process by which an Editor provides the necessary media to a Visual Effects team, with metadata and raw ingredients necessary to begin their work. This workflow is enabled by camera media workflows, which would have been done earlier to make the camera media and its metadata available.The VFX Plate Generation &amp; Delivery workflow is started by an Editorial team with an edit decision list timeline file (.edl, .xml) exported from a Non Linear Editing tool. This timeline file contains only the references to media with additional information about time, color, markers and more, but not any of the actual media files. In addition to the timeline, the Editor chooses whether they would want the resulting media to be rescaled to UHD and how many extra frames they would like to have added for each event referenced in the timeline.After processing the timeline file, each individual media reference is extracted with relevant timecode, media reference, color decisions and markers. To support different editorial tools, each having its own edit decision list timeline format, our Video Encoding platform interprets the timeline into a standardized interchange format called OpenTimelineIO.Media reference, color decisions and markers are linked with the original camera media and transcoded from raw camera formats onto ACES EXR. Most Visual Effects tools are not able to process raw camera files directly. Along with image media, color metadata is extracted from the timeline to generate Color Decision List files (.cdl, .xml) which are used to communicate color decisions made by an Editor. All of the media transformations and metadata are then persisted as VFX Plate assets.The Editor then reviews VFX Plate Generation &amp; Delivery details, with all of the timeline events clearly identified and any inconsistencies spotted, such as if raw camera media is not found or there are any challenges with transcoding media. If all looks good, an Editor is able to submit this workflow onto the final step where results are packaged and shared with the Visual Effects team.To share results with Visual Effects artists, we’re transforming all of the VFX Plate assets and media created earlier and sharing with the recipients, who can either download the files via browser, or use our auto-downloader tools for additional convenience. Concluding this workflow is an email, sharing all the relevant information with the Editorial and VFX teams.We’re leveraging the VFX Plate Generation &amp; Delivery workflow (among others) on shows including the next installments of our amazing series like Money Heist, Selena and others. We’re excited to help even more productions this year, as we’re continuing to build support for more use cases and polish the experiences.Walk, before you run. Scalable components powering Media WorkflowsLet’s now zoom out and take a look at the foundation that supports the 20+ unique media workflows that we’ve enabled in the last two years, with more being added at an accelerating rate.No single monolithic service would scale to support the various demands of this platform. Many teams at Netflix contribute to the success of Media Workflows Platform, by providing the foundations we rely on for many of the steps taken.Media Workflows Platform (also known as “Content Hub”): a component that powers all of our media workflows. At a very high level it is composed of the Platform, UI and Partner APIs.UI + GraphQL Services: facilitating various media workflows, one use case at a time, built with the help of the recently open sourced domain graph service implementation for the federated GraphQL environment.Partner APIs: external partner APIs enabling integrations into Netflix media workflows.Media Workflows Platform: a flexible platform that enables diverse, scalable, easy to customize production media workflows, built on the foundational tenets:— Resource Management to associate files, assets and other workflows — Robust Execution Engine execution engine, powered by Conductor — State Machine defining user and system interaction — Reusable Steps enabling component reuse across different workflowsMedia Inspection and Encoding: scalable media services that are able to handle various media types, including raw camera media. Use cases range from gathering metadata to transforming (change format) or trans-wrapping (trim media).Universal Asset Management: all media with its metadata maintained in a common asset management system enabling a common framework for consuming media assets in a microservice environment.Global Storage: global, fault tolerant storage solution that supports file-based workflows in the cloud.Data Science Platform: all of the layers feed into the data science platform, enabling insights to help us iterate on improving our services using data-driven metrics.Netflix Platform Tools: paved path services provided in building, deploying and orchestrating our services together.Take me home. We ❤️ empowering Media Workflows, do you?We’ve helped productions manage and exchange many petabytes of media which is only accelerating with more usage of the platform. Some of our recent workflows in Editorial are in pilot on a handful of productions, our VFX workflows helped dozens of shows, Media Review assisted hundreds of shows and, our Archival workflows are used on all of our shows. While we’ve innovated on many workflows, we’re continuing to add support for more workflows and are refining existing ones to be more helpful. Our media workflows platform is a robust, scalable and easy to customize solution that helps us create great content!Thanks for getting this far! If you are just learning about how production media management works, we hope this sparks an interest in our problem space. If you are designing tools to empower media workflows, we hope that by sharing our challenges and approaches to solving them, we can all learn from each other. Realizing common challenges inspires more openness and the standardization we crave. We’re really excited to see the proliferation of open APIs and industry standards for media transformation and interchange such as OpenTimelineIO, OpenColorIO, ACES and more.If you’re passionate about building production media workflows, or any of the foundational services, we’re always looking for talented engineers. Please check out our job listings for Studio Engineering, Production Media Engineering, Product Management, Content Engineering, Data Science Engineering and many more.Production Media Management: Transforming Media Workflows by leveraging the Cloud was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-03-13 00:08:03","link":"https://netflixtechblog.com/production-media-management-transforming-media-workflows-by-leveraging-the-cloud-1174699e4a08?source=rss----2615bd06b42e---4","blog":{"id":"netflix","link":"https://netflixtechblog.com","name":"Netflix","rssFeed":"https://netflixtechblog.com/feed","type":"company"},"blogType":"company"},{"id":"ddb8f57e8b9f122157c8678bf0dea79c","publish_timestamp":1615391496,"title":"ConsoleMe: A Central Control Plane for AWS Permissions and Access","blogName":"Netflix","image":"https://miro.medium.com/max/800/1*eaZjefJHEd4XcwO7I-0eCg.png","categories":["identityandaccess","aws","cloudsecurity","ami","permission"],"description":"ConsoleMe: A Central Control Plane for AWS Permissions and AccessBy Curtis Castrapel, Patrick Sanders, and Hee Won KimAt AWS re:Invent 2020, we open sourced two new tools for managing multi-account AWS permissions and access. We’re very excited to bring you ConsoleMe (pronounced: kuhn-soul-mee), and its CLI utility, Weep (pun intended)!If you missed the talk, check it out here.MotivationGrowth in the cloud has exploded, and it is now easier than ever to create infrastructure on the fly. Groups beyond software engineering teams are standing up their own systems and automation. This is an amazing movement providing numerous opportunities for product innovation, but managing this growth has introduced a support burden of ensuring proper security authentication &amp; authorization, cloud hygiene, and scalable processes.At many companies, managing cloud hygiene and security usually falls under the infrastructure or security teams. They are the one-stop-shop for cloud permissions and access. As the company scales, this centralized and manual management approach falls over, becoming impractical for both operations teams and their users.This happened for us at Netflix. Our Cloud Infrastructure Security team is the arbiter of AWS permissions, handling numerous requests from Netflix employees for cloud permissions and access. Our historical approach of helping Netflix internal cloud users looks something like this:A user messages us in our support channelWe clarify what the user needs, and why. Risks are analyzed, and we make suggestions of better approaches if applicable.We hand-craft an IAM policy for the end-userWe log into the AWS account with the applicable IAM role and manipulate the policyIf the request involves a cross-account resource, we log out of the AWS account, log in to the account with the resource, and manipulate the resource policyWe ask the user to testThe user comes back with an additional permissions errorWe play a game of permission whack-a-mole (Steps 3–7) until we resolve all of the user’s issuesWe repeat this multiple times a day with different users.This process is manual, time-consuming, inconsistent, and often a game of trial and error.Permission whack-a-moleAt Netflix, we’re firm believers in empowering our employees and providing low-friction systems that allow users to get their jobs done in a safe way. By integrating best practices such as least privilege into an IAM pipeline, we transitioned the security team from being gatekeepers of the cloud into cloud development accelerators.What is ConsoleMe?ConsoleMe is a self-service tool for AWS that provides an easier way of managing permissions and access across multiple accounts, while encouraging least-privilege permissions. Users can use the following features:Access the AWS consoleRetrieve and utilize short-lived AWS credentials through WeepRequest IAM permissions through a self-service wizardUtilize ConsoleMe’s native policy editors for more advanced requestsQuickly locate and navigate to AWS resources within an organizationIn addition, cloud administrators can use ConsoleMe to:Manage IAM and resource policies without logging in to the AWS ConsoleCreate or clone IAM roles across accountsCheck out the demos in our documentation, give ConsoleMe a test ride by logging in to our demo site (Requires a Google account), then try it locally with your own account.Access the AWS console(docs, talk, demo)ConsoleMe allows users to access the AWS console through the use of temporary IAM role credentials. After the user authenticates, ConsoleMe determines which roles they’re authorized to access based on their identity and group memberships.ConsoleMe generates an authorization mapping that is used to determine which users/groups are allowed to access a given IAM role. This mapping can be generated through role tags that indicate which users/groups are allowed to retrieve credentials for the role, ConsoleMe’s Dynamic Configuration, or through an organization’s custom logic. (docs).Users have a number of ways they can log in to the AWS console. The simplest way is by browsing to ConsoleMe and clicking on the desired role via the web interface. They can also use URL parameters to log into a particular role, access a specific region, AWS service, or AWS resource without having to navigate around ConsoleMe’s web interface.At Netflix, we’ve seen users integrate ConsoleMe with productivity tools like Alfred, chat bots, and custom browser search engines.Retrieve and serve short-lived AWS credentials through Weep(docs, talk)Weep is ConsoleMe’s CLI utility. It retrieves temporary (1-hour) AWS credentials from ConsoleMe, and offers a number of different ways to serve them locally. Weep can automatically refresh credentials. This ensures that long-lived AWS actions are successful (Like an s3:GetObject action taking longer than an hour). Weep can also transparently perform nested AssumeRole calls, and serve the assumed role credentials to the local user. Credentials are discoverable by the AWS CLI and AWS SDKs through the default credential provider chain.Weep supports the following methods of serving credentials:Write credentials to a user’s ~/.aws/credentials fileExport credentials as environment variablesEmulate the EC2 instance metadata proxyEmulate the ECS credential providerGenerate and provide credential_process commands to source credentialsWeep service credentials in ECS credential provider modeRequest IAM permissions through a self-service wizard(docs, talk, demo)ConsoleMe provides a step-by-step self-service wizard to help users request AWS IAM permissions.Users no longer need to worry about the IAM JSON permissions syntax. They can simply search for their role and choose the permissions they need. ConsoleMe will generate an IAM policy and, if required, cross-account resource policies that are applicable to the request. Users can modify the generated policy if they desire, and then submit for approval.Low-risk permission requests can be automatically approved.ConsoleMe’s configurable self-service wizard offers the following features:Fully configurable based on an organization’s most common requestsTypeaheads against all known AWS permissions and resource ARNs across an organizationAutomatic approval of low-risk permission requests, governed by ConsoleMe’s configuration and powered by Zelkovahttps://medium.com/media/aec6ee4564c075904e9030cfda5fa629/hrefConsoleMe’s self-service wizard has reduced our response time in servicing access requests, provided more consistency in our IAM policies, and simplified AWS permissions for our users.Utilize ConsoleMe’s native policy editors for advanced requests(docs, talk, demo)ConsoleMe offers a native policy editor for popular resource types. Administrators use it to manage permissions and tags for common resource types. End-users can manipulate a resource and submit policy change requests.The policy editor offers the following features:Cloud administrators can manage resource policies and tags directlyEnd-users can manipulate policies and tags, then submit changes for approvalCode editors provide typeaheads for AWS permissions and known AWS resourcesPolicy templates make it easy to generate new inline policies consistentlyUsers can view recent CloudTrail errors for a given resourceConsoleMe’s policy editor showing a resource typeahead dropdownToday, ConsoleMe supports a small number of popular resource types. We’d love your help with adding support for new resource types. Reach out to us on Discord or better yet, create an issue or submit a pull request on GitHub.Quickly locate and navigate to AWS resources within an organization(docs, talk, demo)ConsoleMe provides a centralized, filterable view of your most critical cloud resources, synchronized from AWS Config. It allows users to quickly find an AWS resource across all of the accounts within an organization.For resource types that ConsoleMe doesn’t have native policy editors for, ConsoleMe provides a link that will both log users into the AWS console and redirect them to the appropriate resource.Navigating to a DynamoDB table from ConsoleMe’s Policies viewCreate or clone IAM roles across accounts(docs, demo)ConsoleMe makes it easy for cloud administrators to create or clone new IAM roles across multiple AWS accounts. We created this feature because we found ourselves in the AWS Console copying and pasting various policies by hand.The clone feature can copy one or more of the following to a new role:IAM role Trust Policies (Assume Role Policy Document)DescriptionInline PoliciesManaged PoliciesTagsConsoleMe’s Role Creation InterfaceHow does ConsoleMe encourage least-privilege permissions?At Netflix, we use IAM roles instead of IAM users because roles do not allow long-lived, static credentials. IAM user credentials are more vulnerable to accidental exposure, difficult to rotate, and generally harder to secure.In addition, we prefer using inline policies instead of managed policies for our IAM roles because it’s easier to enforce least-privilege as inline policies are specific to an IAM role while managed policies can be attached to multiple roles. It’s hard to remove permissions from shared managed policies because some roles may be using permissions from the policy that other roles are not.We use ConsoleMe in conjunction with RepoKid to remove unused permissions, and then to make the process of requesting them back as painless as possible.Getting StartedConsoleMe is available on GitHub (Give us a ★!). You can try out ConsoleMe using Docker. A quick start guide is available in our documentation.ConsoleMe has example Terraform files that you can reference when you’re ready to deploy.ContributingConsoleMe still has a long way to go, and we could use your help. ConsoleMe and Weep work great for us here at Netflix, and we want them to work great for everyone else too. The best way to get started is to read through the documentation and code, install ConsoleMe, and take a look at our open issues to see what work needs to be done, or submit issues yourself.Not a coder or an IAM expert? No problem. We have a lot of documentation that could use proofreading and clarifying to make it more approachable.For more information on how you can get involved, check out our Contributing guide.Also, we’re hiring! If you’re interested in these sorts of problems, take a look at https://jobs.netflix.com/teams/security, and apply.What’s next?Over the last couple of years, we’ve battle tested ConsoleMe and have added features to scale it with our needs at Netflix. We’ve now brought ConsoleMe out in the open. As companies adopt ConsoleMe, we want to continue growing it to address the unique challenges of large-scale cloud permissions management that many of us face.We have a lot of plans for the future of ConsoleMe. Many of these goals are ambitious, and we can’t do it without your support. If any of these excite you, please reach out to us on our Discord channel or submit feature enhancements on GitHub.Some of the ideas we have in mind are:Easier Permissions DebuggingAWS permissions can be hard to debug with opaque Access Denied errors. We aim to simplify and automate the debugging process. This might include exposing and connecting information from the following sources:CloudTrail logsService Control PoliciesResource policiesPermission boundariesSession policiesInline PoliciesManaged PoliciesIdeally, users would be able to ask ConsoleMe whether an IAM role can take a specific action on a given resource. If not, ConsoleMe would provide an explanation and context about any policies that are preventing the action.Support for Team RolesWe plan to add features supporting the creation and management of team roles. Team roles are IAM roles that an entire team has access to. These roles can be propagated across multiple accounts, and can have differing permissions on each account. A simplified management interface will make it easy to create, request, or modify a team role.Enhanced Cross-Account Policy GenerationConsoleMe only supports cross-account policy generation for a subset of resource types. We hope to expand this in the future and make generated policies as accurate as possible by adding awareness of permission boundaries and service control policies.Decentralized Policy Request managementCloud administrators should have the option to no longer manage and review all policy requests. If ConsoleMe has context on the owner of a resource, and is able to determine that the policy is within a set of defined safety limits, policy requests should be routed to the owners of the resources affected by the policy.Policy RollbackOn occasion, we need to rollback policy changes that either break an IAM role or prevent new functionality from working. ConsoleMe should allow users to revert a role to an older snapshot.Multi-Cloud SupportCentrally manage access and permissions across all of your clouds.Where can I learn more?Here are some helpful resources:ConsoleMe source code on GitHubWeep source code on GitHubConsoleMe Demo SiteDocumentation for ConsoleMe and Weep, including our Contributing guideChat with us on DiscordJoin our team!Special ThanksWe would like to give a special thanks to Srinath Kuruvadi, Jay Dhulia, the Cloud Infrastructure Security Team at Netflix, the Infosec team at Netflix, and our AWS partners.We’d also like to thank our contributors for both ConsoleMe and Weep.ConsoleMe: A Central Control Plane for AWS Permissions and Access was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-03-10 15:51:36","link":"https://netflixtechblog.com/consoleme-a-central-control-plane-for-aws-permissions-and-access-fd09afdd60a8?source=rss----2615bd06b42e---4","blog":{"id":"netflix","link":"https://netflixtechblog.com","name":"Netflix","rssFeed":"https://netflixtechblog.com/feed","type":"company"},"blogType":"company"},{"id":"2a2fc8ac9d0ff7f41070966f13ad9f76","publish_timestamp":1615221086,"title":"Remote Workstations for the Discerning Artists","blogName":"Netflix","image":"https://miro.medium.com/max/720/0*bUqd-hR1OV21ISl5","categories":["remoteworking","spinnaker","infrastructure","saltstack"],"description":"By Michelle BrennerNetflix is poised to become the world’s most prolific producer of visual effects and original animated content. To meet that demand, we need to attract the world’s best artistic talent. Artists like to work at places where they can create groundbreaking entertainment instead of worrying about getting access to the software or source files they need. To meet this need, the Studio Infrastructure team has created Netflix Workstations.Netflix Workstations are remote workstations that allow content creators to get to work wherever they are. As an engineer, I can work anywhere with a standard laptop as long as I have an IDE and access to Stack Overflow. However, the artists creating stunning visual effects and animations for Netflix Originals need more than that. They need specialized hardware, access to petabytes of images, and digital content creation applications with controlled licenses.Historically artists had these machines built for them at their desks and only had access to the data and applications when they were in the office. With global demand for talent skyrocketing, we want the flexibility to hire anyone, anywhere. One of our first partners for the Netflix Workstations is NetFX, a cloud-based VFX platform that enables artists and creators worldwide to collaborate on Netflix VFX content.Now that you know why, here is how we did it. Below is a broad technical overview of how to go from an AWS instance to a Netflix Workstation.Machine Configuration: SpinnakerStarting at the left of the chart, Spinnaker is an open-source platform that controls the creation of workstation pools. Spinnaker uses “pipelines” as instructions for creating the pools. An API in conjunction with variables in the pipeline creates Workstation pools programmatically. Artists need many components to be customized. They could need a GPU when doing graphics-intensive work or extra large storage to handle file management. Some artists needed Centos 7 to support their compositing software, while others required Windows to use their pre-visualization software. To minimize lag, the workstations need to be as close to the artist as possible, so we support a growing list of regions and zones.Initially, we created big pools of workstations that only had the OS and a few internal tools. When the artist requested a workstation, all software was installed just-in-time. That led to long wait times and unhappy artists. Most artists were requesting a handful of standard configurations and did not need maximum flexibility. Instead, we created a service to take the most popular configurations and cache them. Now, artists can get a new workstation in seconds.Software Configuration: SaltToday, there are ~100 different packages that can configure a workstation, from installing software to editing a registry. How did we get here? We needed a system that could manage hundreds to one-day thousands of workstations. It needed to be extremely flexible while easy to jump in and create new packages. That is where SaltStack comes in. We use Salt to make operating system agnostic declarative statements about how to configure a workstation. It has many built-in modules, from installing a package to editing the registry. It also allows for logic statements to handle situations such as mount this storage in this environment only or only run this script if this file does not exist.This salt formula example is the equivalent of running a “yum install sis-lighting-10_1” in the terminal.centos7_lighting_install:    pkg.installed:        - name: sis-lighting-10_1Artist ExperienceWe want the artists to be able to start their workday quickly. They simply select a previously created configuration, wait a few seconds to prepare, and jump right in via their browser.While this is the quickest way to get started, there are a few remote display options. The artist can use the browser to see the desktop, as shown, or application streaming. Application streaming simplifies the experience to the single artist tool they need. They could also skip the browser and use a native client on their desktop.As with any new technology, the experience is not always bug-free. With our front-line support teams’ help, we are responsible for monitoring and quickly fixing any artists’ issues. We rely on our internal partner teams to support components installed on the workstation, such as storage and artist tools. They depend on us to provide observability into a workstation. Part of that is being able to track a workstation’s lifecycle.A gRPC Java Spring Boot control plane and a Golang agent manages and reports on the lifecycle. The lifecycle has many steps, but they fall into three main categories: before login, the artist is working, and the artist is done. Before login, we use Spinnaker &amp; Salt to configure and have free rein to make all necessary changes. While the artist uses the workstation, we track the health but avoid making changes that could disrupt their work. We recommend that artists get new workstations frequently to have the latest updates, but we can use Salt to deploy quick fixes if necessary.Looking AheadWe’ve made it easier for artists to create content remotely. However, we are only at the beginning of creating on-demand, secure, self-service remote workstations. We are looking to the future where Netflix Workstations are a platform for technical artists to make their own configurations. Where we can gather and analyze the usage data to create efficiencies and automation. Where an artist anywhere in the world can focus on their art and not on their commute. There is more work to be done, and if you want to be a part of it, we are growing!Remote Workstations for the Discerning Artists was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-03-08 16:31:26","link":"https://netflixtechblog.com/remote-workstations-for-the-discerning-artists-8155a8fbd190?source=rss----2615bd06b42e---4","blog":{"id":"netflix","link":"https://netflixtechblog.com","name":"Netflix","rssFeed":"https://netflixtechblog.com/feed","type":"company"},"blogType":"company"},{"id":"6bae3dd44676e3b3e81e75ca8294b663","publish_timestamp":1614732343,"title":"A Day in the Life of an Experimentation and Causal Inference Scientist @ Netflix","blogName":"Netflix","image":"https://miro.medium.com/max/1200/1*5lyavS59mazOFnb55Z6znQ.png","categories":["abtesting","experimentation","causalinference","datascience"],"description":"Stephanie Lane, Wenjing Zheng, Mihir TendulkarSource credit: NetflixWithin the rapid expansion of data-related roles in the last decade, the title Data Scientist has emerged as an umbrella term for myriad skills and areas of business focus. What does this title mean within a given company, or even within a given industry? It can be hard to know from the outside. At Netflix, our data scientists span many areas of technical specialization, including experimentation, causal inference, machine learning, NLP, modeling, and optimization. Together with data analytics and data engineering, we comprise the larger, centralized Data Science and Engineering group.Learning through data is in Netflix’s DNA. Our quasi-experimentation helps us constantly improve our streaming experience, giving our members fewer buffers and ever better video quality. We use A/B tests to introduce new product features, such as our daily Top 10 row that help our members discover their next favorite show. Our experimentation and causal inference focused data scientists help shape business decisions, product innovations, and engineering improvements across our service.In this post, we discuss a day in the life of experimentation and causal inference data scientists at Netflix, interviewing some of our stunning colleagues along the way. We talked to scientists from areas like Payments &amp; Partnerships, Content &amp; Marketing Analytics Research, Content Valuation, Customer Service, Product Innovation, and Studio Production. You’ll read about their backgrounds, what best prepared them for their current role at Netflix, what they do in their day-to-day, and how Netflix contributes to their growth in their data science journey.Who we areOne of the best parts of being a data scientist at Netflix is that there’s no one type of data scientist! We come from many academic backgrounds, including economics, radiotherapy, neuroscience, applied mathematics, political science, and biostatistics. We worked in different industries before joining Netflix, including tech, entertainment, retail, science policy, and research. These diverse and complementary backgrounds enrich the perspectives and technical toolkits that each of us brings to a new business question.We’ll turn things over to introduce you to a few of our data scientists, and hear how they got here.What brought you to the field of data science? Did you always know you wanted to do data science?Roxy Du (Product Innovation)[Roxy D.] A combination of interest, passion, and luck! While working on my PhD in political science, I realized my curiosity was always more piqued by methodological coursework, which led me to take as many stats/data science courses as I could. Later I enrolled in a data science program focused on helping academics transition to industry roles.Reza Badri (Content Valuation)[Reza B.] A passion for making informed decisions based on data. Working on my PhD, I was using optimization techniques to design radiotherapy fractionation schemes to improve the results of clinical practices. I wanted to learn how to better extract interesting insight from data, which led me to take several courses in statistics and machine learning. After my PhD, I started working as a data scientist at Target, where I built mathematical models to improve real-time pricing recommendation and ad serving engines.Gwyn Bleikamp (Payments)[Gwyn B.]: I’ve always loved math and statistics, so after college, I planned to become a statistician. I started working at a local payment processing company after graduation, where I built survival models to calculate lifetime value and experimented with them on our brand new big data stack. I was doing data science without realizing it.What best prepared you for your current role at Netflix? Are there any experiences that particularly helped you bring a unique voice/point of view to Netflix?David Cameron (Studio Production)[David C.] I learned a lot about sizing up the potential impact of an opportunity (using back of the envelope math), while working as a management consultant after undergrad. This has helped me prioritize my work so that I’m spending most of my time on high-impact projects.Aliki Mavromoustaki (Content &amp; Marketing)[Aliki M.] My academic credentials definitely helped on the technical side. Having a background in research also helps with critical thinking and being comfortable with ambiguity. Personally I value my teaching experiences the most, as they allowed me to improve the way I approach and break down problems effectively.What we do at NetflixBut what does a day in the life of an experimentation/causal inference data scientist at Netflix actually look like? We work in cross-functional environments, in close collaboration with business, product and creative decision makers, engineers, designers, and consumer insights researchers. Our work provides insights and informs key decisions that improve our product and create more joy for our members. To hear more, we’ll hand you back over to our stunning colleagues.Tell us about your business area and the type of stakeholders you partner with on a regular basis. How do you, as a data scientist, fill in the pieces between product, engineering, and design?[Roxy D.] I partner with product managers to run AB experiments that drive product innovation. I collaborate with product managers, designers, and engineers throughout the lifecycle of a test, including ideation, implementation, analysis, and decision-making. Recently, we introduced a simple change in kids profiles that helps kids more easily find their rewatched titles. The experiment was conceived based on what we’d heard from members in consumer research, and it was very gratifying to address an underserved member need.[David C.] There are several different flavors of data scientist in the Artwork and Video team. My specialties are on the Statistics and Optimization side. A recent favorite project was to determine the optimal number of images to create for titles. This was a fun project for me, because it combined optimization, statistics, understanding of reinforcement learning bandit algorithms, as well as general business sense, and it has far-reaching implications to the business.What are your responsibilities as the data scientist in these projects? What technical skills do you draw on most?[Gwyn B.] Data scientists can take on any aspect of an experimentation project. Some responsibilities I routinely have are: designing tests, metrics development and defining what success looks like, building data pipelines and visualization tools for custom metrics, analyzing results, and communicating final recommendations with broad teams. Coding with statistical software and SQL are my most widely used technical skills.[David C.] One of the most important responsibilities I have is doing the exploratory data analysis of the counterfactual data produced by our bandit algorithms. These analyses have helped our stakeholders identify major opportunities, bugs and tighten up engineering pipelines. One of the most common analyses that I do is a look-back analysis on the explore-data. This data helps us analyze natural experiments and understand which type of images better introduce our content to our members.Wenjing Zheng (Partnerships)Stephanie Lane (Partnerships)[Stephanie L. &amp; Wenjing Z.] As data scientists in Partnerships, we work closely with our business development, partner marketing, and partner engagement teams to create the best possible experience of Netflix on every device. Our analyses help inform ways to improve certain product features (e.g., a Netflix row on your Smart TV) and consumer offers (e.g., getting Netflix as part of a bundled package), to provide the best experiences and value for our customers. But randomized, controlled experiments are not always feasible. We draw on technical expertise in varied forms of causal inference — interrupted time series designs, inverse probability weighting, and causal machine learning — to identify promising natural experiments, design quasi-experiments, and deliver insights. Not only do we own all steps of the analysis and communicate findings within Netflix, we often participate in discussions with external partners on how best to improve the product. Here, we draw on strong business context and communication to be most effective in our roles.What non-technical skills do you draw on most?[Aliki M.] Being able to adapt my communication style to work well with both technical and non-technical audiences. Building strong relationships with partners and working effectively in a team.[Gwyn B.] Written communication is among the topmost valuable non-technical assets. Netflix is a memo-based culture, which means we spend a lot of time reading and writing. This is a primary way we share results and recommendations as well as solicit feedback on project ideas. Data Scientists need to be able to translate statistical analyses, test results, and significance into recommendations that the team can understand and action on.How is working at Netflix different from where you’ve worked before?[Reza B.] The Netflix culture makes it possible for me to continuously grow both technically and personally. Here, I have the opportunity to take risks and work on problems that I find interesting and impactful. Netflix is a great place for curious researchers that want to be challenged everyday by working on interesting problems. The tooling here is amazing, which made it easy for me to make my models available at scale across the company.Mihir Tendulkar (Payments)[Mihir T.] Each company has their own spin on data scientist responsibilities. At my previous company, we owned everything end-to-end: data discovery, cleanup, ETL, analysis, and modeling. By contrast, Netflix puts data infrastructure and quality control under the purview of specialized platform teams, so that I can focus on supporting my product stakeholders and improving experimentation methodologies. My wish-list projects are becoming a reality here: studying experiment interaction effects, quantifying the time savings of Bayesian inference, and advocating for Mindhunter Season 3.[Stephanie L.] In my last role, I worked at a research think tank in the D.C. area, where I focused on experimentation and causal inference in national defense and science policy. What sets Netflix apart (other than the domain shift!) is the context-rich culture and broad dissemination of information. New initiatives and strategy bets are captured in memos for anyone in the company to read and engage in discourse. This context-rich culture enables me to rapidly absorb new business context and ultimately be a better thought partner to my stakeholders.Data scientists at Netflix wear many hats. We work closely with business and creative stakeholders at the ideation stage to identify opportunities, formulate research questions, define success, and design studies. We partner with engineers to implement and debug experiments. We own all aspects of the analysis of a study (with help from our stellar data engineering and experimentation platform teams) and broadly communicate the results of our work. In addition to company-wide memos, we often bring our analytics point of view to lively cross-functional debates on roll-out decisions and product strategy. These responsibilities call for technical skills in statistics and machine learning, and programming knowledge in statistical software (R or Python) and SQL. But to be truly effective in our work, we also rely on non-technical skills like communication and collaborating in an interdisciplinary team.You’ve now heard how our data scientists got here and what drives them to be successful at Netflix. But the tools of data science, as well as the data needs of a company, are constantly evolving. Before we wrap up, we’ll hand things over to our panel one more time to hear how they plan to continue growing in their data science journey at Netflix.How are you looking to develop as a data scientist in the near future, and how does Netflix help you on that path?[Reza B.] As a researcher, I like to continue growing both technically and non-technically; to keep learning, being challenged and work on impactful problems. Netflix gives me the opportunity to work on a variety of interesting problems, learn cutting-edge skills and be impactful. I am passionate about improving decision making through data, and Netflix gives me that opportunity. Netflix culture helps me receive feedback on my non-technical and technical skills continuously, providing helpful context for me to grow and be a better scientist.[Aliki M.] True to our Netflix values, I am very curious and want to continue to learn, strengthen and expand my skill set. Netflix exposes me to interesting questions that require critical thinking from design to execution. I am surrounded by passionate individuals who inspire me and help me be better through their constructive feedback. Finally, my manager is highly aligned with me regarding my professional goals and looks for opportunities that fit my interests and passions.[Roxy D.] I look forward to continuously growing on both the technical and non-technical sides. Netflix has been my first experience outside academia, and I have enjoyed learning about the impact and contribution of data science in a business environment. I appreciate that Netflix’s culture allows me to gain insights into various aspects of the business, providing helpful context for me to work more efficiently, and potentially with a larger impact.As data scientists, we are continuously looking to add to our technical toolkit and to cultivate non-technical skills that drive more impact in our work. Working alongside stunning colleagues from diverse technical and business areas means that we are constantly learning from each other. Strong demand for data science across all business areas of Netflix affords us the ability to collaborate in new problem areas and develop new skills, and our leaders help us identify these opportunities to further our individual growth goals. The constructive feedback culture in Netflix is also key in accelerating our growth. Not only does it help us see blind spots and identify areas of improvement, it also creates a supportive environment where we help each other grow.Learning moreInterested in learning more about data roles at Netflix? You’re in the right place! Check out our post on Analytics at Netflix to find out more about two other data roles at Netflix — Analytics Engineers and Data Visualization Engineers — who also drive business impact through data. You can search our open roles in Data Science and Engineering here. Our culture is key to our impact and growth: read about it here.A Day in the Life of an Experimentation and Causal Inference Scientist @ Netflix was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-03-03 00:45:43","link":"https://netflixtechblog.com/a-day-in-the-life-of-an-experimentation-and-causal-inference-scientist-netflix-388edfb77d21?source=rss----2615bd06b42e---4","blog":{"id":"netflix","link":"https://netflixtechblog.com","name":"Netflix","rssFeed":"https://netflixtechblog.com/feed","type":"company"},"blogType":"company"},{"id":"d80f01ffaa1a572bdf54239a257530a8","publish_timestamp":1614633046,"title":"The Netflix Cosmos Platform","blogName":"Netflix","image":"https://miro.medium.com/max/1200/0*JOdBQaV4d4G7zy4G","categories":["microservices","mediaprocessing","distributedcomputing","serverlesscomputing","workflow"],"description":"Orchestrated Functions as a Microserviceby Frank San Miguel on behalf of the Cosmos teamIntroductionCosmos is a computing platform that combines the best aspects of microservices with asynchronous workflows and serverless functions. Its sweet spot is applications that involve resource-intensive algorithms coordinated via complex, hierarchical workflows that last anywhere from minutes to years. It supports both high throughput services that consume hundreds of thousands of CPUs at a time, and latency-sensitive workloads where humans are waiting for the results of a computation.A Cosmos serviceThis article will explain why we built Cosmos, how it works and share some of the things we have learned along the way.BackgroundThe Media Cloud Engineering and Encoding Technologies teams at Netflix jointly operate a system to process incoming media files from our partners and studios to make them playable on all devices. The first generation of this system went live with the streaming launch in 2007. The second generation added scale but was extremely difficult to operate. The third generation, called Reloaded, has been online for about seven years and has proven to be stable and massively scalable.When Reloaded was designed, we were a small team of developers operating a constrained compute cluster, and focused on one use case: the video/audio processing pipeline. As time passed the number of developers more than tripled, the breadth and depth of our use cases expanded, and our scale increased more than tenfold. The monolithic architecture significantly slowed down the delivery of new features. We could no longer expect everyone to possess the specialized knowledge that was necessary to build and deploy new features. Dealing with production issues became an expensive chore that placed a tax on all developers because infrastructure code was all mixed up with application code. The centralized data model that had served us well when we were a small team became a liability.Our response was to create Cosmos, a platform for workflow-driven, media-centric microservices. The first-order goals were to preserve our current capabilities while offering:Observability — via built-in logging, tracing, monitoring, alerting and error classification.Modularity — An opinionated framework for structuring a service and enabling both compile-time and run-time modularity.Productivity — Local development tools including specialized test runners, code generators, and a command line interface.Delivery — A fully-managed continuous-delivery system of pipelines, continuous integration jobs, and end to end tests. When you merge your pull request, it makes it to production without manual intervention.While we were at it, we also made improvements to scalability, reliability, security, and other system qualities.OverviewA Cosmos service is not a microservice but there are similarities. A typical microservice is an API with stateless business logic which is autoscaled based on request load. The API provides strong contracts with its peers while segregating application data and binary dependencies from other systems.A typical microserviceA Cosmos service retains the strong contracts and segregated data/dependencies of a microservice, but adds multi-step workflows and computationally intensive asynchronous serverless functions. In the diagram below of a typical Cosmos service, clients send requests to a Video encoder service API layer. A set of rules orchestrate workflow steps and a set of serverless functions power domain-specific algorithms. Functions are packaged as Docker images and bring their own media-specific binary dependencies (e.g. debian packages). They are scaled based on queue size, and may run on tens of thousands of different containers. Requests may take hours or days to complete.A typical Cosmos serviceSeparation of concernsCosmos has two axes of separation. On the one hand, logic is divided between API, workflow and serverless functions. On the other hand, logic is separated between application and platform. The platform API provides media-specific abstractions to application developers while hiding the details of distributed computing. For example, a video encoding service is built of components that are scale-agnostic: API, workflow, and functions. They have no special knowledge about the scale at which they run. These domain-specific, scale-agnostic components are built on top of three scale-aware Cosmos subsystems which handle the details of distributing the work:Optimus, an API layer mapping external requests to internal business models.Plato, a workflow layer for business rule modeling.Stratum, a serverless layer called for running stateless and computational-intensive functions.The subsystems all communicate with each other asynchronously via Timestone, a high-scale, low-latency priority queuing system. Each subsystem addresses a different concern of a service and can be deployed independently through a purpose-built managed Continuous Delivery process. This separation of concerns makes it easier to write, test, and operate Cosmos services.Separation of Platform and ApplicationA Cosmos service requestTrace graph of a Cosmos service requestThe picture above is a screenshot from Nirvana, our observability portal. It shows a typical service request in Cosmos (a video encoder service in this case):There is one API call to encode, which includes the video source and a recipeThe video is split into 31 chunks, and the 31 encoding functions run in parallelThe assemble function is invoked onceThe index function is invoked onceThe workflow is complete after 8 minutesLayering of servicesCosmos supports decomposition and layering of services. The resulting modular architecture allows teams to concentrate on their area of specialty and control their APIs and release cycles.For example, the video service mentioned above is just one of many used to create streams that can be played on devices. These services, which also include inspection, audio, text, and packaging, are orchestrated using higher-level services. The largest and most complex of these is Tapas, which is responsible for taking sources from studios and making them playable on the Netflix service. Another high-level service is Sagan, which is used for studio operations like marketing clips or daily production editorial proxies.Layering of Cosmos servicesWhen a new title arrives from a production studio, it triggers a Tapas workflow which orchestrates requests to perform inspections, encode video (multiple resolutions, qualities, and video codecs), encode audio (multiple qualities and codecs), generate subtitles (many languages), and package the resulting outputs (multiple player formats). Thus, a single request to Tapas can result in hundreds of requests to other Cosmos services and thousands of Stratum function invocations.The trace below shows an example of how a request at a top level service can trickle down to lower level services, resulting in many different actions. In this case the request took 24 minutes to complete, with hundreds of different actions involving 8 different Cosmos services and 9 different Stratum functions.Trace graph of a service request through multiple layersWorkflows rule!Or should we say workflow rules? Plato is the glue that ties everything together in Cosmos by providing a framework for service developers to define domain logic and orchestrate stateless functions/services. The Optimus API layer has built-in facilities to invoke workflows and examine their state. The Stratum serverless layer generates strongly-typed RPC clients to make invoking a serverless function easy and intuitive.Plato is a forward chaining rule engine which lends itself to the asynchronous and compute-intensive nature of our algorithms. Unlike a procedural workflow engine like Netflix’s Conductor, Plato makes it easy to create workflows that are “always on”. For example, as we develop better encoding algorithms, our rules-based workflows automatically manage updating existing videos without us having to trigger and manage new workflows. In addition, any workflow can call another, which enables the layering of services mentioned above.Plato is a multi-tenant system (implemented using Apache Karaf), which greatly reduces the operational burden of operating a workflow. Users write and test their rules in their own source code repository and then deploy the workflow by uploading the compiled code to the Plato server.Developers specify their workflows in a set of rules written in Emirax, a domain specific language built on Groovy. Each rule has 4 sections:match: Specifies the conditions that must be satisfied for this rule to triggeraction: Specifies the code to be executed when this rule is triggered; this is where you invoke Stratum functions to process the request.reaction: Specifies the code to be executed when the action code completes successfullyerror: Specifies the code to be executed when an error is encountered.In each of these sections, you typically first record the change in state of the workflow and then perform steps to move the workflow forward, such as executing a Stratum function or returning the results of the execution (For more details, see this presentation).Latency-sensitive applicationsCosmos services like Sagan are latency sensitive because they are user-facing. For example, an artist who is working on a social media post doesn’t want to wait a long time when clipping a video from the latest season of Money Heist. For Stratum, latency is a function of the time to perform the work plus the time to get computing resources. When work is very bursty (which is often the case), the “time to get resources” component becomes the significant factor. For illustration, let’s say that one of the things you normally buy when you go shopping is toilet paper. Normally there is no problem putting it in your cart and getting through the checkout line, and the whole process takes you 30 minutes.Resource scarcityThen one day a bad virus thing happens and everyone decides they need more toilet paper at the same time. Your toilet paper latency now goes from 30 minutes to two weeks because the overall demand exceeds the available capacity. Cosmos applications (and Stratum functions in particular) have this same problem in the face of bursty and unpredictable demand. Stratum manages function execution latency in a few ways:Resource pools. End-users can reserve Stratum computing resources for their own business use case, and resource pools are hierarchical to allow groups of users to share resources.Warm capacity. End-users can request compute resources (e.g. containers) in advance of demand to reduce startup latencies in Stratum.Micro-batches. Stratum also uses micro-batches, which is a trick found in platforms like Apache Spark to reduce startup latency. The idea is to spread the startup cost across many function invocations. If you invoke your function 10,000 times, it may run one time each on 10,000 containers or it may run 10 times each on 1000 containers.Priority. When balancing cost with the desire for low latency, Cosmos services usually land somewhere in the middle: enough resources to handle typical bursts but not enough to handle the largest bursts with the lowest latency. By prioritizing work, applications can still ensure that the most important work is processed with low latency even when resources are scarce. Cosmos service owners can allow end-users to set priority, or set it themselves in the API layer or in the workflow.Throughput-sensitive applicationsServices like Tapas are throughput-sensitive because they consume large amounts of computing resources (e.g millions of CPU-hours per day) and are more concerned with the completion of tasks over a period of hours or days rather than the time to complete an individual task. In other words, the service level objectives (SLO) are measured in tasks per day and cost per task rather than tasks per second.For throughput-sensitive workloads, the most important SLOs are those provided by the Stratum serverless layer. Stratum, which is built on top of the Titus container platform, allows throughput sensitive workloads to use “opportunistic” compute resources through flexible resource scheduling. For example, the cost of a serverless function invocation might be lower if it is willing to wait up to an hour to execute.The strangler figWe knew that moving a legacy system as large and complicated as Reloaded was going to be a big leap over a dangerous chasm littered with the shards of failed re-engineering projects, but there was no question that we had to jump. To reduce risk, we adopted the strangler fig pattern which lets the new system grow around the old one and eventually replace it completely.Still learningWe started building Cosmos in 2018 and have been operating in production since early 2019. Today there are about 40 cosmos services and we expect more growth to come. We are still in mid-journey but we can share a few highlights of what we have learned so far:The Netflix culture played a key roleThe Netflix engineering culture famously relies on personal judgement rather than top-down control. Software developers have both freedom and responsibility to take risks and make decisions. None of us have the title of Software Architect; all of us play that role. In this context, Cosmos emerged in fits and starts from disparate attempts at local optimization. Optimus, Plato and Stratum were conceived independently and eventually coalesced into the vision of a single platform. The application developers on the team kept everyone focused on user-friendly APIs and developer productivity. It took a strong partnership between infrastructure and media algorithm developers to turn the vision into reality. We couldn’t have done that in a top-down engineering environment.Microservice + Workflow + ServerlessWe have found that the programming model of “microservices that trigger workflows that orchestrate serverless functions” to be a powerful paradigm. It works well for most of our use cases but some applications are simple enough that the added complexity is not worth the benefits.A platform mindsetMoving from a large distributed application to a “platform plus applications” was a major paradigm shift. Everyone had to change their mindset. Application developers had to give up a certain amount of flexibility in exchange for consistency, reliability, etc. Platform developers had to develop more empathy and prioritize customer service, user productivity, and service levels. There were moments where application developers felt the platform team was not focused appropriately on their needs, and other times when platform teams felt overtaxed by user demands. We got through these tough spots by being open and honest with each other. For example after a recent retrospective, we strengthened our development tracks for crosscutting system qualities such as developer experience, reliability, observability and security.Platform winsWe started Cosmos with the goal of enabling developers to work better and faster, spending more time on their business problem and less time dealing with infrastructure. At times the goal has seemed elusive, but we are beginning to see the gains we had hoped for. Some of the system qualities that developers like best in Cosmos are managed delivery, modularity, and observability, and developer support. We are working to make these qualities even better while also working on weaker areas like local development, resilience and testability.Future plans2021 will be a big year for Cosmos as we move the majority of work from Reloaded into Cosmos, with more developers and much higher load. We plan to evolve the programming model to accommodate new use cases. Our goals are to make Cosmos easier to use, more resilient, faster and more efficient. Stay tuned to learn more details of how Cosmos works and how we use it.The Netflix Cosmos Platform was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-03-01 21:10:46","link":"https://netflixtechblog.com/the-netflix-cosmos-platform-35c14d9351ad?source=rss----2615bd06b42e---4","blog":{"id":"netflix","link":"https://netflixtechblog.com","name":"Netflix","rssFeed":"https://netflixtechblog.com/feed","type":"company"},"blogType":"company"},{"id":"e0d695221a98c74aaa2fc0ce708acbe9","publish_timestamp":1614299625,"title":"Packaging award-winning shows with award-winning technology","blogName":"Netflix","image":"https://miro.medium.com/max/350/1*A5PR2QJ7STPbUd2xTg6z_g.png","categories":["netflix","streaming","opensource","awardwinning","fileformat"],"description":"By Cyril ConcolatoIntroductionIn previous blog posts, our colleagues at Netflix have explained how 4K video streams are optimized, how even legacy video streams are improved and more recently how new audio codecs can provide better aural experiences to our members. In all these cases, prior to being delivered through our content delivery network Open Connect, our award-winning TV shows, movies and documentaries like The Crown need to be packaged to enable crucial features for our members. In this post, we explain these features and how we rely on award-winning standard formats and open source software to enable them.The CrownKey Packaging FeaturesIn typical streaming pipelines, packaging is the step that happens just after encoding, as depicted in the figure below. The output of an encoder is a sequence of bytes, called an elementary stream, which can only be parsed with some understanding of the elementary stream syntax. For example, detecting frame boundaries in an AV1 video stream requires being able to parse so-called Open Bitstream Units (OBU) and identifying Temporal Delimiters OBU. However, high level operations performed on client devices, such as seeking, do not need to be aware of the elementary syntax and benefit from a codec-agnostic format. The packaging step aims at producing such a codec-agnostic sequence of bytes, called packaged format, or container format, which can be manipulated, to some extent, without a deep knowledge of the coding format.Figure 1 — Simplified architecture of a streaming preparation pipelineA key feature that our members rightfully deserve when playing audio, video, and timed text is synchronization. At Netflix, we strive to provide an experience where you never see the lips of the Queen of England move before you hear her corresponding dialog in The Crown. Synchronization is achieved by fundamental elements of signaling such as clocks or time lines, time stamps, and time scales that are provided in packaged content.Our members don’t simply watch our series from beginning to end. They seek into Bridgerton when they resume watching. They rewind and replay their favorite chess move in The Queen’s Gambit. They skip introductions and recaps when they frantically binge-watch Lupin. They make playback decisions when they watch interactive titles such as You vs. Wild. Due to the nature of the audio or video compression techniques, a player cannot necessarily start decoding the stream exactly where our members want. Under the hood, players have to locate points in the stream where decoding can start, decode as quickly as they can, until the user seek point is reached before starting playback. This is another basic feature of packaging: signaling frame types and particularly Random Access Points.When our members’ kids watch Carmen Sandiego in the back seats of their parents’ car or more generally when the network throughput varies, adaptive streaming technologies are applied to provide the best viewing experience under the network conditions. Adaptive streaming technologies require that streams of various qualities be encoded to common constraints but they also rely on another key feature of packaging to offer seamless quality switching, called indexing. Indexing lets the player fetch only the corresponding segments of the new stream.Many other elements of signaling are provided in our packaged content to enable the viewing to start as quickly as possible and in the best possible conditions. Decryption modules need to be initialized with the appropriate scheme and initialization vector. Hardware video decoders need to know in advance the resolution and bit depth of the video streams to allocate their decoding buffers. Rendering pipelines need to know ahead of time the speaker configuration of audio streams or whether the video streams are HDR or SDR. Being able to signal all these elements is also a key feature of modern packaging formats.The role of standards and open source softwareOur 200+ million members watch Netflix on a wide variety of devices, from smartphones, to laptops, to TVs and many more, developed by a large number of partners. Reducing the friction when on-boarding a new device and making sure that our content will be playable on old devices for a long time is very important. That is where standards play a key role. The ISO Base Media File Format (ISOBMFF) is the key packaging standard in the entertainment industry as recently recognized with a Technology &amp; Engineering Emmy® Award by the National Academy of Television Arts &amp; Sciences (NATAS).ISOBMFF provides all the key packaging features mentioned above, and as history proves, it is also versatile and extensible, in its capabilities of adding new signaling features and in its support of codec. Streams encoded with well-established codecs such as AVC and AAC can be carried in ISOBMFF files, but the specification is also regularly extended to support the latest codecs. The Media Systems team at Netflix actively contributes to the development, the maintenance, and the adoption of ISOBMFF. As an example, Netflix led the specification for the carriage of AOM’s AV1 video streams in ISOBMFF.With 20+ years of existence, ISOBMFF accumulated a lot of technical tools for various use cases. Figure 2 illustrates the complexity of ISOBMFF today through the concept of ‘brands’, a concept similar to profiles in audio or video standards. Initially, limited and well-nested, the standard is now very broad and evolving in various directions.Figure 2 — Illustrating the complexity of the 6th edition of ISOBMFF. Each rectangle represents a ‘brand’ (indicated by a four character code in bold), and its required set of tools (indicated by a ‘+’ line). Brands are nested. All the tools of inner brands are required by outer brands.For the Netflix streaming service, we rely on a subset of these tools as identified by the Common Media Application Format (CMAF) standard, and the content protection tools defined in the Common Encryption (CENC) standard.Multimedia standards like ISOBMFF, CMAF and CENC go hand in hand with open source software implementations. Open source software can demonstrate the features of the standard, enabling the industry to understand its benefits and broadening its adoption. Open source software can also help improve the quality of a standard by highlighting possible ambiguities through a neutral, reference implementation. The Media Systems team at Netflix maintains such a reference open source implementation, called Photon, for the SMPTE IMF standard. For ISOBMFF, Netflix uses MP4Box, the reference open source implementation from the GPAC team.In this packaging ecosystem of standards and open source software, our work within the Media Systems team includes identifying the tools within the existing standards to address new streaming use cases. When such tools don’t exist, we define new standards or expand existing ones, including ISOBMFF and CMAF, and support open source software to match these standards. For example, when our video encoding colleagues design dynamically optimized encoding schemes producing streaming segments with variable durations, we modify our workflow to ensure that segments across video streams with different bit rates remain time aligned. Similarly, when our audio encoding colleagues introduce xHE-AAC, which obsoletes the old assumption that every audio frame is decodable, we guarantee that audio/video segments remain aligned too. Finally, when we want to help the industry converge to a common encryption scheme for new video codecs such as AV1, we coordinate the discussions to select the scheme, in this case pattern-based subsample encryption (a.k.a ‘cbcs’), and lead the way by providing reference bitstreams. And of course, our work includes handling the many types of devices in the field that don’t have proper support of the standards.ConclusionWe hope that this post gave you a better understanding of a part of the work of the Media Systems team at Netflix, and hopefully next time you watch one of our award-winning shows, you will recognize the part played by ISOBMFF, a key, award-winning technology. If you want to explore another facet of the team’s work, have a look at the other award-winning technology, TTML, that we use for our Japanese subtitles.We’re hiring!If this work sounds exciting to you and you’d like to help the Media Systems team deliver an even better experience, Netflix is searching for an experienced Engineering Manager for the team. Please contact Anne Aaron for more info.Packaging award-winning shows with award-winning technology was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-02-26 00:33:45","link":"https://netflixtechblog.com/packaging-award-winning-shows-with-award-winning-technology-c1010594ba39?source=rss----2615bd06b42e---4","blog":{"id":"netflix","link":"https://netflixtechblog.com","name":"Netflix","rssFeed":"https://netflixtechblog.com/feed","type":"company"},"blogType":"company"},{"id":"d832388b939dab06a050429ca74e9891","publish_timestamp":1614277803,"title":"Beyond REST","blogName":"Netflix","image":"https://miro.medium.com/max/1200/0*QupchUd58_jK8ObO","categories":["rest","rapidprototyping","graphqlvsrest","microservices","graphql"],"description":"Rapid Development with GraphQL Microservicesby Dane AvillaThe entertainment industry has struggled with COVID-19 restrictions impacting productions around the globe. Since early 2020, Netflix has been iteratively developing systems to provide internal stakeholders and business leaders with up-to-date tools and dashboards with the latest information on the pandemic. These software solutions allow executive leadership to make the most informed decisions possible regarding if and when a given physical production can safely begin creating compelling content across the world. One approach that is gaining mind-share within Netflix Studio Engineering is the concept of GraphQL microservices (GQLMS) as a backend platform facilitating rapid application development.Many organizations are embracing GraphQL as a way to unify their enterprise-wide data model and provide a single entry point for navigating a sea of structured data with its network of related entities. Such efforts are laudable but often entail multiple calendar quarters of coordination between internal organizations followed by the development and integration of all relevant entities into a single monolithic graph.In contrast to this “One Graph to Rule Them All” approach, GQLMS leverage GraphQL simply as an enriched API specification for building CRUD applications. Our experience using GQLMS for rapid proof-of-concept applications confirmed two theories regarding the advertised benefits of GraphQL:The GraphiQL IDE displays any available GraphQL documentation right alongside the schema, dramatically improving developer ergonomics for API consumers (in contrast to the best-in-class Swagger UI).GraphQL’s strong type system and polyglot client support mean API providers do not need to concern themselves with generating, versioning, and maintaining language-specific API clients (such as those generated with the excellent Swagger Codegen). Consumers of GraphQL APIs can simply leverage the open-source GraphQL client of their preference.GraphiQL: Auto-generated test GUI for the Star Wars APIOur experience has led to an architecture with a number of best-practices for teams interested in GQLMS as a platform for rapid development.GraphileDuring early GraphQL exploration efforts, Netflix engineers became aware of the Graphile library for presenting PostgreSQL database objects (tables, views, and functions) as a GraphQL API. Graphile supports smart comments allowing control of various features by tagging database tables, views, columns, and types with specifically formatted PostgreSQL comments. Documentation can even be embedded in the database comments such that it displays in the GraphQL schema generated by Graphile.We hypothesized that a Docker container running a very simple NodeJS web server with the Graphile library (and some additional Netflix internal components for security, logging, metrics, and monitoring) could provide a “better REST than REST” or “REST++” platform for rapid development efforts. Using Docker we defined a lightweight, stand-alone container that allowed us to package the Graphile library and its supporting code into a self-contained bundle that any team can use at Netflix with no additional coding required. Simply pull down the defined Docker base image and run it with the appropriate database connection string. This approach proved to be very successful and yielded several insights into the use of Graphile.Specifically:Use database views as an “API layer” to preserve flexibility in order to allow modifying tables without changing an existing GraphQL schema (built on the database views).Use PostgreSQL Composite Types when taking advantage of PostgreSQL Aggregate Functions.Increase flexibility by allowing GraphQL clients to have “full access” to the auto-generated GraphQL queries and mutations generated by Graphile (exposing CRUD operations on all tables &amp; views); then later in the development process, remove schema elements that did not end up being used by the UI before the app goes into production.Database views as APIWe decided to put the data tables in one PostgreSQL schema and then define views on those tables in another schema, with the Graphile web app connecting to the database using a dedicated PostgreSQL user role. This ended up achieving several different goals:Underlying tables could be changed independently of the views exposed in the GraphQL schema.Views could do basic formatting (like rendering TIMESTAMP fields as ISO8601 strings).All permissions on the underlying table had to be explicitly granted for the web application’s PostgreSQL user, avoiding unexpected write access.Tables and views could be modified within a single transaction such that the changes to the exposed GraphQL schema happened atomically.On this last point: changing a table column’s type would break the associated view, but by wrapping the change in a transaction, the view could be dropped, the column could be updated, and then the view could be re-created before committing the transaction. We run Graphile with pgWatch enabled, so as soon as any updates were made to the database, the GraphQL schema immediately updated to reflect the change.PostgreSQL composite typesGraphile does an excellent job reading the PostgreSQL database schema and transforming tables and basic views into a GraphQL schema, but our experience revealed limitations in how Graphile describes nested types when PostgreSQL Aggregate Functions or JSON Functions exist within a view. Native PostgreSQL functions such as json_build_object will be translated into a GraphQL JSON type, which is simply a String, devoid of any internal structure. For example, take this simplistic view returning a JSON object:postgres_test_db=# create view postgraphile.json_object_example as  select json_build_object(‘hello world’::text, 1, ‘2’::text, 3)  as json;postgres_test_db=# select * from postgraphile.json_object_example;          json— — — — — — — — — — — — -{“hello world”: 1, “2”: 3}(1 row)In the generated schema, the data type is JSON:The internal structure of the json field (the hello world and 2 sub-fields) is opaque in the generated GraphQL schema.To further describe the internal structure of the json field — exposing it within the generated schema — define a composite type, and create the view such that it returns that type:postgres_test_db=# CREATE TYPE postgraphile.custom_type AS (  &quot;hello world&quot; integer,  &quot;2&quot; integer);Next, create a function that returns that type:postgres_test_db=# CREATE FUNCTION postgraphile.custom_type(  &quot;hello world&quot; integer,  &quot;2&quot; integer)RETURNS postgraphile.custom_typeAS &#39;select $1, $2&#39;LANGUAGE SQL;Finally, create a view that returns that type:postgres_test_db=# create view postgraphile.json_object_example2 as  select postgraphile.custom_type(1, 3)  as json;postgres_test_db=# select * from postgraphile.json_object_example2; json— — — -(1,3)(1 row)At first glance, that does not look very useful, but hold that thought: before viewing the generated schema, define comments on the view, custom type, and fields of the custom type to take advantage of Graphile’s smart comments:postgres_test_db=# comment on  type postgraphile.custom_type  is E’A description for the custom type’;postgres_test_db=# comment on  view postgraphile.json_object_example2  is E’A description for the view’;postgres_test_db=# comment on  column postgraphile.custom_type.”hello world”  is E’A description for hello world’;postgres_test_db=# comment on  column postgraphile.custom_type.field_2  is E’@name field_two\\nA description for the second field’;Now, when the schema is viewed, the json field no longer shows up with opaque type JSON, but with CustomType:(also note that the comment made on the view — A description for the view — shows up in the documentation for the query field).Clicking CustomType displays the fields of the custom type, along with their comments:Notice that in the custom type, the second field was named field_2, but the Graphile smart comment renames the field to field_two and subsequently gets camel-cased by Graphile to fieldTwo. Also, the descriptions for both fields display in the generated GraphQL schema.Allow “full access” to the Graphile-generated schema (during development)Initially, the proposal to use Graphile was met with vigorous dissent when discussed as an option in a “one schema to rule them all” architecture. Legitimate concerns about security (how does this integrate with our IAM infrastructure to enforce row-level access controls within the database?) and performance (how do you limit queries to avoid DDoSing the database by selecting all rows at once?) were raised about providing open access to database tables with a SQL-like query interface. However, in the context of GQLMS for rapid development of internal apps by small teams, having the default Graphile behavior of making all columns available for filtering allowed the UI team to rapidly iterate through a number of new features without needing to involve the backend team. This is in contrast to other development models where the UI and backend teams first agree on an initial API contract, the backend team implements the API, the UI team consumes the API and then the API contract evolves as the needs of the UI change during the development life cycle.Initially, the overall app’s performance was poor as the UI often needed multiple queries to fetch the desired data. However, once the app’s behavior had been fleshed out, we quickly created new views satisfying each UI interaction’s needs such that each interaction only required a single call. Because these requests run on the database in native code, we could perform sophisticated queries and achieve high performance through the appropriate use of indexes, denormalization, clustering, etc.Once the “public API” between the UI and backend solidified, we “hardened” the GraphQL schema, removing all unnecessary queries (created by Graphile’s default settings) by marking tables and views with the smart comment @omit. Also, the default behavior is for Graphile to generate mutations for tables and views, but the smart comment @omit create,update,delete will remove the mutations from the schema.ConclusionFor those taking a schema-first approach to their GraphQL API development, the automatic GraphQL schema generation capabilities of Graphile will likely unacceptably restrict schema designers. Graphile may be difficult to integrate into an existing enterprise IAM infrastructure if fine-grained access controls are required. And adding custom queries and mutations to a Graphile-generated schema (i.e. to expose a gRPC service call needed by the UI) is something we currently do not support in our Docker image. However, we recently became aware of Graphile’s makeExtendSchemaPlugin, which allows custom types, queries, and mutations to be merged into the schema generated by Graphile.That said, the successful implementation of an internal app over 4–6 weeks with limited initial requirements and an ad hoc distributed team (with no previous history of collaboration) raised a large amount of interest throughout the Netflix Studio. Other teams within Netflix are finding the GQLMS approach of:1) using standard GraphQL constructs and utilities to expose the database-as-API2) leveraging custom PostgreSQL types to craft a GraphQL schema3) increasing flexibility by auto-generating a large API from a database4) and exposing additional custom business logic and data types alongside those generated by Graphileto be a viable solution for internal CRUD tools that would historically have used REST. Having a standardized Docker container hosting Graphile provides teams the necessary infrastructure by which they can quickly iterate on the prototyping and rapid application development of new tools to solve the ever-changing needs of a global media studio during these challenging times.Beyond REST was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-02-25 18:30:03","link":"https://netflixtechblog.com/beyond-rest-1b76f7c20ef6?source=rss----2615bd06b42e---4","blog":{"id":"netflix","link":"https://netflixtechblog.com","name":"Netflix","rssFeed":"https://netflixtechblog.com/feed","type":"company"},"blogType":"company"},{"id":"200ae2e371d8394be6e8877f9b0de811","publish_timestamp":1613494616,"title":"Building a Rule-Based Platform to Manage Netflix Membership SKUs at Scale","blogName":"Netflix","image":"https://miro.medium.com/max/856/0*f7OVF4jrU0vwNkK3","categories":["rulebasedplatform","netflixengineering","membershipengineering"],"description":"By Budhaditya Das, Wallace Wang, and Scott YaoAt Netflix, we aspire to entertain the world. From mailing DVDs in the US to a global streaming service with over 200 million subscribers across 190 countries, we have come a long way. For the longest time, Netflix had three plans (basic/standard/premium) with a single 30-day free trial offer at signup. As we expand offerings rapidly across the globe, our ideas and strategies around plans and offers are evolving as well. For example, the mobile plan launch in India and Southeast Asia was a huge success. We are inspired to provide the best offer and plan setup tailored to our customers’ needs to make their choice easier to start a membership.Membership Engineering at Netflix is responsible for the plan and pricing configurations for every market worldwide. Our team is also the primary source of truth for various offers and promotions. Internally, we use the term SKU (Stock Keeping Unit) to represent these entities. The original SKU catalog is a logic-heavy client library packaged with complex metadata configuration files and consumed by various services. However, with our rapid product innovation speed, the whole approach experienced significant challenges:Business Complexity: The existing SKU management solution was designed years ago when the engagement rules were simple — three plans and one offer homogeneously applied to all regions. As the business expanded globally, the complexity around pricing, plans, and offers increased exponentially.Operational Efficiency: The majority of the changes require metadata configuration files and library code changes, usually taking days of testing and service release to adopt the updates.Reliability: It is exceptionally challenging to effectively gauge the impact of metadata changes in the current form. With 50+ services consuming the SKU catalog library, a small change could inadvertently result in a significant outage with a global blast radius. Additionally, the business implications for pricing-related errors are enormous.Maintainability: With the increase in ongoing experimentation around SKUs, the configuration files have exploded exponentially. Besides, the mixed-use of the metadata files and business logic code adds another layer of maintenance complexity.To solve the challenges mentioned above and meet our rapidly evolving business needs, we re-architected the legacy SKU catalog from the ground up and partnered with the Growth Engineering team to build a scalable SKU platform. This re-design enabled us to reposition the SKU catalog as an extensible, scalable, and robust rule-based “self-service” platform. It was a massive but necessary undertaking to ensure that Netflix is ready for the next phase of rapid global growth and business challenges.A Platform Based on RulesOur initial use case analysis highlighted that most of the change requests were related to enhancing, configuring, or tweaking existing SKU entities to enable business teams to carry out plans or offer related A/B experiments across various geo-locations. Most of these changes are mechanical and amenable to the “self-service” model. This critical insight helped us re-envision the SKU catalog as a seamless, scalable platform that empowers our stakeholders to make rapid changes with confidence while the platform ensures suitable guardrails for data accuracy and integrity. With that idea in mind, we defined the core principles of the new SKU Platform:Ownership Clarity: Membership Engineering team owns the SKU catalog data and provides a platform for stakeholders to configure SKUs based on their needs.Self Service: SKU changes need to be flexibly configurable, validated comprehensively, and released rapidly. In comparison, the API interface for consumer services should be consistent and static regardless of the business requirement iteration.Auditability: SKU changes workflow would require engineers’ review and approval. Bad changes can quickly revert to mitigate issues and provide history for auditing.Observability: SKU resolution insight is critical and helpful for engineers to diagnose what went wrong in the change lifecycle.Building a scalable SKU catalog platform that allowed for rapid changes with the minimal intervention was challenging. We realized that abstracting out the business rules into a “rules engine” would enable us to achieve our stated goals. After evaluating multiple open-source and commercial rule evaluation frameworks, we chose our internal Rules Management and Evaluation Framework — Hendrix. Hendrix is a simple interpreted language that expresses how configuration values should be computed. These expressions (rules) are evaluated in the current request session context and can access data such as A/B test assignments, necessary member information, customized input, etc. We’ll skip over Hendrix’s specific details and focus on the SKU platform adoption in this article for brevity.The adoption of an externalized rule evaluation engine was a major game-changer. It allowed us to remove boilerplate code and took us a step forward in becoming a true self-service platform. The rules, now encoded in JSON, were easy to generate, manage and modify via automated means. It eliminated many complex conditional branching logic, making the core codebase simple and easy to enhance. Most importantly, it allowed runtime and quick business logic changes in production without code change deployments. Overall, it simplified SKU selections and increased our testing and product delivery confidence. Here is a snippet of the mobile plan availability rule:{    parameter: &quot;plans&quot;,    default: [Basic, Standard, Premium],    values:         {            feature: &quot;mobilePlanLaunched&quot;,            value: [Mobile, Basic, Standard, Premium],        },    ],},{    feature: &quot;mobilePlanLaunched&quot;,    requirements: [        {            type: &quot;request&quot;,            key: &quot;country&quot;,            oneOf: [&quot;IN&quot;, &quot;ID&quot;, &quot;MY&quot;, &quot;PH&quot;, &quot;TH&quot;],        },    ],},In addition to the rule engine, the following components make up the core building blocks of the new SKU platform:Service Layer — SKUService: A service layer replaced the original SKU catalog client library to provide a unified interface for consumers to access the SKU catalog.Persistence Layer — SKUDB: SKU catalog data was migrated from the metadata configuration files to a relational database. Adding and updating entities is audited and tightly controlled via “privileged” APIs exposed by the service layer.Business Rules — SKURules: Various business rules are defined as Hendrix expressions. For example, our business requirements dictate that a mobile plan should be available for specific markets only, while the rest of the world receives the default set of plans. These rule definitions are hosted in a separate git repository and Hendrix module within SKUService load and refresh it periodically. The changes are administered by the regular git pull request flow and guarded by the validation infrastructure.Observability/Validation Guardrails: A comprehensive validation infrastructure designed to ensure that SKURules changes are accurate and do not break existing behavior.Self Service Management UI: A straightforward visualization tool for rules management and are in the process of supporting direct rules editing.The new SKU flow for consumer services is simple, generic, and easy to maintain with business rules isolated in SKURules. Consumers pass a map of context to be used as rules evaluation criteria. Rule owners are responsible for making sure the requirements match the rule definition and request context. By choosing a generalized context map, we keep the API interface consistent regardless of the rules change. In-memory rules evaluation returns a list of SKU ids, which hibernates with the SKUDB query for the full entity metadata.Managing Rules at ScaleAs the platform evolved from code towards externalized rules to manage business flows, we realized that maintaining an ever-growing set of volatile rule configurations at Netflix scale was a critical challenge:Debugging is difficult when navigating through an ever-increasing set of rules.The impact of changing an existing rule is tricky to measure due to the nature that Hendrix evaluates rules based on first-match. There is always a possibility of prior rules taking precedence over the later ones if the changes are not handled carefully.Rules naturally have different lifecycles and impacts. Some are short-lived with specific targeted audiences (for most of our A/B tests) compared to the stabilized ones with an enormous impact on our broader member base.Rules’ ownership needed to be defined clearly for long-term maintenance health.To manage the complexity of rules and the associated lifecycle, we introduced the concept of rules categorization. Based on our use case, we classified our rules into three groups:FallbackFallback rules will execute first and short circuit the evaluation. It should easily understand, with no experimental information and domain-specific context.Most restricted access and owned by the Membership Engineering team. Since this will impact all later rules, adding rules to this category should be cautious and thoroughly reviewed.ExperimentalEvaluate right after fallback rules, solely serving our A/B tests. Frequent changes are expected to these rules from different stakeholders. Experimental rules can eventually transform into stabilized rules if we decide to ship them.Stakeholders take ownership of this rule group to initiate fast iteration of product experimentations.StabilizedEvaluate at last after fallback and experimental rules. Rarely changed but with a more significant impact on our member base.Membership Engineering team owns this group to ensure the stability of our broadest SKU offering.The diagram above demonstrates the rules evaluation order for each group. Categorizing the rules allowed the platform to streamline complex configuration and lifecycle management, enabling our stakeholders to make frequent experimentation changes with confidence.As we adopted more rules into Hendrix, we recognized that understanding the JSON format structure was not straightforward. To overcome this challenge, our tools team built a UI to visualize the rules configuration. It significantly improved the overall experience of understanding and debugging rules. Here is a sample visualization of our mobile plan availability rules:Visualization is just the first step in our endeavor to make this a truly self-service platform. The long-term goal is to support complete rule lifecycle management (edition/auditing/validation) via the UI tool.Build Confidence with Validation InfrastructureThe move towards a rule-based platform shifted a lot of assumptions around change management and deployment. The legacy paradigm involved a fixed process in applying code changes and service release can take a couple of days. With the introduction of external rules, the platform enabled our stakeholders to make near-real-time changes to business flows. It reduces the turnaround time from days to a few hours.But with great power comes great responsibility. Ensuring the SKU catalog and the associated rules’ correctness is exceptionally critical for the platform’s long-term stability. Errors in pricing will have a direct impact on our members. To protect the integrity of rules and empower stakeholders to make changes, we built a comprehensive infrastructure that implements a series of validation and verification guardrails. The primary goals of the validation infrastructure are as follows:Rule change release workflow: Establish a scalable workflow to ensure rule changes get the expected outcome from the beginning of the pull request to the final deployment stage.Snapshot and auditability: Expose mechanisms to capture a holistic snapshot of SKU rules resolution for auditing.Production alerts: Create an exhaustive set of alerts to detect anomalies and react to them quickly.Rules are just another representation of code, so the best practices that apply to code management should also apply to rules. For each rule change pull request, we also require the author to include unit-tests to ensure correctness and prevent future changes from breaking the current one unnoticed. Unit-tests are categorized with the same rules group concept. Below is an example from the stabilized rules test that mobile plan is available in India:{    test: &quot;testMobilePlanAvailableIndia&quot;,    context: {      request: [        {          key: &quot;country&quot;,          value: &quot;IN&quot;,        },      ],    },    assertions: {      plans: [Mobile, Basic, Standard, Premium],    },  },The second component of the validation infrastructure is the audit framework by leveraging our big data platform. Every rule change triggers a pipeline (spark job) that takes a snapshot of various SKUs’ current state with the latest rules. The framework carries out a differential analysis against the preceding version of the snapshot to quickly identify unintended bugs in rule changes. Additionally, the results are stored in a Hive table for auditing purposes.The last and the most crucial piece of the validation ecosystem are the production alerts. These alerts are built around Netflix’s vast real-time monitoring infrastructure and focus on ensuring that our members, across the world, are getting the expected SKUs. These alerts enable us to quickly flag anomalous trends and notify on-call engineers for speedy resolution. It provides an additional safety layer, which is critical, given the platform’s size and complexity.With the validation infrastructure providing enhanced reliability for the SKU platform, both Membership engineers and stakeholders can confidently change SKU rules. The diagram below summarizes the complete SKU rule change release workflow.What’s Next?We had tremendous success with the new rule-based SKU platform. Engineering efficiency took a significant boost that sped up the operation from weeks of collaboration to few days of effort for AB experiment and product launch. Stakeholders are empowered to make changes to rules reliably thanks to the comprehensive validation infrastructure. Moreover, we are committed to more platform enhancements like better rules debugging experience, one-stop UI for rules management, and continuously evaluating other membership domains’ opportunities to adopt rule-based solutions.If you have experience or planning to build a rule-based application, we’d love to hear it. We value knowledge sharing, and it’s the drive for industry innovation. Please check our website to learn more about our work building a subscription business at Netflix Scale. Lastly, if you are interested in joining us, Membership Engineering and Revenue Growth Tools are hiring!Building a Rule-Based Platform to Manage Netflix Membership SKUs at Scale was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-02-16 16:56:56","link":"https://netflixtechblog.com/building-a-rule-based-platform-to-manage-netflix-membership-skus-at-scale-e3c0f82aa7bc?source=rss----2615bd06b42e---4","blog":{"id":"netflix","link":"https://netflixtechblog.com","name":"Netflix","rssFeed":"https://netflixtechblog.com/feed","type":"company"},"blogType":"company"},{"id":"351a0b18626f22d537b7ab6d77e18123","publish_timestamp":1612974532,"title":"Hawkins: Diving into the Reasoning Behind our Design System","blogName":"Netflix","image":"https://miro.medium.com/max/1200/0*h6n9982qECJeRjjA","categories":["componentlibraries","designsystems","buildvsbuy","react"],"description":"Stranger Things imagery showcasing the inspiration for the Hawkins Design Systemby Hawkins team member Joshua Godi; with cover art from Martin Bekerman and additional imagery from Wiki ChavesHawkins may be the name of a fictional town in Indiana, most widely known as the backdrop for one of Netflix’s most popular TV series “Stranger Things,” but the name is so much more. Hawkins is the namesake that established the basis for a design system used across the Netflix Studio ecosystem.Have you ever used a suite of applications that had an inconsistent user experience? It can be a nightmare to work efficiently. The learning curve can be immense for each and every application in the suite, as the user is essentially learning a new tool with each interaction. Aside from the burden on these users, the engineers responsible for building and maintaining these applications must keep reinventing the wheel, starting from scratch with toolsets, component libraries and design patterns. This investment is repetitive and costly. A design system, such as the one we developed for the Netflix Studio, can help alleviate most of these headaches.We have been working on our own design system that is widely used across the Netflix Studio’s growing application catalogue, which consists of 80+ applications. These applications power the production of Netflix’s content, from pitch evaluation to financial forecasting and completed asset delivery. A typical day for a production employee could require using a handful of these applications to entertain our members across the world. We wanted a way to ensure that we can have a consistent user experience while also sharing as much code as possible.In this blog post, we will highlight why we built Hawkins, as well as how we got buy-in across the engineering organization and our plans moving forward. We recently presented a talk on how we built Hawkins; so if you are interested in more details, check out the video.What is a design system?Before we can dive into the importance of having a design system, we have to define what a design system means. It can mean different things to different people. For Hawkins, our design system is composed of two main aspects.General design system component mocksFirst, we have the design elements that form the foundational layer of Hawkins. These consist of Figma components that are used throughout the design team. These components are used to build out mocks for the engineering team. Being the foundational layer, it is important that these assets are consistent and intuitive.Second, we have our React component library, which is a JavaScript library for building user interfaces. The engineering team uses this component library to ensure that each and every component is reusable, conforms to the design assets and can be highly configurable for different situations. We also make sure that each component is composable and can be used in many different combinations. We made the decision to keep our components very atomic; this keeps them small, lightweight and easy to combine into larger components.At Netflix, we have two teams composed of six people who work together to make Hawkins a success, but that doesn’t always need to be the case. A successful design system can be created with just a small team. The key aspects are that it is reusable, configurable and composable.Why is a design system important?Having a solid design system can help to alleviate many issues that come from maintaining so many different applications. A design system can bring cohesion across your suite of applications and drastically reduce the engineering burden for each application.Examples of Figma components for the Hawkins Design SystemQuality user experience can be hard to come by as your suite of applications grow. A design system should be there to help ease that burden, acting as the blueprint on how you build applications. Having a consistent user experience also reduces the training required. If users know how to fill out forms, access data in a table or receive notifications in one application, they will intuitively know how to in the next application.The design system acts as a language that both designers and engineers can speak to align on how applications are built out. It also helps with onboarding new team members due to the documentation and examples outlined in your design system.The last and arguably biggest win for design systems is the reduction of burden on engineering. There will only be one implementation of buttons, tables, forms, etc. This greatly reduces the number of bugs and improves the overall health and performance of every application that uses the design system. The entire engineering organization is working to improve one set of components vs. each using their own individual components. When a component is improved, whether through additional functionality or a bug fix, the benefit is shared across the entire organization.Taking a wide view of the Netflix Studio landscape, we saw many opportunities where Hawkins could bring value to the engineering organization.Build vs. buyThe first question we asked ourselves is whether we wanted to build out an entire design system from scratch or leverage an existing solution. There are pros and cons to each approach.Building it yourself — The benefits of DIY means that you are in control every step of the way. You get to decide what will be included in the design system and what is better left out. The downside is that because you are responsible for it all, it will likely take longer to complete.Leveraging an existing solution — When you leverage an existing solution, you can still customize certain elements of that solution, but ultimately you are getting a lot out of the box for free. Depending on which solution you choose, you could be inheriting a ton of issues or something that is battle tested. Do your research and don’t be afraid to ask around!For Hawkins, we decided to take both approaches. On the design side, we decided to build it ourselves. This gave us complete creative control over how our user experience is throughout the design language. On the engineering side, we decided to build on top of an existing solution by utilizing Material-UI. Leveraging Material-UI, gave us a ton of components out of the box that we can configure and style to meet the needs of Hawkins. We also chose to obfuscate a number of the customizations that come from the library to ensure upgrading or replacing components will be smoother.Generating users and getting buy-inThe single biggest question that we had when building out Hawkins is how to obtain buy-in across the engineering organization. We decided to track the number of uses of each component, the number of installs of the packages themselves, and how many applications were using Hawkins in production as metrics to determine success.There is a definitive cost that comes with building out a design system no matter the route you take. The initial cost is very high, with research, building out the design tokens and the component library. Then, developers have to begin consuming the libraries inside of applications, either with full re-writes or feature by feature.Graph depicting the cost of building a design systemA good representation of this is the graph above. While an organization may spend a lot of time initially making the design system, it will benefit greatly once it is fully implemented and trusted across the organization. With Hawkins, our initial build phase took about two quarters. The two quarters were split between Q1 consisting of creating the design language and Q2 being the implementation phase. Engineering and Design worked closely during the entire build phase. The end result was a significant number of components in Figma and a large component library leveraging Material-UI. Only then could we start to look for engineering teams to start using Hawkins.When building out the component library, we set out to accomplish four key aspects that we felt would help drive support for Hawkins:Document components — First, we ensured that each component was fully documented and had examples using Storybook.On-call rotation for support — Next, we set up an on-call rotation in Slack, where engineers could not only seek guidance, but report any issues they may have encountered. It was extremely important to be responsive in our communication channels. The more support engineers feel they have, the more receptive they will be to using the design library.Demonstrate Hawkins usefulness — Next, we started to do “road shows,” where we would join team meetings to demonstrate the value that Hawkins could bring to each and every team. This also provided an opportunity for the engineers to ask questions in person and for us to gather feedback to ensure our plans for Hawkins would meet their needs.Bootstrap features for proof of concept — Finally, we helped bootstrap out features or applications for teams as a proof of concept. All of these together helped to foster a relationship between the Hawkins team and engineering teams.Even today, as the Hawkins team, we run through all of the above exercises and more to ensure that the design system is robust and has the level of support the engineering organization can trust.Handling the outliersThe Hawkins libraries all consist of basic components that are the building blocks to the applications across the Netflix Studio. When engineers increased their usage of Hawkins, it became clear that many folks were using the atomic components to build more complex experiences that were common across multiple applications, like in-app chat, data grids, and file uploaders, to name a few. We did not want to put these components straight into Hawkins because of the complexity and because they weren’t used across the entire Studio. So, we were tasked with identifying a way to share these complex components while still being able to benefit from all the work we accomplished on Hawkins.To meet this challenge, developers decided to spin up a parallel library that sits right next to Hawkins. This library builds on top of the existing design system to provide a home for all the complex components that didn’t fit into the original design system.Venn diagram showing the relationship between the librariesThis library was set up as a Lerna monorepo with tooling to quickly jumpstart a new package. We followed the same steps as Hawkins with Storybook and communication channels. The benefit of using a monorepo was that it gave engineering a single place to discover what components are available when building out applications. We also decided to version each package independently, which helped avoid issues with updating Hawkins or in downstream applications.With so many components that will go into this parallel library, we decided on taking an “open source” approach to share the burden of responsibility for each component. Every engineer is welcome to contribute new components and help fix bugs or release new features in existing components. This model helps spread the ownership out from just a single engineer to a team of developers and engineers working in tandem.It is the goal that eventually these components could be migrated into the Hawkins library. That is why we took the time to ensure that each repository has the same rules when it came to development, testing and building. This would allow for an easy migration.Wrapping upWe still have a long way to go on Hawkins. There are still a plethora of improvements that we can do to enhance performance and developer ergonomics, and make it easier to work with Hawkins in general, especially as we start to use Hawkins outside of just the Netflix Studio!Logo for the Hawkins Design SystemWe are very excited to share our work on Hawkins and dive into some of the nuances that we came across.Hawkins: Diving into the Reasoning Behind our Design System was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-02-10 16:28:52","link":"https://netflixtechblog.com/hawkins-diving-into-the-reasoning-behind-our-design-system-964a7357547?source=rss----2615bd06b42e---4","blog":{"id":"netflix","link":"https://netflixtechblog.com","name":"Netflix","rssFeed":"https://netflixtechblog.com/feed","type":"company"},"blogType":"company"},{"id":"4137ea9c002798b24d409774ec5a2140","publish_timestamp":1612891637,"title":"Growth Engineering at Netflix- Creating a Scalable Offers Platform","blogName":"Netflix","image":"https://miro.medium.com/max/1200/1*ozt2fJcpuvOxslEv8S_fhg.png","categories":["softwareengineering","growth","netflix","technology","softwaredevelopment"],"description":"by Eric EiswerthBackgroundNetflix has been offering streaming video-on-demand (SVOD) for over 10 years. Throughout that time we’ve primarily relied on 3 plans (Basic, Standard, &amp; Premium), combined with the 30-day free trial to drive global customer acquisition. The world has changed a lot in this time. Competition for people’s leisure time has increased, the device ecosystem has grown phenomenally, and consumers want to watch premium content whenever they want, wherever they are, and on whatever device they prefer. We need to be constantly adapting and innovating as a result of this change.The Growth Engineering team is responsible for executing growth initiatives that help us anticipate and adapt to this change. In particular, it’s our job to design and build the systems and protocols that enable customers from all over the world to sign up for Netflix with the plan features and incentives that best suit their needs. For more background on Growth Engineering and the signup funnel, please have a look at our previous blog post that covers the basics. Alternatively, here’s a quick review of what the typical user journey for a signup looks like:Signup Funnel DynamicsThere are 3 steps in a basic Netflix signup. We refer to these steps that comprise a user journey as a signup flow. Each step of the flow serves a distinct purpose.Introduction and account creationHighlight our value propositions and begin the account creation process.Plans &amp; offersHighlight the various types of Netflix plans, along with any potential offers.PaymentHighlight the various payment options we have so customers can choose what suits their needs best.The primary focus for the remainder of this post will be step 2: plans &amp; offers. In particular, we’ll define plans and offers, review the legacy architecture and some of its shortcomings, and dig into our new architecture and some of its advantages.Plans &amp; OffersDefinitionsLet’s define what a plan and an offer is at Netflix. A plan is essentially a set of features with a price.An offer is an incentive that typically involves a monetary discount or superior product features for a limited amount of time. Broadly speaking, an offer consists of one or more incentives and a set of attributes.When we merge these two concepts together and present them to the customer, we have the plan selection page (shown above). Here, you can see that we have 3 plans and a 30-day free trial offer, regardless of which plan you choose. Let’s take a deeper look at the architecture, protocols, and systems involved.Legacy ArchitectureAs previously mentioned, Netflix has had a relatively static set of plans and offers since the inception of streaming. As a result of this simple product offering, the architecture was also quite straightforward. It consisted of a small set of XML files that were loaded at runtime and stored in local memory. This was a perfectly sufficient design for many years. However, there are some downsides as the company continues to grow and the product continues to evolve. To name a few:Updating XML files is error-prone and manual in nature.A full deployment of the service is required whenever the XML files are updated.Updating the XML files requires engaging domain experts from the backend engineering team that owns these files. This pulls them away from other business-critical work and can be a distraction.A flat domain object structure that resulted in client-side logic in order to extract relevant plan and offer information in order to render the UI. For example, consider the data structure for a 30 day free trial on the Basic plan.{ &quot;offerId&quot;: 123, &quot;planId&quot;: 111, &quot;price&quot;: &quot;$8.99&quot;, &quot;hasSD&quot;: true, &quot;hasHD&quot;: false, &quot;hasFreeTrial&quot;: true, etc…}As the company matures and our product offering adapts to our global audience, all of the above issues are exacerbated further.Below is a visual representation of the various systems involved in retrieving plan and offer data. Moving forward, we’ll refer to the combination of plan and offer data simply as SKU (Stock Keeping Unit) data.New ArchitectureIf you recall from our previous blog post, Growth Engineering owns the business logic and protocols that allow our UI partners to build lightweight and flexible applications for almost any platform. This implies that the presentation layer should be void of any business logic and should simply be responsible for rendering data that is passed to it. In order to accomplish this we have designed a microservice architecture that emphasizes the Separation of Concerns design principle. Consider the updated system interaction diagram below:There are 2 noteworthy changes that are worth discussing further. First, notice the presence of a dedicated SKU Eligibility Service. This service contains specialized business logic that used to be part of the Orchestration Service. By migrating this logic to a new microservice we simplify the Orchestration Service, clarify ownership over the domain, and unlock new use cases since it is now possible for other services not shown in this diagram to also consume eligible SKU data.Second, notice that the SKU Service has been extended to a platform, which now leverages a rules engine and SKU catalog DB. This platform unlocks tremendous business value since product-oriented teams are now free to use the platform to experiment with different product offerings for our global audience, with little to no code changes required. This means that engineers can spend less time doing tedious work and more time designing creative solutions to better prepare us for future needs. Let’s take a deeper look at the role of each service involved in retrieving SKU data, starting from the visitor’s device and working our way down the stack.Step 1 — Device sends a request for the plan selection pageAs discussed in our previous Growth Engineering blog post, we use a custom JSON protocol between our client UIs and our middle-tier Orchestration Service. An example of what this protocol might look like for a browser request to retrieve the plan selection page shown above might look as follows:GET /plans{  “flow”: “browser”,  “mode”: “planSelection”}As you can see, there are 2 critical pieces of information in this request:Flow — The flow is a way to identify the platform. This allows the Orchestration Service to route the request to the appropriate platform-specific request handling logic.Mode — This is essentially the name of the page being requested.Given the flow and mode, the Orchestration Service can then process the request.Step 2 — Request is routed to the Orchestration Service for processingThe Orchestration Service is responsible for validating upstream requests, orchestrating calls to downstream services, and composing JSON responses during a signup flow. For this particular request the Orchestration Service needs to retrieve the SKU data from the SKU Eligibility Service and build the JSON response that can be consumed by the UI layer.The JSON response for this request might look something like below. Notice the difference in data structures from the legacy implementation. This new contextual representation facilitates greater reuse, as well as potentially supporting offers other than a 30 day free trial:{  “flow”: “browser”,  “mode”: “planSelection”,  “fields”: {    “skus”: [      {        “id”: 123,        “incentives”: [“FREE_TRIAL”],        “plan”: {          “name”: “Basic”,          “quality”: “SD”,          “price” : “$8.99”,          ...        }        ...      },      {        “id”: 456,        “incentives”: [“FREE_TRIAL”],        “plan”: {          “name”: “Standard”,          “quality”: “HD”,          “price” : “$13.99”,          ...        }        ...      },      {        “id”: 789,        “incentives”: [“FREE_TRIAL”],        “plan”: {          “name”: “Premium”,          “quality”: “UHD”,          “price” : “$17.99”,          ...        }        ...      }    ],    “selectedSku”: {      “type”: “Numeric”,      “value”: 789    }    &quot;nextAction&quot;: {      &quot;type&quot;: &quot;Action&quot;      &quot;withFields&quot;: [        &quot;selectedSku&quot;      ]    }  }}As you can see, the response contains a list of SKUs, the selected SKU, and an action. The action corresponds to the button on the page and the withFields specify which fields the server expects to have sent back when the button is clicked.Step 3 &amp; 4 — Determine eligibility and retrieve eligible SKUs from SKU Eligibility ServiceNetflix is a global company and we often have different SKUs in different regions. This means we need to distinguish between availability of SKUs and eligibility for SKUs. You can think of eligibility as something that is applied at the user level, while availability is at the country level. The SKU Platform contains the global set of SKUs and as a result, is said to control the availability of SKUs. Eligibility for SKUs is determined by the SKU Eligibility Service. This distinction creates clear ownership boundaries and enables the Growth Engineering team to focus on surfacing the correct SKUs for our visitors.This centralization of eligibility logic in the SKU Eligibility Service also enables innovation in different parts of the product that have traditionally been ignored. Different services can now interface directly with the SKU Eligibility Service in order to retrieve SKU data.Step 5 — Retrieve eligible SKUs from SKU PlatformThe SKU Platform consists of a rules engine, a database, and application logic. The database contains the plans, prices and offers. The rules engine provides a means to extract available plans and offers when certain conditions within a rule match. Let’s consider a simple example where we attempt to retrieve offers in the US.Keeping the Separation of Concerns in mind, notice that the SKU Platform has only one core responsibility. It is responsible for managing all Netflix SKUs. It provides access to these SKUs via a simple API that takes customer context and attempts to match it against the set of SKU rules. SKU eligibility is computed upstream and is treated just as any other condition would be in the SKU ruleset. By not coupling the concepts of eligibility and availability into a single service, we enable increased developer productivity since each team is able to focus on their core competencies and any change in eligibility does not affect the SKU Platform. One of the core tenets of a platform is the ability to support self-service. This negates the need to engage the backend domain experts for every desired change. The SKU Platform supports this via lightweight configuration changes to rules that do not require a full deployment. The next step is to invest further into self-service and support rule changes via a SKU UI. Stay tuned for more details on this, as well as more details on the internals of the new SKU Platform in one of our upcoming blog posts.ConclusionThis work was a large cross-functional effort. We rebuilt our offers and plans from the ground up. It resulted in systems changes, as well as interaction changes between teams. Where there was once ambiguity, we now have clearly defined ownership over SKU availability and eligibility. We are now capable of introducing new plans and offers in various markets around the globe in order to meet our customer’s needs, with minimal engineering effort.Let’s review some of the advantages the new architecture has over the legacy implementation. To name a few:Domain objects that have a more reusable and extensible “shape”. This shape facilitates code reuse at the UI layer as well as the service layers.A SKU Platform that enables product innovation with minimal engineering involvement. This means engineers can focus on more challenging and creative solutions for other problems. It also means fewer engineering teams are required to support initiatives in this space.Configuration instead of code for updating SKU data, which improves innovation velocity.Lower latency as a result of fewer service calls, which means fewer errors for our visitors.The world is constantly changing. Device capabilities continue to improve. How, when, and where people want to be entertained continues to evolve. With these types of continued investments in infrastructure, the Growth Engineering team is able to build a solid foundation for future innovations that will allow us to continue to deliver the best possible experience for our members.Join Growth Engineering and help us build the next generation of services that will allow the next 200 million subscribers to experience the joy of Netflix.Growth Engineering at Netflix- Creating a Scalable Offers Platform was originally published in Netflix TechBlog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-02-09 17:27:17","link":"https://netflixtechblog.com/growth-engineering-at-netflix-creating-a-scalable-offers-platform-69330136dd87?source=rss----2615bd06b42e---4","blog":{"id":"netflix","link":"https://netflixtechblog.com","name":"Netflix","rssFeed":"https://netflixtechblog.com/feed","type":"company"},"blogType":"company"},{"id":"90471658ee37795960867ebdeafdec25","publish_timestamp":1615919383,"title":"Inside Connect: Supporting Apprentices as an Engineering Leader","blogName":"Airbnb","image":"https://miro.medium.com/max/1200/1*HkaDxErvc_pvLdQpUOdkiA.jpeg","categories":["people","hiring","apprenticeship","engineeringmanagement"],"description":"The third of a three-part blog series on Connect, an apprenticeship opportunity for aspiring Airbnb software engineers.As an engineering leader with 15+ years of industry experience, I am currently head of engineering for the Marketing Technology Platform at Airbnb. Our goal is to build world-class technology systems and tools that power the unique blend of performance and brand marketing strategies at Airbnb.In this post, I’d like to share why fostering talent from the Connect program helps me achieve this vision for my teams. Furthermore, I will share from my experience — with Connect and other apprenticeship programs — what good support looks like. Programs like this don’t happen in a vacuum, they require engineering leaders and contributors from around the company to step up and help make them successful.To learn more about the goals and structure of Connect, see Part 1, or read Part 2 to hear about the program from an apprentice’s perspective.IntroductionI was born and raised in Kenya and moved to the US to pursue higher education and opportunities in the technology industry. I have seen first hand what education can do to further someone’s life. Unfortunately, not everyone has access to the same type of education or encouragement towards STEM careers due to various reasons, many of which arise from the biases and inequity that continue to affect our society.I never considered myself a minority until I moved to the US. Unfortunately, academic excellence does not always equate to career opportunities or growth for underrepresented talent. Underrepresented groups normally have to work much harder for the same opportunities. I have made it a personal goal, to help in my own small way, to level the playing field for as many people as possible. I do this through helping with programs that provide access to education to kids from underrepresented backgrounds, mentorship programs, and career prep programs.At Airbnb, we have a mission to create a world where anyone can belong anywhere. One of the ways we achieve this is through programs like Connect that enable individuals from non-traditional technical backgrounds to get a chance at a career at Airbnb. Good talent is very hard to get, especially those from underrepresented groups, if we stick to traditional recruiting and hiring methods. To increase representation and strive to achieve a workforce that is reflective of the diversity of Airbnb’s community, we need to find the best talent wherever they might be. It is only by creating teams that adequately reflect the diversity of our community that we can possibly achieve a best-in-class product and growth of Airbnb’s businesses.I have had the privilege to be involved with career prep programs at Microsoft (Microsoft Leap Apprenticeship), Uber (Hidden Genius), and Airbnb (Connect) and have witnessed their success firsthand as a mentor and as a manager.The remainder of this post will focus on my role, as an engineering leader, in the Connect program at Airbnb and what it means to offer my support both at the program level and directly to apprentices. The following summarizes my role before the apprentices joined my teams, during the team placement period, and after the initial cohort wrapped up.Before Apprentices ArrivedVolunteering to Host Apprentices: Having seen the impact of such programs at former workplaces, I was quick to advocate for my Marketing Technology Engineering teams to be involved with the first Connect cohort. Furthermore, Airbnb has a core value we refer to as “Be a Host” — care for others and make them feel like they belong; encourage others to participate to their fullest; listen, communicate openly, and set clear expectations. I saw this as an opportunity to embody this value as an engineering organization, so I welcomed two apprentice engineers to my teams. I introduced the program to all team members and asked for their help to welcome the new apprentices. Doing this in 2020 was definitely challenging, as everyone was remote, so we had to get creative.Selecting Team Buddies: This is an extremely important part of the program. Assigning the right buddy to each apprentice is key to their success, as buddies play an overwhelmingly pivotal role providing technical guidance and coaching. I identified seasoned senior engineers on my team who were looking for opportunities to develop their technical leadership skills.Scoping a Project: Apprentice engineers would be joining the team for only a handful of months and during that time would work on a project, with guidance from their team buddy. I worked with buddies and managers to identify a project with the right scope and complexity. Providing the opportunity to learn and develop, while still delivering on some business impact, is a delicate balance. And on top of that, I wanted to find something they could be proud of and excited about — ideally a small feature that ends up in production.Contributing to the Career Framework: Engineering rubrics are a great tool for managing development and performance of engineers by setting clear growth and development expectations. With Connect’s goals of expanding access to engineering roles at Airbnb, we added a new level specifically for talent from the Connect program. I worked with the talent team and other engineering managers and leaders to contribute to updating our engineering development framework with clear guidelines on expectations for the new level across all engineering traits.During the Team PlacementDefining Success: From the beginning, I set clear expectations and goals with the apprentice engineers, their managers, and their team buddies. This covered the task and project plan for their time on our team, the evaluation criteria, milestones, and what success looks like. I was completely transparent with everyone involved, that the program would be a learning experience, and though apprentices may require significant guidance at times, we would be evaluating them on their ability to learn and grow.Holding Weekly Check-ins: I conducted skip 1–1s with the apprentice engineers to provide coaching, gauge progress, give and get feedback, and check-in with them more generally on work and life. In 2020, it was especially important to be intentional with 1–1s as they offered an opportunity for connection that all of us were yearning for while working remotely.Providing a Midpoint Review: I worked with managers and buddies to provide midpoint feedback to the apprentices on what was going well as well as areas for continued development. We also calibrated across all the 10 apprentices engineers in the first cohort to make sure they were having consistent experiences. Our primary goal was to make sure the apprentice engineers were successful. By providing transparency and continuous feedback, we aimed for no surprises — give apprentices the information they need to grow and develop their skills.Evaluating Final Performance: After the two-month team placement, the apprentice engineers received a performance evaluation done by their team buddies and managers. Based on how they had performed and grown during their time on the team, they would have a chance to convert to full-time software engineers and start their career journey with Airbnb. Both apprentice engineers in our teams performed extremely well, and we were thrilled to offer them full-time positions at the end of the program.After the Connect ProgramFully Integrating into the Team: After starting as full-time software engineers, I worked with their managers to create personalized career development plans for them so we could continue to support their growth as software engineers. We started the engineers with frontend development with a plan to grow them into full stack engineers. They both continued to thrive and received glowing performance reviews EOY 2020. When engineers are starting out, I always advise them to go for breadth — explore various skills like frontend, backend, native, and data engineering so that they can be exposed to many options. Then, later in their careers, they will be in a better position to identify what they would like to specialize in.Supporting Growth of the Connect Program: From the start of my participation with the Connect program, I saw an opportunity to leverage my previous experiences with apprenticeships and contribute to shaping the future of the program. I continue to be a trusted thought partner working closely with the Connect team to provide feedback and insight that will continue to improve the program.The OutcomeI am proud to say that the Connect program was successful and all 10 apprentice engineers in the first cohort received full time offers and placement in various teams at Airbnb. This was a win-win in so many aspects. The team buddies benefited by growing their technical leadership and mentoring skills; this is expected of our senior engineers. The teams were able to get new team members that were equal contributing members and enable them to further their roadmaps. And most importantly, Airbnb gained 10 new amazing engineers that enabled Airbnb to make forward progress on the company’s mission of belonging!Inside Connect: Supporting Apprentices as an Engineering Leader was originally published in Airbnb Engineering &amp; Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-03-16 18:29:43","link":"https://medium.com/airbnb-engineering/inside-connect-supporting-apprentices-as-an-engineering-leader-the-third-of-a-three-part-blog-ef2e631b4899?source=rss----53c7c27702d5---4","blog":{"id":"airbnb","link":"https://medium.com/airbnb-engineering","name":"Airbnb","rssFeed":"https://medium.com/feed/airbnb-engineering","type":"company"},"blogType":"company"},{"id":"8b1d45656b25ed6385adae529dcef605","publish_timestamp":1615316408,"title":"Inside Connect: An Apprentice Perspective","blogName":"Airbnb","image":"https://miro.medium.com/max/1200/1*QVRM956xCulND7LaC_Rhew.jpeg","categories":["people","airbnb","softwaredevelopment","apprenticeship","careerchange"],"description":"The second of a three-part blog series on Connect, an apprenticeship opportunity for aspiring Airbnb software engineers.Connect 2020 Apprentices at Airbnb HQHi, I’m Ale. I was a Connect 2020 Apprentice and I’m going to write a bit about my experience with the program, especially for those who are interested in applying. To learn more about the goals and structure of the Connect program, take a look at Part 1 of this blog series.How It StartedI first heard about Connect from a friend in October 2019. She was planning on applying and encouraged me to do the same. We had both just graduated from a coding bootcamp and were starting to venture into the job search. At a glance, Connect presented an invaluable opportunity for professional growth so I decided to take a shot. I was mainly attracted to the fact that the apprenticeship would allow me to relearn some concepts that were covered very quickly during my bootcamp. It was an extended learning period coupled with on-the-job training and felt like an ideal next step for my career transition.About a month after I sent in my application and following a few interviews, I was accepted into the program. In fact, my friend and I both made it in. That was the best news I received in all of 2019.The First Few MonthsWhen the program finally started in January 2020, I came into Airbnb along with a cohort of 9 other Connect apprentices. We would all be working together to get through the program and relying on each other quite a bit as we explored our new role. We took a deep dive into the Connect curriculum for the first few months of the program while sitting in a corner of one of the buildings on Airbnb’s San Francisco campus.The curriculum was very hands-on. We worked on code samples to put into practice languages that were new to us — for me, this was Java — or to demonstrate existing familiarity with another language or framework. The material covered Airbnb-specific languages, libraries, testing philosophies, IDEs, design patterns, and more. Our first few months of the program consisted of semi-independently completing readings, submitting PRs, attending lectures, and presenting during knowledge sharing sessions on the Airbnb tech stack. We marked our technical growth and progress in the form of a bimonthly review where we received plenty of feedback on what was going well and how to get better.We had several lectures and conversations with a variety of engineers in the company who came ready to answer all our questions on the subject of professional growth and career development. They talked to us about both technical and non-technical subjects. I thought it spoke very well of the company that its employees took time out of their days to teach us Rails basics or explain service-oriented architecture. Also, we had a lot of good food.The Team PlacementTeam placement was an opportunity to start writing code for real Airbnb codebases and experience the highly collaborative team environment. I won’t forget how cool it was to realize that I’d be making small changes to a product that I loved using in the real world.We were mainly supported by a team buddy on our new teams, and they would oversee our tasks and projects. A huge shoutout to my amazing team buddy, Oliver! Team buddies were key to introducing us to what work was like on an actual team. When team placement came, we now had to understand our team’s specific corners of the codebase.I spent the first two weeks of team placement reading team documentation, getting more familiar with the product, and figuring out all the ways to find answers to my questions. The initial tasks were well scoped out, and I got a ton of help from Oliver and other teammates. Since I ended up in a frontend role, I focused on getting better at React, TypeScript, writing unit tests, and understanding our custom-built components. I also dipped my toes into the Slack world and followed all the channels where people readily responded to my questions. As the weeks flew by, the most daunting part of the whole program got closer: the final project.Final projects were a way for us to demonstrate all that we had learned and make a meaningful contribution to the team. We were not expected to come up with our own projects, but I liked that I was given a choice between a few different things when the time came. I ended up choosing a testing project because I wanted to learn more about testing. The goal was to write the first set of integration tests for the web application that my team owned, using Cypress as a testing framework. I was given the opportunity to go figure out what kind of configuration we would need in order to write the tests, as Cypress was relatively new in the company. I found a handful of engineers from different teams, who had more information about how to set up and use this framework, and asked them many questions. I found that all the people outside of my team that I interacted with, regardless of their awareness of the apprenticeship, were both helpful and kind to me.After a few weeks of gathering information and implementing tests, I got the opportunity to present the project in front of a large audience via Zoom, to chat about how the tests worked and what their impact was on the team’s goals. I was nervous to talk about technical things in front of a technical audience, but the Connect team prepared me well with a couple of rehearsal and feedback sessions.Post-ApprenticeshipGetting the offer to convert into a full-time software engineering role was even more rewarding than completing the final project, especially when I found out that all ten of the apprentices would be returning to Airbnb.At this point, it’s worth mentioning that since the start of the pandemic we had to adjust the way we work and the tools we use. As my first time working with a highly collaborative team, I’ve definitely missed out on some important parts of the team experience. For one, I imagine that information would be even more accessible if I was sitting next to the people I work with daily. More than anything, I wish that I had time to hang out and get lunch with other apprentices and my new teammates, because the people at Airbnb are generally very interesting. Even with the challenges that working remotely presents, I’ve found that we have good systems and tools in place to help us work successfully, and my team has made me feel well-supported. I look forward to when my teammates and I return to the office and can share those in-person moments of connection.Six months into my new role, I’m very happy that I applied to the Connect apprenticeship, and I’m motivated to work hard and keep learning every day.*All trademarks are the properties of their respective owners. Any use of these are for identification purposes only and do not imply sponsorship or endorsement.Inside Connect: An Apprentice Perspective was originally published in Airbnb Engineering &amp; Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-03-09 19:00:08","link":"https://medium.com/airbnb-engineering/inside-connect-an-apprentice-perspective-c9f299e11e51?source=rss----53c7c27702d5---4","blog":{"id":"airbnb","link":"https://medium.com/airbnb-engineering","name":"Airbnb","rssFeed":"https://medium.com/feed/airbnb-engineering","type":"company"},"blogType":"company"},{"id":"7a63ddf150872ab9018689987b55143a","publish_timestamp":1614711577,"title":"Inside Connect: Airbnb’s Engineering Apprenticeship Program","blogName":"Airbnb","image":"https://miro.medium.com/max/1200/1*P4rfUgxvMFE_5yaR5Wv8kw.jpeg","categories":["engineering","apprenticeship","people"],"description":"The first of a three-part blog series on Connect, an apprenticeship opportunity for aspiring Airbnb software engineers.by Beti Gathegi &amp; Jacqui WattsHello! We’re excited that you’re here to learn more about the Connect program. Before we share details, though, we’d like to introduce ourselves. We’re Beti Gathegi and Jacqui Watts — a team of two whose purpose at Airbnb is to help bring this program to life. We couldn’t be more proud of the apprentices that we’ve gotten to work with to-date, and that’s why we want to share more, so that we can continue to attract even more amazing software engineering talent in the future.In this post, we’ll focus on the goals and structure of the program. Then, in upcoming posts, you’ll get the chance to hear directly from one of the apprentices, now full-time engineer, from our 2020 cohort, in addition to an engineering manager who supported an apprentice. We hope this series gives you clarity around what the Connect program is all about, how it differs from other programs, and hopefully inspires you to spread the word to those who might be interested in joining or supporting the program in some way.About ConnectThe Connect engineering apprenticeship program is a six-month program that gives individuals from non-traditional technical backgrounds an onramp to a potential career at Airbnb. At Airbnb, we believe in self-starters and know that people can be successful in tech regardless of their background. But the reality is barriers exist that make it hard for people from non-traditional technical backgrounds to break into professional engineering jobs. So we set out to create a new opportunity specifically focused on this group of talent. Ultimately, Connect is one more way we can fulfill Airbnb’s mission of creating a world where anyone can belong anywhere.Originally piloted back in 2016, Connect was brought back this past year with a dedicated team to help grow the program and double down on our commitment to expanding access to software engineering positions. With that goal in mind, we crafted a program to launch in January 2020. Little did we know what the year would have in store for us!Despite these challenges, we are proud to share that the inaugural round of the program was a success, and we are looking to continue bringing in new non-traditional engineering talent through the Connect program moving forward.Our PartnersWe’re proud to partner with a number of local Bay Area organizations — such as TechSF, Kapor Center’s Pathways to Tech Initiative (formerly TechHire Oakland), Code Tenderloin, dev/mission, The Hidden Genius Project, and Hack Reactor’s Telegraph Track. These organizations are doing a fantastic job supporting talent from non-traditional training backgrounds who are a great fit for our apprentice roles. Although we’re planning to keep our Connect 2021 local to our San Francisco HQ again in 2021, we envision Connect growing to support talent needs for other Airbnb locations in the future.The Program StructureOver the course of six months, the Connect program aims to prepare apprentices to contribute to engineering teams across the company. We know that folks who are newly transitioning to software engineering are still learning and growing their technical skills, so we invest time in training and mentorship before they join a team. But what does that really look like?TrainingThere are numerous non-traditional paths to learning technical skills — from coding bootcamps to community college to self-study regimens. Even within different software development bootcamps, the languages and technologies learned will inevitably be different. We spend the first several months of the program supporting apprentices in getting up to speed on Airbnb’s preferred tools and tech stack, what we call the “paved road”. Furthermore, this training period builds exposure to what codebases look like at scale. Anyone who’s done the non-traditional path — or really any path into software engineering, for that matter — can tell you that navigating a large production codebase with millions of lines of code is one of the most challenging parts!The training period puts a heavy emphasis on projects and hands-on coding assignments, while also incorporating some more traditional classroom learning. And though apprentices are guided by a technical instructor throughout the training period, they also get to learn from dozens of engineers from around the company who are thrilled to support them in developing their technical skills.MentorshipTechnical support isn’t the only thing we focus on during Connect. For many, this may be the first time working in the tech industry, so having a mentor to help them navigate the transition has been enormously helpful. A Connect mentor is a well-established member of Airbnb’s engineering team who serves as a non-evaluative partner, providing guidance in everything from technical skills to how to build relationships around the company. Mentors are a huge asset to the apprentice experience.Team PlacementIn the latter half of the six-month Connect program, apprentices join different engineering teams from around the company. During this team placement period, they work on real tasks and project work, with scoping, pairing, and code review support from a dedicated team buddy. Projects that apprentices have the opportunity to contribute to can vary widely from team to team and may include frontend UI features, backend API changes, or internal tools. In all cases, the team placement provides a chance for apprentices to hone their technical skills, side-by-side with Airbnb engineers, and demonstrate their ability to contribute to the team.Figure 1. An example of an apprentice’s project — implementing design changes for an embed modal.Opportunity for a Full-time RoleSo what happens at the end of the six months? Apprentices may have the opportunity to join the team full-time! This decision takes into account business need and performance during the apprenticeship period. This isn’t just a program for educational exposure, the Airbnb Connect program has the potential to lead into a full-time engineering role. We love being able to watch Connect alum continue to grow their career and develop their software engineering talent here at Airbnb.Moving ForwardWe’re thrilled to continue offering and growing the Connect program in 2021 and beyond and can’t wait to see all that our apprentices are able to accomplish. Furthermore, we are excited about the partnerships we’ve built to help make that happen. Applications for the Connect 2021 round open on March 8, please check out the Airbnb Careers page to apply.Stay tuned for upcoming posts in this series to get a peek into the world of Connect from a former apprentice.Inside Connect: Airbnb’s Engineering Apprenticeship Program was originally published in Airbnb Engineering &amp; Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-03-02 18:59:37","link":"https://medium.com/airbnb-engineering/inside-connect-airbnbs-engineering-apprenticeship-program-c26d6eb2768c?source=rss----53c7c27702d5---4","blog":{"id":"airbnb","link":"https://medium.com/airbnb-engineering","name":"Airbnb","rssFeed":"https://medium.com/feed/airbnb-engineering","type":"company"},"blogType":"company"},{"id":"b307ca49a02bf5cf2ea2f5afcc51f470","publish_timestamp":1614103730,"title":"Visualizing Data Timeliness at Airbnb","blogName":"Airbnb","image":"https://miro.medium.com/max/1200/0*TQFEQRjueA0OLXpV","categories":["design","airbnb","data","visualization","analytics"],"description":"by Chris Williams, Ken Chen, Krist Wongsuphasawat, and Sylvia TomiyamaImagine you are a business leader ready to start your day, but you wake up to find that your daily business report is empty — the data is late, so now you are blind.Over the last year, multiple teams came together to build SLA Tracker, a visual analytics tool to facilitate a culture of data timeliness at Airbnb. This data product enabled us to address and systematize the following challenges of data timeliness:When should a dataset be considered late?How frequently are datasets late?Why is a dataset late?This project is a critical part of our efforts to achieve high data quality and required overcoming many technical, product, and organizational challenges in order to build. In this article, we focus on the product design: the journey of how we designed and built data visualizations that could make sense of the deeply complex data of data timeliness.Yes, Data Can Be LateTo avoid blinding the business, it is critical to deliver data in a timely manner. However, this can be difficult to do because the journey from data collection to final data output typically requires many steps. At Airbnb — and anywhere with large-scale data processing pipelines — “raw” datasets are cleaned up, merged, and transformed into structured data. Structured data then powers product features and enables analytics to inform business decisions.To ensure timeliness of the final output data, we aim to have owners of each intermediate step commit to Service Level Agreements (SLAs) for the availability of their data by a certain time. For example, the dataset owner promises that the “bookings” metric will have the latest data by 5 AM UTC, and if it is not available by this time, it is considered “late.”How Often Are My Datasets Late?As a first step, we set out to enable data producers to understand when data is landing and how frequently they meet SLAs in the Report view (Figure 1). In this view, producers can track real-time and historical trends across multiple datasets they own or care about. We also ensured that producers get value even when there is no formal SLA set by surfacing typical landing times. No SLAs were set when we first launched the tool, and SLAs may be unneeded for datasets that are not widely consumed.Figure 1 SLA Report view provides a high-level overview of SLA performance across lists of datasets. Each row includes a status indicator of the latest data partition and bar charts that display historical landing times (red bars show the days when datasets missed SLAs).The Report view makes use of traditional lists of data entities, with embedded small visuals that concisely summarize typical and historical landing time data. Data producers can organize their datasets across lists and collaborate on lists with others (e.g., their team).With this data-rich summary, understanding landing times and SLA performance became as simple as curating a list of datasets.Reporting Is the Tip of the IcebergWhile the Report view dramatically simplified understanding whether a dataset was late, it did not address two major challenges of SLAs:What is a reasonable SLA for a dataset?When a dataset is late, how do you understand why?These questions are challenging because datasets are not independent from each other. Instead, datasets are derived stepwise in a specific sequence, where one or more transformations must happen before another (Figure 2).Figure 2 An example of data lineage for creating dataset “A.” “A” depends on “B,” which depends on “C” and “D” and so on.Thus, the availability of one dataset is intrinsically linked to a complex hierarchical “lineage” of data ancestors. To set a realistic SLA for a dataset, one has to take into account its entire dependency tree — sometimes comprising 100s of entities — and their SLAs.To add to the complexity, when things go wrong, trying to match up the hierarchical dependencies with the temporal sequence makes SLA misses hard to reason about without visual aid. Existing tooling at Airbnb enabled data engineers to identify problems within their own data pipeline, but it was exponentially more difficult to do this across pipelines which are often owned by different teams.Why Is My Dataset Late?To enable data producers to identify the root cause(s) of an SLA hit or miss across data pipelines, and to set realistic SLAs by taking into account full data lineage, we designed the Lineage view.Early Design AttemptTo be successful, the Lineage view needed to enable data producers to reason about both dataset dependencies and the timelines of those dependencies. Since data lineages can include 10s — 100s of tables, each with 30 days of historical data, SLAs, and relationships between them all, we needed to concisely represent on the order of 1,000s — 10,000s of individual data points.In our initial explorations, we heavily emphasized lineage over landing time sequence (Figure 3). Although it was easy to understand dependencies for small lineages, it failed to highlight which dependencies caused delays in the overall pipeline on a given run, and it was difficult to understand where time was spent to produce the dataset overall.Figure 3 Early exploration with emphasis on dataset lineage. Each box shows historical landing times for each dataset in the larger data pipeline.Focus On Time With the Timeline ViewWe then pivoted to emphasizing temporal sequence over lineage. To do this, we designed a dependency-inclusive gantt chart (Figure 4) with the following features:Each row represents a dataset in the lineage, with the “final” dataset of interest at the top.Each dataset has a horizontal bar representing the start, duration, and end time for its data processing job on the date or time selected.If a dataset has an SLA, it is indicated with a vertical line.Distributions of typical start and end times are marked to help data producers evaluate if the data processing job is ahead of schedule, or behind, putting its downstream datasets at risk.Arcs are drawn between parent and child datasets so data producers can trace the lineage and see if delays are caused by upstream dependencies.Emphasized arcs represent the most important “bottleneck” path (described below).Figure 4 The Timeline view gives a clear sense of sequence and the duration of data transformations, while preserving important hierarchical dependencies which provide sequence context. Historical landing times are displayed for each dataset row, to the left of the current run.With this design, it became easy to find the problematic step — often the long, red bar — or to identify system-wide delays, where all steps just took longer than usual (lots of yellow bars, each past their typical landing time). This visualization is used by many teams today at Airbnb to debug data delays.Finding the Needle in the Haystack — “Bottlenecks”For datasets with very large dependency trees, we found it difficult to find the relevant, slow “bottleneck” steps that delay the entire data pipeline. We were able to drastically reduce noise and highlight these problematic datasets by developing the concept of a “bottleneck” path — the sequence of the latest-landing data ancestors which prevented a child data transformation from starting, thus delaying the entire pipeline (Figure 5).Figure 5 Comparison of a full data lineage (left, n=82) versus just the filtered “bottleneck” path (right, n=8). Bottleneck paths significantly improve signal-to-noise, and make it easy to find problematic steps of large data pipelines possible.Is it Me or You? Diving into the Historical ViewOnce the bottleneck step was identified, the next important question became whether the delay in that step was due to long runtimes or delays in upstream dependencies. This helps data producers understand whether they need to optimize their own pipeline or instead negotiate with owners of upstream datasets for earlier SLA’s. To enable this, we built a view of the detailed historical runtimes of a single dataset, showing both when they ran, and the duration (Figure 6).Figure 6 Historical runtimes and duration distributions quickly distinguish whether SLA misses (red) are due to starting late (top) versus long runtime durations (bottom).By combining these complementary views in SLA Tracker, we were able to provide a full perspective of data timeliness (Figure 7).Figure 7 SLA Tracker is comprised of multiple views. The Report view provides an overview of dataset status, the Lineage view enables root cause analysis of data landing times, and the Historical view captures historical trends in detail.Process and ToolingWe spent roughly 12 months conceptualizing, designing, prototyping, and productionizing SLA Tracker. Much of this time was spent developing the data APIs that power the UI, and iterating on the Lineage view.For the simpler Report view, we leveraged static designs and click-through prototypes with generic mock data. Throughout alpha and beta product releases, we iterated on visual language and made data more visual to improve comprehension (Figure 8).Figure 8 Evolution of making the “current vs typical” landing time visual.We used an entirely different approach in designing the Lineage view. Its information hierarchy is dictated by the shape of the data, which makes prototyping with real data samples critical. We built these prototypes in TypeScript using our low-level visx visualization component suite for React, which allows for partial code reuse during productionization (Figure 9).Figure 9 Evolution of the Lineage Page gantt chart (left to right): early multi-run summary boxplots; single run tracks with dependency arcs; “bottleneck” simplification.After we were confident in our visualization, we refined the visual elements in static mocks in Figma before productionizing (Figure 10).Figure 10 Developing a simple but coherent design language (left) across SLA Tracker views (right) helped balance information density by making elements more quickly understandable.ConclusionIn this project, we have applied data visualization and UI/UX design — the interdisciplinary craft we refer to as “Data Experience” — to important data timeliness problems that require deep understanding of complex temporal and hierarchical information. This has enabled us to make data timeliness insights accessible, even in the complex data ecosystem of a large-scale company. It requires time and iteration to develop sophisticated visual analytics tools but the resulting product can provide great value to your organization.Acknowledgements ❤️SLA Tracker was the culmination of the efforts of many people and teams. While we focus on the data visualization aspect in this article, there were other important challenges we had to overcome in order to make the analytical tool possible. Thanks to the entire team who worked on the frontend, backend, and data engineering to make this product possible: Conglei Shi, Erik Ritter, Jiaxin Ye, John Bodley, Michelle Thomas, Serena Jiang, Shao Xie, Xiaobin Zheng, and Zuzana Vejrazkova.All trademarks are the property of their respective owners. Any use of these are for identification purposes only and do not imply sponsorship or endorsement.Visualizing Data Timeliness at Airbnb was originally published in Airbnb Engineering &amp; Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-02-23 18:08:50","link":"https://medium.com/airbnb-engineering/visualizing-data-timeliness-at-airbnb-ee638fdf4710?source=rss----53c7c27702d5---4","blog":{"id":"airbnb","link":"https://medium.com/airbnb-engineering","name":"Airbnb","rssFeed":"https://medium.com/feed/airbnb-engineering","type":"company"},"blogType":"company"},{"id":"694ff612ac90ce38c009651050ae5958","publish_timestamp":1612893750,"title":"Supercharging Apache Superset","blogName":"Airbnb","image":"https://miro.medium.com/max/1200/1*ZspvzFiFizHwcPzN09h6sw.png","categories":["opensource","scalability","data","datascience","businessintelligence"],"description":"How Airbnb customized Superset for business intelligence at scaleBy: Erik Ritter, Grace Guo, Jesse Yang, John Bodley, and Zuzana VejrazkovaIntroductionAt Airbnb, many employees rely on data every day to do their jobs. While several different tools are used for analysis, at the core of Airbnb’s self-serve business intelligence (BI) solution is Apache Superset™ (“Superset”).Superset is an open-source data exploration and visualization platform designed to be visual, intuitive, and interactive. It enables users to analyze data using its SQL editor, and easily build charts and dashboards. It began at Airbnb in 2015 as a hackathon project, was open sourced in 2016, and joined the Apache Incubator program in May 2017. After nearly four years of incubating at Apache, on January 21st, 2021, the Apache Software Foundation announced Superset as a top-level project. This coincided with the official release of Superset 1.0, the first major version and a turning point for the project.As Superset progressed, Airbnb has remained a consistent contributor to the project. In the past, we discussed how this journey started and the introduction of new product features. This post will cover high-level technical details on how we scaled Superset to a mature BI tool that supports enterprise use cases and how working with the community enabled us to build custom integrations with our other enterprise tools and systems.By the NumbersAs the longest-running Superset environment, Airbnb has spent the last five years working closely with open-source contributors to build a product that scales and grows with the company. This has culminated in the ability to enable staggering amounts of data-driven intelligence. Superset at Airbnb handles on a weekly basis around:2,000 users50,000 SQL Lab queries6,000 and 125,000 dashboard and chart views† respectively† Views are defined as unique (day, user, entity) tuples and chart views encompass both dashboard and explorer surfaces.Our ecosystem now comprises more than 100,000 tables and virtual datasets backing over 200,000 charts and 14,000 dashboards.All this analysis, slicing and dicing, and decision making was performed by users across many job functions at Airbnb; over 25% of the company uses Superset on a weekly basis.How We Scaled SupersetTo support Airbnb’s scale, we built several custom features in and around Superset. These configuration options, daily offline jobs, and warehouse optimizations were key to scaling Superset.Cache Warmup JobOf all dashboards viewed each day at Airbnb, 90% are viewed more than once. This, combined with the fact that currently most new data only lands once a day through our ETL (extract, transform, load) jobs, means that caching the results of dashboard charts daily can dramatically improve performance for the majority of users. Using Apache Airflow™, we implemented an effective offline cache warmup strategy focused on warming up recently viewed dashboards, resulting in an 86% cache hit rate for Presto®-backed charts. Since Superset natively supports caching chart requests in Redis™*, we were able to programmatically load the popular dashboards during non-business hours, thus reducing the load on our query engines, Presto® and Apache Druid™, during peak hours. This improved cached chart load times from over 30 seconds when uncached to under four seconds.Domain ShardingWhen loading a dashboard in Superset, individual requests are fired off concurrently for every visible chart on the dashboard. Although this works when requests are fast and dashboards are small, with dashboards containing many charts we quickly run into issues with browser settings. Most modern browsers limit the number of concurrent requests made to a single domain (i.e., the Superset API) to six, resulting in a bottleneck that slows down large dashboards. To handle this issue, we built the SUPERSET_WEBSERVER_DOMAINS configuration option. By setting this option, admins can allow as many concurrent dashboard queries as their database engine can support (an effective cache may be required to ensure that the engine is not overloaded). We route four different subdomains to our web server, supporting up to 24 concurrent queries on a single dashboard. This functionality was key to allowing users to build complex dashboards and improving performance.Database Engine Load ManagementWhile Superset allows for a lot of native optimizations, some performance and stability improvements can only be done at the database engine level. Because of complex business needs and the size of our datasets, many dashboard-triggered queries take 25 seconds on average to execute. Therefore, we took the following steps to make sure our database engine clusters do not become overloaded:Route queries based on importance: We route queries to different clusters (using the DB_CONNECTION_MUTATOR configuration) to avoid resource contention; optimized dashboard queries are sent to one cluster while ad hoc SQL Lab and explore queries are sent to another.Limit concurrency for each user: We limit each user to running only three queries simultaneously on our database engine. While this may seem like a small number, it’s actually more than sufficient given the cache warmup job previously mentioned. In the ideal case, very few queries actually make their way to our database engine, and the cache efficiently returns the results instead of rerunning the query.Restrict large queries: We limit the size of queries that can be run to a certain memory size or partition count. This encourages users to create efficient queries of reasonable complexity.Where Superset Really ShinesAs a BI solution, Superset is capable of satisfying most of our needs, though there are also many similar products on the market. We continue to use and invest in Superset for many reasons — familiarity, content, migration costs, etc. — but where Superset really shines for us is the fact that it is open source. This has allowed Airbnb to implement a number of advanced customizations that would likely have been difficult with commercial products.As an active contributor to the Apache Superset project, we were able to adapt Superset for our business needs via:Helping to define the open-source roadmapProposing and implementing open-source featuresCreating custom, in-house overrides or mutations. The Superset backend is written in Python which supports easy augmentation and customization via monkey patching.When evaluating whether to build an internal solution or buy something off the shelf, one is faced with the 80/20 conundrum. An off-the-shelf solution will likely get you 80% of what you need, but the final 20% may be fraught with insurmountable challenges. Although Superset currently lacks some of the features and polish that other SaaS solutions offer, it makes up for these deficits in spades through the level of potential customization it provides.Below are a few projects where Airbnb has leveraged or augmented Superset to enhance — either by streamlining or enriching — the user experience.Metric ExplorerMetric Explorer, a component of the Dataportal (Airbnb’s search and discovery tool), enables out-of-the-box data exploration for teams across Airbnb. The goal was to make it easy and safe for anyone to explore curated business metrics, courtesy of the Minerva framework, for a typical reporting period — last 7 days, prior week, etc.When designing Metric Explorer, we were faced with a dilemma. We wanted to provide a highly curated and vetted experience for slicing and dicing metrics that leveraged rich metadata and surfaced business context, while providing sufficient guardrails. However, we did not want to build another dashboarding tool and reimplement large swaths of Superset’s features.We decided to solve this dilemma by factoring out the frontend foundation of Superset visualizations into the @superset-ui NPM packages. Not only did this solve the Metric Explorer use case, it also enabled any Superset installation to build other custom data applications that leverage the Superset backend. Figures 1 and 2 are Metric Explorer screenshots illustrating the Superset integrations.Figure 1: Metric Explorer illustrating a collection of metrics powered by @superset-ui.Figure 2: Metric Explorer illustrating a single metric where the header and left hand panel are powered by @superset-ui. Purposefully, Metric Explorer has limited slice-and-dice functionality, thus a link to Superset is also provided for more advanced analytics.Security Manager and Data Access Policy IntegrationThough Superset ships with a default Security Manager, the scale of the data at Airbnb and the complexity of our data access policy required a custom implementation. Restrictions are defined at the underlying table or metric level rather than at the level of Superset entities — charts, dashboards, etc.We leveraged Superset’s custom security manager functionality, via the CUSTOM_SECURITY_MANAGER configuration option, and some RESTful API and Flask-AppBuilder overrides. Using this approach, we were able to seamlessly integrate Superset to adhere to the data access policy enforced by Gandalf, Airbnb’s internal security controller.We wanted to further enrich the user experience by integrating the access request flow directly within Superset. This was achieved by adding frontend customizations, in conjunction with the custom security manager, which prompted users through a flow whenever we detected a priori (i.e., before the actual query was run) that they did not have the relevant permissions to access the underlying data. Figures 3–5 illustrate the data access policy integration within Superset.By surfacing access requests in-place, instead of having users decipher cryptic database errors or directing them elsewhere, we were able to preserve the user flow and simultaneously provide the approvers with the necessary context about the request. This deeply integrated experience would most likely be very difficult to provide with other, less customizable tools.Figure 3: A user is denied access if they do not have the relevant permissions to access either a datasource or a metric. Access can be requested in place (Figure 4).Figure 4: The modal for requesting access to a restricted datasource or metric. In addition to being provided a reason, the approvers are also informed of the context for the request — i.e., which chart or dashboard the user is trying to access.Figure 5: If the user is denied access but has a pending request the state of the request (which may require multiple approvers) is shown.Metrics for the MassesAs mentioned previously, Airbnb developed the in-house Minerva metric framework. Data can be queried in Superset via the Minerva-DB, a metric-centric, pseudo-datasource-agnostic SQL database backed by an Apache Druid cluster. To aid with discovery and enhance the user experience, all the metrics and dimensions are defined in a single non-mutable virtual Superset datasource with pre-defined metric expressions. Since Gandalf supports permissioning at the metric level, the datasource remains functional from an access control perspective.This single datasource now encompasses thousands of metrics and dimensions. Since most metrics and dimensions are generally scoped to products or projects by construction, the vast majority of metric-dimension combinations are not viable.To avoid user frustration on accidentally selecting an infeasible combination of metrics and dimensions, we made an open source contribution to Superset’s chart controls, introducing a hook to asynchronously update control props based on user inputs. This allows us to filter out invalid metrics and dimensions according to what users have selected. Figures 6 and 7 illustrate the behavior.Figure 6: The Superset query panel and metric popover for the Minerva virtual datasource containing an immense number of metrics and dimensions. Given the vastness of the datasource, most metric-dimension combinations would be invalid without adding custom logic to determine the feasible subset (Figure 7). Numbers are shown for illustrative purposes only.Figure 7: The Superset query panel. By selecting the bookings metric, the viable set of dimensions has reduced from 1,000 to 100. Furthermore, by grouping by the dim_origin_city dimension, the viable set of dimensions further reduced to around 50 because the increased specificity had reduced the set of feasible Apache Druid datasources. Numbers are shown for illustrative purposes only.A traditional BI tool would likely not be able to handle data at this scale or would result in severe usability issues given it would not be apparent to the user which dimensions are applicable to filter or group-by for a specific set of metrics.ConclusionAs evidenced by the above examples, we at Airbnb have invested heavily in Superset over the past half decade. The time and effort we have put in allowed us to create a BI ecosystem that enables any employee to self-serve the analytics they need to perform their job in a data-informed manner. Superset’s configuration and customization capabilities, along with the ability to build the product roadmap through open source, have provided a stable foundation to keep Superset relevant for years to come.In this post, we focused on how we evolved Superset for Airbnb’s large-scale needs, but other companies have leveraged Superset in different ways as well. You can learn about some of these here:Dropbox: Why we chose Apache Superset as our data exploration platformNielsen: How Nielsen Scaled Access To Data Analytics Using Apache SupersetPreset: Apache Superset 1.0 is out!AcknowledgmentsThanks to everyone who contributed to the work represented in this blog post, especially Chris Williams, Gustavo Torres, Jinyang Li, Krist Wongsuphasawat, Michelle Thomas, Serena Jiang, and Sylvia Tomiyama.Apache Superset, Apache Druid, Apache Airflow, Superset, Druid, Airflow, Apache, and the Apache Superset logo are either registered trademarks or trademarks of The Apache Software Foundation in the United States and other countries.* Redis is a trademark of Redis Labs Ltd. Any rights therein are reserved to Redis Labs Ltd. Any use by Airbnb is for referential purposes only and does not indicate any sponsorship, endorsement or affiliation between Redis and Airbnb.All trademarks, service marks, company names and product names are the property of their respective owners. Any use of these are for identification purposes only and do not imply sponsorship and endorsement.Supercharging Apache Superset was originally published in Airbnb Engineering &amp; Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-02-09 18:02:30","link":"https://medium.com/airbnb-engineering/supercharging-apache-superset-b1a2393278bd?source=rss----53c7c27702d5---4","blog":{"id":"airbnb","link":"https://medium.com/airbnb-engineering","name":"Airbnb","rssFeed":"https://medium.com/feed/airbnb-engineering","type":"company"},"blogType":"company"},{"id":"e56474852525dec91e808712a4b8be3e","publish_timestamp":1611771540,"title":"Designing Experimentation Guardrails","blogName":"Airbnb","image":"https://miro.medium.com/max/1200/1*egAw4MTqrCvWxbCPcg-fiw.jpeg","categories":["datascience","abtesting","data"],"description":"Introducing the Experiment Guardrails framework we implemented at Airbnb, which helps us prevent negative impact on key metrics while experimenting at scale.by Tatiana Xifara, Reid Andersen &amp; Ali RauhEach week, thousands of online experiments run concurrently on the Airbnb platform to measure the impact of potential product changes monitoring approximately tens of metrics per experiment. When making launch decisions, each team is often focused on different evaluation criteria — for example, the Trust team prioritizes Fraud Identification, while the Experiences team may prioritize discovery of the Online Experiences product in our Homepage. Experiments that positively impact one team’s metrics can also harm another team’s metrics, and it’s not always obvious how to weigh these trade-offs — for example if house rules are not displayed in Checkout we might see an increase in bookings but lower ratings. In the worst case, a team might discover that another team recently launched a treatment that does significant harm to one of their key metrics without a proper analysis of the tradeoffs, requiring a roll-back of the new changes.Introducing the Experiment Guardrails SystemTo help our dozens of experiment-running teams ensure their launches don’t harm our most important metrics, we rolled out a company-wide Experiment Guardrails system in 2019. This system helps protect key metrics by identifying potentially negative impacts prior to launch. If a team wishes to launch an experiment that has “triggered” a guardrail (that is, where our guardrails system has found that the treatment negatively impacts a key metric, or is underpowered to ensure it does not have a substantial risk of negatively impacting a key metric), they will go through an escalation process first, where stakeholders can discuss the results transparently.Selecting Metrics to ProtectGuardrail Metrics are ones that are important to the company as a whole. While a feature does not necessarily need to improve a Guardrail Metric to be considered successful, all launches are meant to avoid having substantial negative impact on the guardrails.We found that useful Guardrail Metrics tend to fit into one of three categories:Key business/financial metrics that represent overall company performance — for example, revenue.User experience metrics that capture how it feels to use the product — for example, bounce rate or page load speed.Strategic priority metrics that focus on areas of strategic importance to the company — for example, Seats booked for Experiences. These may change over time as the company’s strategy evolves.While it may be tempting to guardrail every team’s favorite metric, it’s important to keep in mind that more guardrail metrics doesn’t necessarily mean better — there is a trade-off between how many metrics are protected, how thoroughly they are protected, and how much friction is added to the product development process. For example, if we choose 50 metrics and alert on any degradation that is significant at the 0.05 level, then we would have at least one false alert 92% of the time in an AA test.Defining The Three GuardrailsOur system consists of three guardrails that each must be passed individually for an experiment to launch without escalation:The Impact Guardrail requires that the global average treatment effect is not more negative than a preset threshold. This guardrail protects against large negative effects regardless of statistical significance.The Power Guardrail ensures the experiment has been exposed to enough users so the Impact Guardrail has a reasonable false positive escalation rate and power.The Stat Sig Negative Guardrail provides additional protection for metrics where any statistically significant negative impact — even if it’s small in magnitude — would warrant escalation.We’ll walk through each of these in detail below:The Impact GuardrailThe Impact Guardrail escalates an experiment if:where percent change is the relative change of the means and t is the escalation threshold. For example, if t is 0.5%, an experiment that has an impact more negative than -0.5% will be escalated.Note that throughout this blog post, we assume we are working with metrics where an increase is desired (e.g., revenue). For metrics where a decrease is desired, the relationship with percent_change should be flipped (e.g., customer service tickets should escalate if percent_change &gt; t)The Power GuardrailThe Power Guardrail impacts experiment runtime. It requires the standard error for our estimate of the percent change to satisfy:This is to ensure the Impact Guardrail has reasonably good power and false positive rate (FPR). If an experiment just meets the power guardrail for a single metric, it will have the following profile for that metric:If an experiment runs longer than is required by the Power Guardrail, the FPR gets smaller and the power to detect an impact of -0.8*t gets larger since standard error gets smaller The more metrics an experiment includes, the larger the experiment-level FPR will be at any given runtime.The constant 0.8 can be adjusted down if you want a tighter set of Power and FPR requirements, or adjusted up if you can tolerate lower Power and higher FPR. Keep in mind that the lower the constant, the longer experiments must run to pass the Power Guardrail. A good way to evaluate the practicality of your Power Guardrail is to run a backtest: What percent of experiments launched in the past 6 months would have passed the guardrail as-is? For the ones that did not pass, how much longer would they have needed to run? You’ll want to consider how implementing your Power Guardrail might affect experiment run times across your organization.The Stat Sig Negative GuardrailThe Stat Sig Negative Guardrail escalates an experiment if it shows a statistically significant negative impact on certain metrics:Most likely, you won’t want to apply the Stat Sig Negative Guardrail to all metrics, as some may not warrant escalation for a small negative impact. For example, a 0.1% degradation (increase) in page performance is undesirable, but probably not worth escalating. On the other hand, a 0.1% degradation (decrease) in revenue at a company the size of Airbnb could potentially translate to millions of dollars. This guardrail is an extra safeguard for your top metric(s) where even a small negative impact should be surfaced.Adjusting for Global CoverageWe define the global coverage of an experiment to be the % of airbnb visitors that are assigned into the experiment. If all experiments had the same Power Guardrail, low-coverage experiments would have a harder time passing it. To allow all experiments to pass the Power Guardrail in roughly the same time, we allow t to vary with global coverage, the percentage of the total value of the metric that is covered by subjects assigned to the experiment, in the following manner:We set an Escalation Parameter T for each metric, which represents the percent change that will always trigger an experiment with 100% global coverage. We then let the escalation threshold t vary depending on coverage:Because the Impact Guardrail also uses the coverage-adjusted t, all experiments that just pass the Power Guardrail will see the same power / FPR profile as outlined in the Power Guardrail section.In the table below, you can see how escalation thresholds for percent change and global impact vary by coverage. As coverage decreases, the threshold on percent change increases — this is desirable, as it makes the Power Guardrail pass rate similar across coverage levels. The Threshold for global impact, on the other hand, decreases with decreasing coverage — this is also desirable, as we should be tougher on lower-coverage experiments in terms of global impact.Putting It All TogetherAll together, the three Guardrails protect our most important metrics by escalating negative impacts that we’ve deemed meaningful, while ensuring we have appropriate power to detect it. Visually, we can see the guardrails at work across point estimate and standard error for an example metric:The Power Guardrail is represented as the horizontal line, and an experiment with a larger standard error would need to continue running until StdError &lt; 0.8 * t. An experiment owner can also choose to escalate prior to reaching the required standard error, if the estimated run time is too long.The Impact Guardrail is represented as the vertical line at -T, and an experiment with an impact more negative than -0.5% would require escalation prior to launch.Finally, the Stat Sig Negative Guardrail is represented as the diagonal line, where the percent change is negative and p-value = 0.05. Metrics with this guardrail enabled will escalate experiments with a statistically significant negative effect.For all remaining experiments, we are comfortable that there is low risk of meaningful negative impact, and they are able to launch without escalation.Refining the guardrails — Cases with automatic approvalTo refine the set of experiments that require escalation, we have carved out some cases where we automatically approve experiments even if they trigger one of the guardrails above. Here are two major refinements we have made:For some metrics where it’s easy for us to reach statistical significance, but where only large changes in the metric are material for our business, we ignore the Stat Sig Guardrail and only apply the Power and Impact Guardrails.For experiments that have not yet passed the Power Guardrail, but have positive point estimates, we allow these to pass without escalation if they pass a noninferiority test with the same threshold as our Power Guardrail (which you could view as a relaxation of the power guardrail). In particular we let an experiment pass if the lower bound of the confidence interval satisfies:This allows treatments with positive point estimates to pass before reaching the power guardrail.Making It Your OwnThis Guardrails system is extremely configurable, and the parameters can be adjusted to fit your needs. We’ve discussed how you can adjust the 0.8*t multiplier and refine the guardrails in some cases.The most important decision is setting Parameter T for each Guardrail metric, which is as much an art as it is a science. A tighter T will allow you to catch smaller negative impacts, but it will also make the Power Guardrail harder to pass. You should set T as the larger of the two between “What impact is worth escalating for?”, and “What impact is feasible to detect?” This ensures that the guardrails neither require egregious runtime, nor waste escalations on low-magnitude impacts.Another factor to consider is how many Guardrail metrics you have.The more metrics you cover, the higher the overall escalation rate (and false positive rate) will be. If your organization experiments often, there will be a limit to the number of experiments that can be realistically reviewed. Requiring escalations slows down the speed at which you iterate and launch, so you want to find a balance between protecting your metrics and moving quickly. Once again, the best way to decide is to look at historical data — evaluate different sets of metrics and T values by estimating the resulting overall escalation rates, and decide what configuration is best for your organization’s needs.Final ThoughtsExperimentation at scale can be challenging, especially when multiple teams may be focused on different goals. We introduced an Experimentation Guardrails system to help bring visibility into Airbnb’s most important metrics and to protect them from potentially harmful launches. The system flags roughly 25 experiments per month for escalation/review. Of these 80% eventually launch after discussion between stakeholders and additional analysis, and 20% (5 per month) are stopped before launch. Our configurable system allows us to balance safeguarding our metrics and maintaining a nimble product development process, and we hope this can be a useful reference for thinking through guardrail systems of your own. Happy experimenting!AcknowledgementsMany thanks to Kathy Yang for her contributions to this project and blog post.Designing Experimentation Guardrails was originally published in Airbnb Engineering &amp; Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-01-27 18:19:00","link":"https://medium.com/airbnb-engineering/designing-experimentation-guardrails-ed6a976ec669?source=rss----53c7c27702d5---4","blog":{"id":"airbnb","link":"https://medium.com/airbnb-engineering","name":"Airbnb","rssFeed":"https://medium.com/feed/airbnb-engineering","type":"company"},"blogType":"company"},{"id":"becedd76512535c5e2363adee50f74b0","publish_timestamp":1608587775,"title":"Introducing Showkase: A Library to Organize, Discover, and Visualize Your Jetpack Compose Elements","blogName":"Airbnb","image":"https://miro.medium.com/max/1200/1*_nGzU-zeLfCXI6UuoZohHw.jpeg","categories":["jetpackcompose","androidlibraries","mobile","opensource","android"],"description":"Over the last few years, Android™ development has gone through significant changes in how apps are structured, the language used for development, the tooling &amp; libraries that speed up our development, and the improvements in testing apps. What didn’t change in all these years is the Android UI toolkit.This changes with Jetpack Compose — Android’s modern toolkit for building native UI. It simplifies and accelerates UI development on Android and is a paradigm shift in Android development as we know it. The move to using declarative programming for describing UI is a welcome change and one that is prevalent across platforms with the introduction of frameworks like React for web, SwiftUI for iOS, and Flutter for cross-platform development.Declarative UI frameworks encourage the creation of reusable components that can be shared across different parts of the app. We know that reusability is a good engineering practice but what often ends up happening is a little more nuanced.What’s The Problem?As developers add new UI components, the codebase ends up with hundreds of components that are often hard to visualize.Discoverability of these components is a challenge and there is no easy way to search. As a result, your codebase often ends up with duplicate components that offer similar functionality. You might be able to use your IDE to search for a component if you used a very descriptive name. However, naming things is hard and descriptive names still do not always help us find the right component to use.The same problems extend to other aspects of your design system like colors, typography, icons, etc.In order to get around this, most mobile teams build their version of a “component browser” that lets you visualize your design system. This is often maintained manually with little-to-no tooling around it.If you have ever worked on any frontend platform, you can probably relate to some of these issues that contribute to a sub-par developer experience. As engineers, our job is to make software do this work for us.Introducing ShowkaseShowkase is an annotation-processor based Android library that helps you organize, discover, search and visualize Jetpack Compose UI elements. With minimal configuration it auto-generates a UI browser that organizes your design system for you.Showkase auto-generates a browser for your Jetpack Compose UI ElementsShowkase helps you visualize and document your design systemShowkase takes it a step further and auto-generates permutations of your components in common situations like dark mode, right-to-left layouts, scaled fonts, etc. This helps to preview your components in common scenarios without the need to do any additional work. This is helpful in detecting issues early and fixing them while you are still developing the components.Multiple permutations are auto generated for each component allowing you to preview it in different scenariosHow do I use Showkase?Setting up Showkase just requires a few simple steps-Step 1: Add the dependency to your module’s build.gradle file. If you have a multi-module setup, add this dependency to all the modules with UI elements that should be displayed inside the Showkase browser.https://medium.com/media/b97365b9ec13ba4931f82c3eacd3ebcc/hrefStep 2: Add the relevant annotations for every supported UI element that should be a part of the Showkase browser.Showkase provides different annotations to mark UI elements that you’d like to see inside the Showkase UI browser.For @Composable components, use the @ShowkaseComposable annotation:https://medium.com/media/b4b2a1be7f2582cfe85bb1e6c36ff3c4/hrefShowkase provides first class support for @Preview. If you are already using the @Preview annotation that Compose provides for previewing Composable in Android Studio, you don’t need to do anything else.https://medium.com/media/42e70f2dea2888f023f3488f97bb39d1/hrefFor color properties, you can add the @ShowkaseColor annotation to the field:https://medium.com/media/0efbeed62d5104309885bd863fde332a/hrefSimilarly, for TextStyle properties that are useful for representing typography in Jetpack Compose, you can add the @ShowkaseTypography annotation to the field:https://medium.com/media/3d3da8a5869b638df0b9fa395a68ccee/hrefStep 3: Define an implementation of the ShowkaseRootModule interface in your root module.https://medium.com/media/c4bd1a87a75618c6355afda59409b849/hrefStep 4: Showkase is now ready to use!Showkase comes with a predefined Activity that does the necessary scaffolding for accessing the UI browser with the elements you annotated. Typically you would start this activity from the debug menu of your app but you are free to start this from any place you like! A nice helper function createShowkaseBrowserIntent is generated for you — just start the intent and you are good to go.https://medium.com/media/ce4f19e20123c96f22f6e9e891646333/hrefShowkase in action demonstrating how you can visualize and search all your Jetpack Compose components in your codebase.Showkase also lets you visualize the other aspects of your design system like colors &amp; typography.By reducing the amount of manual work needed in maintaining your design system/UI components and making it more discoverable, Showkase helps in driving adoption of the reusable components in your Android codebase.If you like what you see and are experimenting with Jetpack Compose, give Showkase a spin and let us know what you think.How can we improve Showkase further?We are thinking of ways in which we can extend Showkase. Here are some ideas that we are thinking about.Hooks for screenshot testing. Since all your components are a part of the Showkase browser, this would be a good opportunity to make this a part of your CI and detect differences in components across pull requests.Support for other UI elements that are a part of your design system (like icons, spacing, etc).Generating a web version of the Showkase browser with documentation, search, and screenshots.AcknowledgementsShowkase was inspired by a similar internal tool that was built at Airbnb by Nathanael Silverman.Many thanks to Eli Hart, Ben Schwab, Laura Kelly, Andreas Rossbacher, Gaurav Mathur, Laura Skelton, Madison Capps, Steve Flanders, Shreya Sharma, Brett Bukowski, Lauren Mackevich &amp; David Shariff for helping with various aspects of Showkase.Airbnb is always looking for talented Android engineers such as yourself. Check out our careers page for current openings and apply!All trademarks, registered trademarks, product names, brands, are the property of their respective owners. Any use of such in this website are for identification purposes only and do not imply endorsement.Android is a trademark of Google LLC.Introducing Showkase: A Library to Organize, Discover, and Visualize Your Jetpack Compose Elements was originally published in Airbnb Engineering &amp; Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-12-21 21:56:15","link":"https://medium.com/airbnb-engineering/introducing-showkase-a-library-to-organize-discover-and-visualize-your-jetpack-compose-elements-d5c34ef01095?source=rss----53c7c27702d5---4","blog":{"id":"airbnb","link":"https://medium.com/airbnb-engineering","name":"Airbnb","rssFeed":"https://medium.com/feed/airbnb-engineering","type":"company"},"blogType":"company"},{"id":"65285eaf478e64397b19284e3f84dc18","publish_timestamp":1608230024,"title":"Detecting Vulnerabilities With Vulnture","blogName":"Airbnb","image":"https://miro.medium.com/max/1200/0*LqEFhJIWY9azXKtL","categories":["infrastructure","vulnerability","infosec","security"],"description":"Introducing a new open source tool for more quickly identifying security vulnerabilities across your assets.IntroductionSecuring assets and data is a challenging and important task, regardless of your industry or company size. On Airbnb’s InfoSec team, we are constantly thinking about how to quickly and effectively detect and remediate known security vulnerabilities that impact our assets.To address this challenge, we created a tool called Vulnture, which we are happy to open source and share with the software community!Vulnture vastly simplifies the otherwise laborious process of collecting your known assets in one place and identifying security vulnerabilities in a timely manner. Before we share how Vulnture works, let’s take a deeper look at the challenges we’re addressing.BackgroundIn 1999, MITRE and the broader security community launched the Common Vulnerabilities and Exposures (CVE) List. This list was developed to reduce gaps in security coverage, to enable interoperability between vulnerability databases and tools, and to provide a standardized basis for evaluating vulnerabilities across tools.Several years later, in 2005, the National Institute of Standards and Technology (NIST) launched the U.S. National Vulnerability Database (NVD). The NVD is populated with vulnerability data from the CVE List along with additional information associated with each vulnerability such as fix information, severity scores, and impact ratings in an effort to make the CVE information more useful and actionable. The NVD also provides more granular search capabilities such that vulnerabilities can be queried based on which product name and/or version they affect, how recently they were created, their severity, and more.In short, the CVE List provides a list of all known, publicly disclosed security vulnerabilities while the NVD provides more detailed and easily queryable information related to those vulnerabilities. Airbnb saw a great opportunity in these resources. Let’s see how we made use of this data!The ProblemFor the past twenty or so years these data sources have grown vastly, as have the consumers of the data. However, the data is not easy to ingest. To make use of it, you have to have a good way to compare all of your assets against the vulnerable products listed in the NVD. Additionally, the NVD is updated hourly according to the NVD FAQ, so you need to check back often, particularly if you’re on the hunt for critical severity vulnerabilities. Prerequisites for effectively utilizing data from the NVD include:Having a record of all of your assets (e.g., operating systems, applications, services, executables, hardware models, versions of each of these, associated IP addresses, owners)Having the ability to query often in order to pick up all updates, while preferably not reviewing the same vulnerabilities more than once (unless they’ve been modified) to avoid duplicating workSolving for these challenges starts to get even more complex when we look at the actual turnaround time for NVD updates. The NVD FAQ mentions that it can take 1–3 days for information to be published depending on the volume of new CVEs. According to a report from threat intelligence company Recorded Future, in 2017 there was a median lag time of 7 days before vulnerabilities were populated within the NVD. Obviously, waiting up to a week just to be notified about a known vulnerability is far from ideal.This means that in addition to searching the NVD, you probably want to consider searching other sources of vulnerability data as well, such as vendor sites of the products potentially affected. This way you’re more likely to find critical vulnerabilities that impact you before several days pass, thereby no longer granting attackers precious time to exploit those vulnerabilities.The challenge here, of course, is that you now have to:Seek out all possible vulnerability feeds from your vendorsCraft a suitable query to sift out unwanted dataParse out the non-standardized resultsThe latter of these two steps needs to be repeated for each separate vulnerability feed meaning exponentially more work for each vulnerability source you want to get data from.Considering the above, it may come as no surprise that, according to the 2020 Verizon Data Breach Investigations Report, nearly 85% of security breaches and 99% of security incidents are discovered by an external or partner source, such as a third-party monitoring service, security researcher, or customer report. That means most security breaches, and practically all security incidents, are detected by somebody other than the company experiencing the security incident or breach!Not all security incidents or breaches are caused by a known vulnerability, but the key point here is that overwhelmingly companies are relying on third parties to tell them that they have a security issue rather than being able to identify it themselves.Introducing VulntureVulnture is a completely serverless solution that ingests all of your known assets, searches for vulnerabilities that impact them, and then notifies you of all discovered vulnerabilities that you should know about. Let’s briefly go over how it works.Vulnture High Level Overview DiagramIn the currently released version, Vulnture has the ability to pull asset information from an AWS DynamoDB table and then query the NVD, as well as Cisco Security Advisories publications, for vulnerabilities. For notifications, it sends emails to the recipient(s) configured in the configuration file:https://medium.com/media/2f46187658cde23162d7d15b532093b5/hrefVulnture is set to query its vulnerability data sources once per day to ensure that you’re never missing critical information related to vulnerabilities impacting your assets. If you don’t already have some type of asset management or inventory solution, you can start building one in a DynamoDB table which you can then have Vulnture reference. If you do have one, you can either export that data to a DynamoDB table or update Vulnture so that it is capable of querying your existing asset data (if you do make an update for your specific tech stack, consider making a pull request to make this a native feature of Vulnture!)Vulnture is written entirely in Python and deployed via Terraform using the included Terraform modules (.tf files). This makes it quick and easy to deploy without having to worry about manually managing various pieces of infrastructure or making code changes in multiple languages.We’re planning some exciting updates to Vulnture which are pending public release. These include an additional notification option (JSON files uploaded to an AWS S3 bucket) and support for more detailed asset information from a DynamoDB table (e.g., IP, FQDN, source).e have many more updates in the pipeline for future releases, including leveraging the relatively new NVD API to speed up and narrow down vulnerability queries. We’re also planning to support more sources of vulnerability data directly from vendors (e.g., Amazon, Microsoft, Ubuntu, Red Hat) to find vulnerabilities sooner.We aim to make Vulnture easily extensible with a plugin model to allow it to work seamlessly with various asset, vulnerability, and notification channels that can be quickly selected via straightforward configuration changes.ConclusionAlthough securing your assets is no simple task, you can feel more at ease knowing that you have a tool working for you by regularly querying industry standard vulnerability data repositories, comparing that data against your known asset information, and notifying you of vulnerabilities discovered.We’ve used Vulnture at Airbnb to replace some of our previous, less efficient methods of vulnerability identification and notification and hope that others find benefit in this tool as well. Beyond that, we would love to work with the community to continue to augment Vulnture so that it can become a more robust and feature-rich tool that’s simple to use and adaptable to various company environments and toolsets.Try out Vulnture today, you may be surprised at what you’re missing!Interested in helping protect people and data? Airbnb InfoSec is hiring! Check out our open positions and apply today!AppendixAll trademarks are the property of their registered owners; Airbnb claims no responsibility for nor proprietary interest in them.Amazon Web Services, the “Powered by AWS” logo, AWS, AWS Lambda, and DynamoDB are trademarks of Amazon.com, Inc. or its affiliates in the United States and/or other countries.Cisco is a registered trademark or trademark of Cisco Systems, Inc. and/or its affiliates in the United States and certain other countries.Terraform is a trademark of HashiCorp.Detecting Vulnerabilities With Vulnture was originally published in Airbnb Engineering &amp; Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-12-17 18:33:44","link":"https://medium.com/airbnb-engineering/detecting-vulnerabilities-with-vulnture-f5f23387f6ec?source=rss----53c7c27702d5---4","blog":{"id":"airbnb","link":"https://medium.com/airbnb-engineering","name":"Airbnb","rssFeed":"https://medium.com/feed/airbnb-engineering","type":"company"},"blogType":"company"},{"id":"d83925fb927d078be39ed736432fe24f","publish_timestamp":1607459078,"title":"WIDeText: A Multimodal Deep Learning Framework","blogName":"Airbnb","image":"https://miro.medium.com/max/1200/1*x2Oo4Rk3bWmWU4bYuUV1IA.jpeg","categories":["computervision","ai","artificialintelligence","nlp","deeplearning"],"description":"How we designed a multimodal deep learning framework for quick product development, and how the Room Type Classification models built upon it helped us better understand the homes on our platform.By Wayne Zhang, Mia Zhao, Yuanpei CaoDeep learning (DL) is helping us at Airbnb serve our stakeholders better and enhance belonging. For example, we use it for search ranking models, fraud detection models, issue prediction models in customer support, content understanding on listings and many other areas. A broad division within DL are classification tasks, which use a set of features to predict labels or categories within a taxonomy, such as predicting room types from listing images.The complexity of classification problems in the real world is extremely high since there are all sorts of different signals and potentially useful features in production. Data scientists and machine learning engineers always aim for improving model performance (e.g., precision, recall, etc.) as much as possible when building the classification models, by adequately incorporating the rich signals in the problem domain. This has proven to be helpful from literature review across academia and industry.This post will introduce WIDeText, a multimodal deep learning framework built by Airbnb that enables developing and productionizing classification systems at ease, and walk you through an example of using WIDeText to build a state-of-the-art room type classifier.Overview of WIDeText based model architecture having Text, Wide, Image and Dense channelsBackground of Multimodal Classification TasksTypically, ML engineers and data scientists start with a simple classification problem — there are core features, such as text or image, and the target is to train a DL model encoding and make predictions of their best category. For example, there are room images on Airbnb listings, and the model wants to predict the room type based on them.Single model design for classification taskHowever, we realized that many other features are also of great value for understanding the room types in the Airbnb listings. For instance, an image caption saying “living space w/ dining area, TV and Electric Fireplace”, which is a text feature, provides strong signals to classify the room as a living room.As we incorporate more features, the model becomes a lot more complicated. First, more types of features and “experts” (encoding or embedding models) in charge of understanding each of them are added to boost its performance. Second, a more sophisticated “decision maker” (classifier) that can summarize all of the voices and make predictions is needed. Here is where engineering overheads become significant.More complicated multimodal model design for classification taskFor most of the cases, one has to build ad-hoc model architectures and a feature processing pipeline, write a training and evaluating script, and deploy the model into the pipeline or endpoint. In the meantime, great efforts have to be made to keep track of every detail in order to properly review the work, which are shown as steps 3 to 7 in the diagram. This turns out to be a major portion of the machine learning development process. It is neither time efficient nor scalable in the long run.Briefing steps on developing and deploying ML models in AirbnbWe propose a unified framework to simplify, expedite, and streamline the development and deployment process for this type of multimodal classification tasks.WIDetext — Multimodal Deep Learning FrameworkBy taking a look at several multimodal classification tasks and the features they used, it’s not hard to identify that the features fall into several common buckets and can be tackled by specific “experts” (model architectures).Image channelExamples: Listing images, amenities images, etc.MobileNet, ResNet, etc. are experts on this. We will cover more details in the section later.Text channelExamples: Image captions, reviews, descriptions, etc.There are many NLP models such as CNN, LSTM, transformers, etc. that are experts on it.Dense channelExamples: Categorical features, numerical features, such as amenity types, image quality scores, location information, etc.GBDT is one of the experts.Wide channelExisting embeddings which are generated by experts somewhere else, and can be directly leveraged by our decision-maker (classifier).Thus, at Airbnb, we developed an in-house PyTorch based multimodal deep learning framework, named WIDeText: Wide, Image, Deep, and Text, to enable developing and productionizing classification systems at ease.The core concept here is model fusion. We can leverage the state-of-the-art model architectures for different types of features and assemble the embeddings to boost the final classifier’s ability. It provides an experience of building deep learning models in the way of building blocks — one can easily plug in / off channels and adjust their architecture per the objective.Let’s take a closer look at how WIDeText gets you covered on:Model prototyping and developing (Configure channels and architecture)Training and deploy (Build pipeline in production)Model Development — Json based model configurationWIDeText supports using JSON alike to configure the models: every channel in the framework is pluggable and configurable in terms of their architecture and training hyper-parameters.Hyper-parameters are required for WIDeText classifiers. Other channels’ can be set up optionally for different user cases.Below snippet shows a dummy example of setting it up for a multimodal classification model.A dummy example of setting up a multimodal classification model using WIDeTextTo help visualize this WIDeText based model the snippets just built, it includesVGG based image channelCNN based text channelGBDT based dense channelWide channelMLP based classifierand their training and evaluating hyper-parametersVisualization of the WIDeText based model having Text, Wide, Image and Dense channelsTraining and Deployment — integrated with Airbnb’s infrastructureThe integration with Airbnb’s machine learning infrastructure makes model development and deployment easy.For context, Airbnb’s Bighead Machine Learning Infrastructure provides users with a composable, consistent, versatile interface for the creation of a self-contained model with minimal “glue” code. The Bighead transformer interface (to be noted, this has nothing to do with the other famous transformer architecture in the deep learning domain) provides a way to define stateful or stateless functions that transform a collection of named feature tensors to another collection of named feature tensors. Each transformer can be fitted and configured per your use case. Each transformer can perform a transform given a data source, then later applied on new data for inference. More importantly, a group of transformers can be connected to a directed acyclic graph (DAG) called ML pipeline.We provided a wrapper to make any WIDeText based model a Bighead Transformer. This can be combined with existing preprocessors, transformers, etc. to build and deploy an end-to-end machine learning pipeline together with the WIDeText transformer.For example, in the pipeline shown below, we added several preprocessors for different types of input before feeding them into the WIDeText transformer, which are JPEGResizeDecoder for image data, and one hot encoder, scaler and feature combiner for wide and dense features.An example of the integrated machine learning pipeline having multiple transformers, such as WIDeText transformer, several preprocessors, etc.With this constructed Bighead pipeline, one can use its unified APIs to pour their data in for training and evaluating, and deploy the pipeline to production and expose as an online endpoint.The WIDeText framework has been widely adopted by production teams. Multiple production models have been built and shipped in Airbnb’s products, such as issue prediction in customer support, and experience tagging to better understand experience listings on Airbnb. In the next section, we will describe an example application of WIDeText, Airbnb’s room type classification model.Application on Room ClassificationAs of June 2020, Airbnb has more than 390M active listing photos. As the saying goes, “a picture is worth a thousand words”. Listing photos are key decision factors when guests make reservations, and photo room classification, e.g., bedroom, kitchen, etc., is an important process in providing the best search experience to guests. For example, different room categories are distinguished in the photo gallery for Airbnb Plus listings to provide a better search experience. In Airbnb, we have been using a room classification model based on convolutional neural networks (CNN) on room images. However, as our platform evolves, we come to have a diverse set of features over multiple modalities. These come from different input sources that describe an Airbnb home photo. For example, Figure 1 shows a home visualization typically available on Airbnb. The home photo, the image caption written by the host, and the listing geo-location (city and country) are provided. The multi-modularity leads us to create a joint representation of an image, the text description, and the geo-location category for room classification.An example of home photo with its image caption and home location available from Airbnb. The features selected in room type classifier are: 1) image thumbnail; 2) image caption text displayed on airbnb: “Living Space w/ TV”; 3) image technical features: image size, width, image height, and image quality; 4) computer vision features: amenity detection results (i.e. n_couch = 2, n_tv = 1, n_bed = 0, etc); 5) listing geo-location features: country = ‘US’, and region = ‘north america’;Inspired by these multimodal data sources, we leverage WIDeText to enable developing and productionizing room classification systems at ease.Architecture of WIDeText-based Room Type ClassifierChannel choice for room type classificationText ChannelThe image caption text uploaded by Airbnb hosts is used in the text channel. Room type classification contains an average text length of 4 words, which is relatively short. Since CNN-based text architecture can effectively and efficiently capture local relationships on short phrases, we choose to plug CNN as a plug-in text channel. Text channel allows transfer learning from word vectors pre-trained from a domain-specific larger corpus. All room-related descriptions are served as domain corpus, and multilingual word embedding is pre-trained by first applying the skip-gram model to generate monolingual embedding and then aligning them in a zero-shot learning fashion.We apply multiple filter sizes to capture different region sizes and multiple filters for the same region size to learn complementary features in the same region. 1 — max-pooling is used to extract a scalar from each feature map and then optional dropout followed by a fully connected layer can be used to further shrink the dimension.Dense ChannelCategorical features like listing geo-location (i.e., country and region) are essential signals in room type classification. As a concrete example: “house” as a listing type is widespread in suburbs but far less common in cities, so entrance as a room type tends to be very different between these two places.Entrance to Home in suburb (left) vs. city (right)In the dense channel, those categorical features are encoded as a one-hot encoder representation, then further learned from backpropagation through fully connected layers.Numerical features like the image size/width/height and number of detected amenities, including beds/pillows/microwaves/etc., can also help in predicting the room type. Scaled numerical features are served in a dense channel and then used in feed-forward layers along with categorical features.Image ChannelWe applied multiple models on the test set and compared the performance with a baseline model built on pre-trained mobileNet that only uses image features. It shows that incorporating non-image features using WIDeText significantly improved the overall performance across different room type categories. Finally, we launched a WIDeText model using ResNet 50 due to the trade-off between accuracy and computation time.Table: performance comparison between the baseline model trained by pre-trained MobileNet architecture without using non-image feature and the proposed Room Type classifier trained by WIDeText architecture based on pre-trained MobileNet, fine-tuned ResNet 50 and EfficientNet B4 image channel.TakeawaysIn this post, we reviewed how we designed a multimodal deep learning framework for quick product development and demonstrated that the models built upon it greatly improved prediction accuracy in the room classification task.Here are a few key take-aways from ML practitioners who have been using the WIDeText framework to train the multimodal classifiers:First, WIDeText Framework helps speed up the model development and deployment process from weeks to days. This end-to-end training and deployment framework empowers modelers to utilize as many raw features as possible, making the model debugging easier.Second, it is common practice to compare each channel&#39;s architecture choice separately. We confirmed the effectiveness of doing that in our multi-model frameworks. We started with experimenting different image channel choices for room type classification without changing other channels. As a consequence, we could independently select the optimized archetype for each channel.Third, distillation from GBDT to Neutral Network is recommended for better performance if numerical features play an important role among all features, as it allows missed or unscaled input values. If categorical features are essential, choosing embedding-based dense layers yields better performance.AcknowledgmentThanks to Bo Zeng and Peggy Shao for contributing to the WIDeText framework, adopting it in their work, and providing valuable feedback. We would also like to thank the contributors of open source libraries such as PyTorch and the original inventors of MobileNet, ResNet, and EfficientNet. We benefit tremendously from this friendly open source community. Finally, we appreciate Ari Balogh’s support, and thank Joy Zhang, Hao Wang, and Do-kyum Kim for their kind help in proofreading.Further ReadingBighead: A Framework-Agnostic, End-to-End Machine Learning Platform goes into the details of the Airbnb Machine Learning Infrastructure. DSAA’2019Categorizing Listing Photos at Airbnb describes deep learning models applied on Airbnb photo categorizationWe always welcome ideas from our readers. For those interested in contributing to AI/ML work in Airbnb, please check out our open positions.WIDeText: A Multimodal Deep Learning Framework was originally published in Airbnb Engineering &amp; Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-12-08 20:24:38","link":"https://medium.com/airbnb-engineering/widetext-a-multimodal-deep-learning-framework-31ce2565880c?source=rss----53c7c27702d5---4","blog":{"id":"airbnb","link":"https://medium.com/airbnb-engineering","name":"Airbnb","rssFeed":"https://medium.com/feed/airbnb-engineering","type":"company"},"blogType":"company"},{"id":"cc084833b7c7db49e88f162cd6cbcb1e","publish_timestamp":1606251876,"title":"Data Quality at Airbnb","blogName":"Airbnb","image":"https://miro.medium.com/max/1200/1*jEHxv7keEHTRsoC__lSxZQ.png","categories":["datawarehouse","analyticsengineering","data","datascience","dataengineering"],"description":"Part 2 — A New Gold StandardAuthors: Vaughn Quoss, Jonathan Parks, Paul EllwoodIntroductionAt Airbnb, we’ve always had a data-driven culture. We’ve assembled top-notch data science and engineering teams, built industry-leading data infrastructure, and launched numerous successful open source projects, including Apache Airflow and Apache Superset. Meanwhile, Airbnb has transitioned from a startup moving at light speed to a mature organization with thousands of employees. During this transformation, Airbnb experienced the typical growth challenges that most companies do, including those that affect the data warehouse.In the first post of this series, we shared an overview of how we evolved our organization and technology standards to address the data quality challenges faced during hyper growth. In this post we’ll focus on Midas, the initiative we developed as a mechanism to unite the company behind a shared “gold standard” that serves as a guarantee of data quality at Airbnb.Defining the Gold StandardAs Airbnb’s business grew over the years, the company’s data warehouse expanded significantly. As the scale of our data assets and the size of the teams developing and maintaining them grew, it became a challenge to enforce a consistent set of standards for data quality and reliability across the company. In 2019, an internal customer survey revealed that Airbnb’s data scientists were finding it increasingly difficult to navigate the growing warehouse and had trouble identifying which data sources met the high quality bar required for their work.This was recognized as a key opportunity to define a consistent “gold standard” for data quality at Airbnb.A Multi-dimensional ChallengeWhile all stakeholders agreed that data quality was important, employee definitions of “data quality” encompassed a constellation of different issues. These included:Accuracy: Is the data correct?Consistency: Is everybody looking at the same data?Usability: Is data easy to access?Timeliness: Is data refreshed on time, and on the right cadence?Cost Efficiency: Are we spending on data efficiently?Availability: Do we have all the data we need?The scope of the problem meant that standards focused on individual data quality components would have limited impact. To make real headway, we needed an ambitious, comprehensive plan to standardize data quality expectations across multiple dimensions. As work began, we named our initiative Midas, in recognition of the golden touch we hoped to apply to Airbnb’s data.End-to-end Data QualityIn addition to addressing multiple dimensions of data quality, we recognized that the standard needed to be applicable to all commonly consumed data assets, with end-to-end coverage of all data inputs and outputs. In particular, improving the quality of data warehouse tables was not sufficient, since that covered only a subset of data assets and workflows.Many employees at Airbnb will never directly query a data warehouse table, yet use data on a daily basis. Regardless of function or expertise, data users of all types are accustomed to viewing data through the lens of metrics, an abstraction which does not require familiarity with the underlying data sources. For a data quality guarantee to be relevant for many of the most important data use cases, we needed to guarantee quality for both data tables and the individual metrics derived from them.In Airbnb’s data architecture, metrics are defined in Minerva — a service that enables each metric to be uniquely defined in a single place — and broadly accessed across company data tools. A metric defined in Minerva can be directly accessed in company dashboarding tools, our experimentation and A/B testing framework, anomaly detection and lineage tools, our ML training feature repository, and for ad-hoc analysis using internal R and Python libraries.For example, take Active Listings, a top-line metric used to measure Airbnb’s listing supply. An executive looking up the number of Active Listings in a Apache Superset dashboard, a data scientist analyzing the Active Listings conversion funnel in R, and an engineer reviewing how an experiment affected Active Listings in our internal experiment framework will all be relying on identical units for their analysis. When you analyze a metric across any of Airbnb’s suite of data tools, you can be sure you are looking at the same numbers as everybody else.In Airbnb’s offline data architecture, there is a single source of truth for each metric definition shared across the company. This key architectural feature made it possible for Midas to guarantee end-to-end data quality, covering both data warehouse tables and the metric definitions derived from them.The Midas PromiseTo build data to meet consistent quality standards, we created a certification process. The goal of certification was to make a single, straightforward promise to end users: “Midas Certified” data represents the gold standard for data quality.In order to make this claim, the certification process needed to collectively address the multiple dimensions of data quality, guaranteeing each of the following:Accuracy: certified data is fully validated for accuracy, with exhaustive one-off checks of all historical data, and ongoing automated checks built into the production pipelines.Consistency: certified data and metrics represent the single source of truth for key business concepts across all teams and stakeholders at the company.Timeliness: certified data has landing time SLAs, backed by a central incident management process.Cost Efficiency: certified data pipelines follow data engineering best practices that optimize storage and compute costs.Usability: certified data is clearly labeled in internal tools, and supported by extensive documentation of definitions and computation logic.Availability: certification is mandatory for important company data.As a last step, once data was certified, that status needed to be clearly communicated to internal end users. Partnering with our analytics tools team, we ensured data that was “Midas Certified” would be clearly identified through badging and context within our internal data tools.Fig 1: Midas badging next to metric and table names in Dataportal, Airbnb’s data discovery tool.Fig 2: Midas badging for metrics in Airbnb’s Experimentation Reporting Framework (ERF).Fig 3: Pop-up with Midas context in Airbnb’s internal data tools.The comprehensive Midas quality guarantee, coupled with clear identification of certified data across Airbnb’s internal tools, became our big bet to guarantee access to high quality data across the company.The Midas Certification ProcessThe certification process we developed consists of nine steps, summarized in the figure below.Figure 4: An overview of the nine steps in the Midas Certification process.This certification process is followed on a project-by-project basis for individual data models, which comprise a set of data tables and metrics that correspond to a specific business concept or project feature. Example data models at Airbnb cover subjects such as Active Listings, Customer Service Tickets, and Guest Growth Accounting. While there is no perfect set of criteria to define the boundaries of a given data model, aggregating our data tables, pipelines, and metrics at this level of abstraction allows us to more effectively organize, architect, and maintain our offline data warehouse.While this post won’t describe each step of the certification process in detail, the following sections provide an overview of the most important components of the process.Broad Stakeholder InputAn important feature of the process is the cross-functional partnerships it formalizes. Every Midas model requires a Data Engineering and Data Science owner who share ownership of the data model design and provide expert input from their respective functions. Cross-functional input is pivotal to ensuring certification can address the full scope of data quality dimensions, which span technical implementation concerns as well as requirements for effective business usage and downstream data applications.Furthermore, the process is set up to encourage participation from stakeholders across all teams that consume Midas models. A major goal of certification is ensuring the data models we build meet the data needs of users across the company, rather than just the needs of the team building the model. The certification process gives data consumers from every team the option to sign on as reviewers of new data model designs, and we have found that small requests or feedback early in the design process save substantial time by reducing the need for future revisions.Prior to Midas, these cross-functional, cross-team partnerships were often difficult to form organically. The formal structure provided by a certification process helps streamline collaboration on data design across the company.Design SpecsThe first step in the Midas process is writing a design spec, which serves as both a technical contract describing the pipeline, tables, and metrics that will be built, as well as the primary ongoing documentation for the data model. Design specs follow a shared template with standardized sub-sections. Collectively, these specs form a library of documentation for Airbnb’s offline data assets. This documentation represents a high-value deliverable, as it reduces dependency on data producers’ specialized knowledge, eases future iteration on existing data models, and simplifies transition of data assets between owners.The contents of a design spec are best illustrated with examples. The following figures depict condensed and simplified examples from the design spec for Airbnb’s Active Listings data model.The spec opens with a description of individual and team data model owners, as well as the relevant design reviewers.Fig 5: Owners and reviewers are formalized in the heading for each Midas design spec.The first section of the spec describes the headline metrics included in the data model, along with plain-text business definitions and specific details relevant to interpreting the metrics.Fig 6: An example Metric Definitions section from a Midas design spec.The following section provides a summary of the pipeline used to build the data tables included in the model. This summary includes a simple diagram of input and output tables, an overview pipeline SLA criteria, context on how to backfill historical data, and a short disaster recovery playbook.Fig 7: Example pipeline overview section from a Midas design spec.The overview of the data pipeline is followed by documentation for the table schemas that will be built.Fig 8: Example table schema details from a Midas design spec.Finally, the spec provides an overview of the data quality checks that will be built into the data model’s pipeline for validation (as discussed further below).Fig 9: Example section on data quality checks details from a Midas design spec.The examples above cover the main design spec sections, but are shown in substantially condensed and simplified form. In reality, descriptions of metric and pipeline details are much longer, and some of the more complex design specs exceed 20 pages in length. While this level of documentation requires a large upfront time investment, it ensures data is architected correctly, provides a vehicle for design input from multiple stakeholders, and reduces dependency on the specialized knowledge of a handful of data experts.Data ValidationAfter a design spec has been written and the data pipeline built, the resulting data needs to be validated. There are two groups of data quality checks relied on for validation:Automated checks are built into the data pipeline by a Data Engineer, and described in the design spec. These checks are required for certified data, and cover basic sanity checks, definitional testing, and anomaly detection on new data generated by the pipeline.One-off validation checks against historical data are run by a Data Scientist and documented in a separate validation report. That report summarizes the checks performed, and links to shared data workbooks (e.g. Jupyter Notebook) with code and queries that can be used to re-run the validation whenever a data model is updated. This work covers checks that can not be easily automated in the data pipeline, including more detailed anomaly detection on historical time series, and comparisons against existing data sources or metrics expected to be consistent with the new data model.As with the design specs, this level of validation and documentation requires a larger upfront investment, but substantially reduces data inaccuracies and future bug reports, and makes refreshing the validation easy when the data model evolves in the future.Certification ReviewsCertification reviews are a major component of the Midas process. These third-party reviews are performed by recognized data experts at the company, who are designated as either Data Architects or Metrics Architects. By performing Midas reviews, architects serve as gatekeepers of the company’s data quality.There are four distinct reviews in the Midas process:Spec Review: Review the proposed design spec for the data model, before implementation begins.Data Review: Review the pipeline’s data quality checks and validation report.Code Review: Review the code used to generate the data pipeline.Minerva Review: Review the source of truth metric definitions implemented in Minerva, Airbnb’s metrics service.Collectively, these reviews cover engineering practices and data accuracy across all data assets, and ensure certified data models meet the Midas promise: a gold standard for end-to-end data quality.Bugs and Change RequestsLastly, though not part of the initial pipeline development process, the Midas initiative improved our ability to manage offline data bugs and change requests. Organizing offline data into discrete data models and clarifying ownership allowed us to formalize company-wide processes to address requests from data consumers. Employees can now use a simple form to file tickets for bugs and change requests, a system that was not previously feasible.ConclusionThe Midas initiative has allowed us to define a comprehensive standard for data quality shared across the company. Midas-certified data assets are guaranteed to be accurate, reliable, and cost-efficient, with consistent operational support, and backed by detailed user documentation. As the size of the company and our data warehouse continue to grow at rapid pace, the certification process ensures we are able to provide data consumers with a consistent guarantee for data quality at scale.Midas certification does not come without challenges. In particular, quality takes time. Requirements for documentation, reviews, and input from a broad set of stakeholders mean building a data model to Midas standards is much slower than building uncertified data. Re-architecting data models at scale also requires substantial staffing from data and analytics engineering experts (we’re hiring!), and entails costs for teams to migrate to the new data sources.Offline data is a key technology asset for Airbnb, and this investment is warranted. Certified data models serve as the shared foundation for all data applications, spanning business reporting, product analytics, experimentation, and machine learning and AI. Investing in data quality improves the value of each of these applications, and will improve data-informed decisions at Airbnb for years to come.With special thanks to Aaron Keys, a key partner on the early design and vision of the Midas initiative.Data Quality at Airbnb was originally published in Airbnb Engineering &amp; Data Science on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-11-24 21:04:36","link":"https://medium.com/airbnb-engineering/data-quality-at-airbnb-870d03080469?source=rss----53c7c27702d5---4","blog":{"id":"airbnb","link":"https://medium.com/airbnb-engineering","name":"Airbnb","rssFeed":"https://medium.com/feed/airbnb-engineering","type":"company"},"blogType":"company"},{"id":"92e58c7b7b922ccf34a11e7da20dab57","publish_timestamp":1615912548,"title":"Improving large monorepo performance on GitHub","blogName":"Github","image":"https://github.blog/wp-content/uploads/2019/03/blog-card.png?fit=5001%2C2626","categories":["Engineering"],"description":"Every day, GitHub serves the needs of over 56M developers, working on over 200M code repositories. All but a tiny fraction of those repositories are served with amazing performance, for customers from around the world.","publish_date":"2021-03-16 16:35:48","link":"https://github.blog/2021-03-16-improving-large-monorepo-performance-on-github/","blog":{"id":"github","link":"https://github.blog/category/engineering","name":"Github","rssFeed":"https://github.blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"26530aa0db2530318c3a0c6b7aa2a5f5","publish_timestamp":1615837104,"title":"Highlights from Git 2.31","blogName":"Github","image":"https://github.blog/wp-content/uploads/2021/03/git-2-31-0-release-banner.jpeg?fit=1200%2C630","categories":["Community","Engineering","Open source","Git"],"description":"The open source Git project just released Git 2.31 with features and bug fixes from 85 contributors, 23 of them new. Last time we caught up with you, Git 2.29 had just been released. Two","publish_date":"2021-03-15 19:38:24","link":"https://github.blog/2021-03-15-highlights-from-git-2-31/","blog":{"id":"github","link":"https://github.blog/category/engineering","name":"Github","rssFeed":"https://github.blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"8ce0acca607589a0c54c2663ddbb53bc","publish_timestamp":1614799061,"title":"GitHub Availability Report: February 2021","blogName":"Github","image":"https://github.blog/wp-content/uploads/2019/03/engineering-social.png?fit=1201%2C630","categories":["Engineering"],"description":"Introduction In February, we experienced no incidents resulting in service downtime to our core services. This month’s GitHub Availability Report will provide initial details around an incident from March 1 that caused significant impact and","publish_date":"2021-03-03 19:17:41","link":"https://github.blog/2021-03-03-github-availability-report-february-2021/","blog":{"id":"github","link":"https://github.blog/category/engineering","name":"Github","rssFeed":"https://github.blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"9143c8d348f0aceef04ba901e36b478f","publish_timestamp":1612980033,"title":"New global ID format coming to GraphQL","blogName":"Github","image":"https://github.blog/wp-content/uploads/2021/02/GraphQL.png?fit=2400%2C1260","categories":["Engineering","GraphQL"],"description":"The GitHub GraphQL API has been publicly available for over 4 years now. Its usage has grown immensely over time, and we’ve learned a lot from running one of the largest public GraphQL APIs in","publish_date":"2021-02-10 18:00:33","link":"https://github.blog/2021-02-10-new-global-id-format-coming-to-graphql/","blog":{"id":"github","link":"https://github.blog/category/engineering","name":"Github","rssFeed":"https://github.blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"d8db974ebde1af15434808deca4dfd9f","publish_timestamp":1612465132,"title":"GitHub reduces Marketplace transaction fees, revamps Technology Partner Program","blogName":"Github","image":"https://github.blog/wp-content/uploads/2021/02/105925092-6bca2180-5ff4-11eb-9b56-4ce4158a5f12.png?fit=2388%2C1232","categories":["Engineering","Marketplace","Partners","Product"],"description":"At GitHub, our community is at the heart of everything we do. We want to make it easier to build the things you love, with the tools you prefer to use—which is why we’re committed","publish_date":"2021-02-04 18:58:52","link":"https://github.blog/2021-02-04-github-reduces-marketplace-transaction-fees-revamps-technology-partner-program/","blog":{"id":"github","link":"https://github.blog/category/engineering","name":"Github","rssFeed":"https://github.blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"dacdfc5bdde7f882fb46bf9dfd62d0a5","publish_timestamp":1612375220,"title":"Deployment reliability at GitHub","blogName":"Github","image":"https://github.blog/wp-content/uploads/2019/03/engineering-social.png?fit=1201%2C630","categories":["Engineering"],"description":"Last week, we described how we improved the deployment experience for github.com. When we describe deployments at GitHub, the deployment experience is an important part of what it takes to ship applications to production, especially at GitHub's scale, but there is more to it: the actual deployment mechanics need to be fast and reliable.","publish_date":"2021-02-03 18:00:20","link":"https://github.blog/2021-02-03-deployment-reliability-at-github/","blog":{"id":"github","link":"https://github.blog/category/engineering","name":"Github","rssFeed":"https://github.blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"c9556975c2e31ad069b6bf66ae9e7ae7","publish_timestamp":1612310038,"title":"GitHub Availability Report: January 2021","blogName":"Github","image":"https://github.blog/wp-content/uploads/2019/03/engineering-social.png?fit=1201%2C630","categories":["Engineering","Product"],"description":"Introduction In January, we experienced one incident resulting in significant impact and degraded state of availability for the GitHub Actions service. January 28 04:21 UTC (lasting 3 hours 53 minutes) Our service monitors detected abnormal","publish_date":"2021-02-02 23:53:58","link":"https://github.blog/2021-02-02-github-availability-report-january-2021/","blog":{"id":"github","link":"https://github.blog/category/engineering","name":"Github","rssFeed":"https://github.blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"da4a3de58110f39f074b1222ea81f1e4","publish_timestamp":1611939655,"title":"Making GitHub’s new homepage fast and performant","blogName":"Github","image":"https://github.blog/wp-content/uploads/2021/01/102393310-07478b80-3f8d-11eb-84eb-392d555ebd29.png?fit=1200%2C630","categories":["Engineering","Homepage design"],"description":"This post is the third installment of our five-part series on building GitHub’s new homepage: How our globe is built How we collect and use the data behind the globe How we made the page","publish_date":"2021-01-29 17:00:55","link":"https://github.blog/2021-01-29-making-githubs-new-homepage-fast-and-performant/","blog":{"id":"github","link":"https://github.blog/category/engineering","name":"Github","rssFeed":"https://github.blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"ffd5cfd61eed49174e1ab28261eee7e6","publish_timestamp":1611596768,"title":"Improving how we deploy GitHub","blogName":"Github","image":"https://github.blog/wp-content/uploads/2021/01/card.png?fit=1200%2C630","categories":["Engineering"],"description":"As GitHub doubled it’s developer head count, tooling that worked for us no longer functioned in the same capacity. We aimed to improve the deployment process for all developers at GitHub and mitigate risk associated with deploying one of the largest developer platforms in the world.\n","publish_date":"2021-01-25 17:46:08","link":"https://github.blog/2021-01-25-improving-how-we-deploy-github/","blog":{"id":"github","link":"https://github.blog/category/engineering","name":"Github","rssFeed":"https://github.blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"42b24fe10d45ee9bb749c2d12aae957a","publish_timestamp":1611248432,"title":"The best of Changelog • 2020 Edition","blogName":"Github","image":"https://github.blog/wp-content/uploads/2021/01/Changelog-Best-of-2020.png?fit=3756%2C1836","categories":["Community","Engineering","CICD","GitHub Actions","GitHub for mobile"],"description":"If you haven’t seen it, the GitHub Changelog helps you keep up-to-date with all the latest features and updates to GitHub. We shipped a tonne of changes last year, and it’s impossible to blog about","publish_date":"2021-01-21 17:00:32","link":"https://github.blog/2021-01-21-changelog-2020-edition/","blog":{"id":"github","link":"https://github.blog/category/engineering","name":"Github","rssFeed":"https://github.blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"87191242ff13b04c42957d3608208ca5","publish_timestamp":1616025600,"title":"Optimize GraphQL Server with Lookaheads","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2021/03/images/lookaheads.png","categories":["Zalando","GraphQL","NodeJS"],"description":"GraphQL offers a way to optimize the data between a client and a server. We can use the declarative nature of a GraphQL query to perform lookaheads. Lookaheads provide us a way to optimize the data between the GraphQL server and a backend data provider - like a database or another server that can return partial responses.","publish_date":"2021-03-18 00:00:00","link":"https://engineering.zalando.com/posts/2021/03/optimize-graphql-server-with-lookaheads.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"ee72267a23c2950717243e5137c43017","publish_timestamp":1615852800,"title":"Flexbox Layout Behavior in Jetpack Compose","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2021/03/images/flex-wrap.jpg","categories":["Zalando","Android","Kotlin","UI"],"description":"Much of the layout behavior defined in the flexbox spec has a direct analog in Jetpack Compose.","publish_date":"2021-03-16 00:00:00","link":"https://engineering.zalando.com/posts/2021/03/flexbox-layout-behavior-in-jetpack-compose.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d330c97a9a833d9daf72b90e5c7b36ae","publish_timestamp":1615420800,"title":"Micro Frontends: from Fragments to Renderers (Part 1)","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2021/03/images/architecture_if.png","categories":["Zalando","Frontend","Microservices","GraphQL"],"description":"Moving beyond Project Mosaic. Get an insight into the declarative view composition framework that powers new features for Zalando's website.","publish_date":"2021-03-11 00:00:00","link":"https://engineering.zalando.com/posts/2021/03/micro-frontends-part1.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"9ffa44b5cc9d21fd883ee9163093a02b","publish_timestamp":1614816000,"title":"How we use GraphQL at Europe's largest fashion e-commerce company","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2021/03/images/graphql.png","categories":["Zalando","GraphQL","APIs"],"description":"Managing consistent and backwards-compatible APIs for Web and mobile App frontends is always a complex task in the long-term. At Zalando, we have used GraphQL to solve some of the common problems of frontend data requirements while gaining speed of delivery in a large and quickly growing organisation. This article is about GraphQL as Unified-Backend-For-Frontend (UBFF) application and first in a series of posts about problems we solved with our use of GraphQL at Zalando.","publish_date":"2021-03-04 00:00:00","link":"https://engineering.zalando.com/posts/2021/03/how-we-use-graphql-at-europes-largest-fashion-e-commerce-company.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"9c5eaaba79cc5776374342e0345f38a1","publish_timestamp":1614643200,"title":"Building an End to End load test automation system on top of Kubernetes","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2021/03/images/loadtestconductor-flow.png","categories":["Zalando","Cyber Week","Testing"],"description":"Learn how we built an end-to-end load test automation system to make load tests a routine task.","publish_date":"2021-03-02 00:00:00","link":"https://engineering.zalando.com/posts/2021/03/building-an-end-to-end-load-test-automation-system-on-top-of-kubernetes.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0d51b4b6a05f80c4ccef3ba4c8545afe","publish_timestamp":1614211200,"title":"Integration tests with Testcontainers","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2021/02/images/pyramid-of-testing.png","categories":["Zalando","Java","Testing","Docker"],"description":"We explore how to write integration tests using Testcontainers.org library in Java-based backend applications.","publish_date":"2021-02-25 00:00:00","link":"https://engineering.zalando.com/posts/2021/02/integration-tests-with-testcontainers.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"fe1000e206b50bcee0cb8e0e2029096b","publish_timestamp":1613433600,"title":"A Machine Learning Pipeline with Real-Time Inference","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2021/02/images/payments.png","categories":["Zalando","Artificial Intelligence","AWS","Data Science","Machine Learning"],"description":"How we improved an ML legacy system using Amazon SageMaker","publish_date":"2021-02-16 00:00:00","link":"https://engineering.zalando.com/posts/2021/02/machine-learning-pipeline-with-real-time-inference.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"8d017bce4801e8e82d55ef42f17678ca","publish_timestamp":1613001600,"title":"Find out what challenges Customer Conversion solves at Zalando","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2021/02/images/pascal-hahn.jpg#right","categories":["Zalando","Recruiting","Inside Zalando"],"description":"We have spoken with our Director Customer Conversion, Pascal Hahn to find out more about their Product and to understand what the teams are looking for in the upcoming Hiring Sprint Event","publish_date":"2021-02-11 00:00:00","link":"https://engineering.zalando.com/posts/2021/02/customer-conversion-at-zalando.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f0c9904fe0eb4c7b82d8ed056ea7e0e7","publish_timestamp":1612396800,"title":"It's Never Too Late For a Career Change","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2021/02/images/coding-with-a-cat.jpg","categories":["Zalando","Diversity in Tech","Education","Tech Culture","Tech Jobs","Tour of Mastery","Women in Tech"],"description":"A story of a Business Analyst and Product Manager turning into a Software Engineer.","publish_date":"2021-02-04 00:00:00","link":"https://engineering.zalando.com/posts/2021/02/its-never-too-late-for-a-career-change.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5ce44d846a4060162dc9afd7d26e0770","publish_timestamp":1612224000,"title":"Stop using constants. Feed randomized input to test cases.","blogName":"Zalando","image":"","categories":["Zalando","Testing","iOS","Mobile"],"description":"Most test cases assert using hand typed constants. Leveraging randomized input is a much better approach.","publish_date":"2021-02-02 00:00:00","link":"https://engineering.zalando.com/posts/2021/02/randomized-input-testing-ios.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"07f79411c4081f94aeb1d56e8e098084","publish_timestamp":1611187200,"title":"Creating a uniform landscape for macOS Software","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2021/01/images/preview.png","categories":["Zalando","Apple","Python"],"description":"Here's how we managed to automate the patch management process through the use of JAMF Pro, open source tools and a set of in-house developments to tie these tools together.","publish_date":"2021-01-21 00:00:00","link":"https://engineering.zalando.com/posts/2021/01/creating-a-uniform-landscape-for-mac-software.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"317433e6216fa0e13dc349ce93902f60","publish_timestamp":1610409600,"title":"Experimentation Platform at Zalando: Part 1 - Evolution","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2021/01/images/num_exp.png","categories":["Zalando","Experimentation","Platform Engineering"],"description":"Challenges and solutions of our experimentation platform at Zalando","publish_date":"2021-01-12 00:00:00","link":"https://engineering.zalando.com/posts/2021/01/experimentation-platform-part1.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"14f937745642e6950692336368c7bafe","publish_timestamp":1602115200,"title":"How Zalando prepares for Cyber Week","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2020/10/images/cw-situation-room.jpg","categories":["Zalando","Cyber Week","SRE","Testing"],"description":"Learn how we prepare our platform for Cyber Week - the highest traffic period in the year.","publish_date":"2020-10-08 00:00:00","link":"https://engineering.zalando.com/posts/2020/10/how-zalando-prepares-for-cyber-week.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"69efa3b73cfdc84cafe0a8debd794fd4","publish_timestamp":1599523200,"title":"Meet Boris Malensek, Our Head Of Engineering In Merchant Operations","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2020/09/images/boris-malensek.jpg#right","categories":["Zalando","Inside Zalando"],"description":"We have talked with Boris about his career journey within Zalando, the evolution of Merchant Operations, and the engineering culture within the company.","publish_date":"2020-09-08 00:00:00","link":"https://engineering.zalando.com/posts/2020/09/meet-boris-malensek-head-of-engineering-merchant-operations.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"150a726251120a2bc2d3955d900c8600","publish_timestamp":1594944000,"title":"Inbox Zero is not a Lifestyle","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2020/07/images/tim-laptop-calendar.jpg","categories":["Zalando","Productivity","Leadership","Remote Working"],"description":"Personal productivity is subject of frequent debate and optimization. Learn how to stay organized as a leader and feel accomplished every day.","publish_date":"2020-07-17 00:00:00","link":"https://engineering.zalando.com/posts/2020/07/leading-self.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"daa0809f6eb716061fd0e80f363bf69e","publish_timestamp":1594771200,"title":"Technology Choices at Zalando - Updating our Tech Radar Process","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2020/07/images/zalando-tech-radar.jpg","categories":["Zalando","Leadership","Tech Culture","Tech Radar"],"description":"We have revisited the process of technology selection at Zalando, adjusted the Tech Radar ring semantics, and moved towards principle-based decision making. In this post, we would like to share the process and its outcomes so far.","publish_date":"2020-07-15 00:00:00","link":"https://engineering.zalando.com/posts/2020/07/technology-choices-at-zalando-tech-radar-update.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1f5c7e915fb6ee877630d75afec93d23","publish_timestamp":1593561600,"title":"Launching the Engineering Blog","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2020/07/images/staticgen-pelican.png#center","categories":["Zalando","Engineering Blog","Python","AWS","Kubernetes","Skipper"],"description":"We recently re-launched Zalando's Engineering Blog. Learn how we have set up a blog with a Lighthouse score of 100.","publish_date":"2020-07-01 00:00:00","link":"https://engineering.zalando.com/posts/2020/07/launching-the-engineering-blog.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"ac6761ce3b2d76e04a58f8d34b1cf861","publish_timestamp":1592956800,"title":"PgBouncer on Kubernetes and how to achieve minimal latency","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2020/06/images/pgbouncer-cpu-cores.png","categories":["Zalando","Kubernetes","Open Source","PostgreSQL","Postgres Operator"],"description":"Experiments with connection poolers on Kubernetes for Postgres Operator","publish_date":"2020-06-24 00:00:00","link":"https://engineering.zalando.com/posts/2020/06/postgresql-connection-poolers.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"118cf5ad7952479f258d74696df8d474","publish_timestamp":1592784000,"title":"Learnings from Distributed XGBoost on Amazon SageMaker","blogName":"Zalando","image":"https://engineering.zalando.com/posts/2020/06/images/fullyreplicated.png","categories":["Zalando","Artificial Intelligence","AWS","Machine Learning"],"description":"What I learned from distributed training with XGBoost on Amazon SageMaker.","publish_date":"2020-06-22 00:00:00","link":"https://engineering.zalando.com/posts/2020/06/distributed-xgb-sagemaker.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0b49d95aacd11eebb56a3bf1929661b6","publish_timestamp":1584057600,"title":"How to work remotely at Zalando","blogName":"Zalando","image":"","categories":["Zalando","Remote Working"],"description":"Going fully remote as a company from one day to another is a challenge. Working remotely requires a clear set of “rules to live by” that have 100% buy-in across the company, and a healthy system of meetings, events, and habits that keep people communicating.","publish_date":"2020-03-13 00:00:00","link":"https://engineering.zalando.com/posts/2020/03/how-to-work-remotely-at-zalando.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"cafdca47198593da5dec7287e7c9a9eb","publish_timestamp":1557964800,"title":"Understanding Redis Background Memory Usage","blogName":"Zalando","image":"","categories":["Zalando","Redis"],"description":"A closer look at how the Linux kernel influences Redis memory management","publish_date":"2019-05-16 00:00:00","link":"https://engineering.zalando.com/posts/2019/05/understanding-redis-background-memory-usage.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"91f5d7fb67ab9d211bd3ba087f923aad","publish_timestamp":1557360000,"title":"Back-Pressure Strategy for a Sharded Akka Cluster","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/d30f329247f41cd032faf20ae3d0d779cc03f540_1_wb9vyueprcbh3ybtszkmvw.jpeg?auto=compress,format","categories":["Zalando","Akka","Kubernetes","Scala"],"description":"AWS SQS polling from sharded Akka Cluster running on Kubernetes","publish_date":"2019-05-09 00:00:00","link":"https://engineering.zalando.com/posts/2019/05/back-pressure-strategy-for-a-sharded-akka-cluster.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"ad2b90fb0fdbd9f8d617e4ad5082b82e","publish_timestamp":1556841600,"title":"How to Manage Stakeholder Requests in Big Organizations","blogName":"Zalando","image":"","categories":["Zalando"],"description":"Explaining our Facilitator Role","publish_date":"2019-05-03 00:00:00","link":"https://engineering.zalando.com/posts/2019/05/how-to-manage-stakeholder-requests-in-big-organizations.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"97958024120d78b35237acbd66eefb73","publish_timestamp":1556150400,"title":"Learning DevOps as a Software Engineer","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/2cdba264cd05222e2dbfa39b8befd438e1e97f62_fraud-cockpit-architecture---before-migration-to-kubernetes.png?auto=compress,format","categories":["Zalando","Kubernetes"],"description":"How developing DevOps skills as a Software Engineer helps you to grow and become a better Engineer.","publish_date":"2019-04-25 00:00:00","link":"https://engineering.zalando.com/posts/2019/04/learning-devops-as-a-software-engineer.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"8716934224823bd2ac4b02c06c07a0f9","publish_timestamp":1555545600,"title":"How to set an ideal thread pool size","blogName":"Zalando","image":"","categories":["Zalando","Java"],"description":"How to get the most out of java thread pool","publish_date":"2019-04-18 00:00:00","link":"https://engineering.zalando.com/posts/2019/04/how-to-set-an-ideal-thread-pool-size.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"dda95e602d3f49679756d3a1cc639414","publish_timestamp":1554940800,"title":"End-to-end load testing Zalando’s production website","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/272697ee7923d8c67cbda1b3bde0c3fa2dd02081_har_to_locust.png?auto=compress,format","categories":["Zalando","Cyber Week","SRE","Testing"],"description":"How we made sure we stayed online for Black Friday 2018","publish_date":"2019-04-11 00:00:00","link":"https://engineering.zalando.com/posts/2019/04/end-to-end-load-testing-zalandos-production-website.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e30475df2aae1d5dce1cd21ca7051e59","publish_timestamp":1554336000,"title":"Developing Zalando APIs","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/744bc650cffbdf996f309d276f1830f6bd26f123_api-portal.png?auto=compress,format","categories":["Zalando","APIs"],"description":"How Zalando software engineers develop internal and external APIs","publish_date":"2019-04-04 00:00:00","link":"https://engineering.zalando.com/posts/2019/04/developing-zalando-apis.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e75c61e5e09c0dfb5e5f102de32eceab","publish_timestamp":1553731200,"title":"A Story of Rust","blogName":"Zalando","image":"","categories":["Zalando","Rust"],"description":"Introducing Rust in an Enterprise Environment","publish_date":"2019-03-28 00:00:00","link":"https://engineering.zalando.com/posts/2019/03/story-rust.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a277d06e1e7024383a9097dede27132d","publish_timestamp":1553212800,"title":"Running Apache Flink on Kubernetes","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/e4659175b2b5f3efbfca911300b6e9828368c687_1.png?auto=compress,format","categories":["Zalando","Apache Flink","Kubernetes"],"description":"What I learned deploying Flink and a stream processing application on Kubernetes","publish_date":"2019-03-22 00:00:00","link":"https://engineering.zalando.com/posts/2019/03/running-apache-flink-on-kubernetes.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"54f7896b0f0431bb1043bcba459127a3","publish_timestamp":1552521600,"title":"Rotating Engineers at Zalando","blogName":"Zalando","image":"","categories":["Zalando"],"description":"Rotating engineers to establish cross-functional knowledge sharing, encourage cross team collaboration, and bring greater product awareness.","publish_date":"2019-03-14 00:00:00","link":"https://engineering.zalando.com/posts/2019/03/rotating-engineers-at-zalando.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"743efa1886af206304ed32274ebb7c09","publish_timestamp":1552262400,"title":"How to Rock your Next Product Training","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/486a121a97bfdf0a7bd8d5311eeb9911dc12bf46_picture1_corrected.png?auto=compress,format","categories":["Zalando","Product Management"],"description":"Need to introduce end-users into your product? It can be fun: we show you how","publish_date":"2019-03-11 00:00:00","link":"https://engineering.zalando.com/posts/2019/03/how-to-rock-your-next-product-training.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d54cd38e24a31096ce1c5ae6d3145ebd","publish_timestamp":1551312000,"title":"How to Make Space for Research & Innovation?","blogName":"Zalando","image":"","categories":["Zalando","Coaching","Culture"],"description":"Redesigning research and product development so that the explorative nature of data science becomes a driver for innovation","publish_date":"2019-02-28 00:00:00","link":"https://engineering.zalando.com/posts/2019/02/make-space-research-innovation.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f6326a37921a766eae84c99e8c4ac25e","publish_timestamp":1550707200,"title":"A Journey On End To End Testing A Microservices Architecture","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/23dedd00f618579555479be132e249ec966ce4a4_untitled-diagram-1.jpg?auto=compress,format","categories":["Zalando","Microservices"],"description":"In microservices architecture there are different components working together to enable a business capability, therefore testing all of them can get tricky.","publish_date":"2019-02-21 00:00:00","link":"https://engineering.zalando.com/posts/2019/02/end-to-end-microservices.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a74ee51593cdf004a2d6158735501752","publish_timestamp":1550102400,"title":"Typescript Best Practices","blogName":"Zalando","image":"","categories":["Zalando","TypeScript"],"description":"Learning Typescript as a backender","publish_date":"2019-02-14 00:00:00","link":"https://engineering.zalando.com/posts/2019/02/typescript-best-practices.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3921143a5f9bf0383454ecb8bd016a0d","publish_timestamp":1549497600,"title":"On the Effectiveness of Online Marketing","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/523fcf067cc35347ab7cc4043c4f8a941428fa5b_blog_post_1.png?auto=compress,format","categories":["Zalando","Data Science"],"description":"Measuring the incremental effect of online marketing to optimize advertising investment","publish_date":"2019-02-07 00:00:00","link":"https://engineering.zalando.com/posts/2019/02/effectiveness-online-marketing.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"22b733141899b2b19d65ab94e5331870","publish_timestamp":1548892800,"title":"The Product Playbook","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/f7fbfb0f4570eb7cee215c7ad702db9826993d01_ashton-clark-424090-unsplash.jpg?auto=compress,format","categories":["Zalando","Culture","Design","Product","Product Management","UX"],"description":"Shared language and visualizing to deliver great products","publish_date":"2019-01-31 00:00:00","link":"https://engineering.zalando.com/posts/2019/01/product-playbook.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"ab1ece62f95c8a648dee23010c131289","publish_timestamp":1548288000,"title":"A Day in the Life of a Frontend Engineer at Zalando","blogName":"Zalando","image":"","categories":["Zalando","Frontend"],"description":"How we work at Zalando","publish_date":"2019-01-24 00:00:00","link":"https://engineering.zalando.com/posts/2019/01/frontend-engineer-zalando.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d789f8f14f1f6bdd7ab48832b42b152d","publish_timestamp":1547078400,"title":"The Magic Coaching Wand","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/67504ff1b73ac0b381c5c32a64e60bc4c30ec630_screen-shot-2018-12-19-at-4.29.11-pm.png?auto=compress,format","categories":["Zalando","Coaching","Tech Culture"],"description":"How the Zalando Personalization Unit improved with a diagnostic","publish_date":"2019-01-10 00:00:00","link":"https://engineering.zalando.com/posts/2019/01/magic-coaching-wand.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f848e013f3f346e8e162c2875797b476","publish_timestamp":1544054400,"title":"Front-End Micro Services","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/933f056972448facab5a63ded6571388f24783ff_html_page_fragments.png?auto=compress,format","categories":["Zalando","Frontend","Microservices"],"description":"Fragments: limitations, solutions and our approach","publish_date":"2018-12-06 00:00:00","link":"https://engineering.zalando.com/posts/2018/12/front-end-micro-services.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"74417c0c51fadd2894fd9ee95aa99b1a","publish_timestamp":1543449600,"title":"Tag-based Navigation of a Fashion Catalog","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/ae212856e289ba9dd5d264ebcd7e9ce4819834bd_screen-shot-2018-11-29-at-7.26.10-pm.png?auto=compress,format","categories":["Zalando","Fashion Insights Centre","Fashion Search","Research"],"description":"Exploring the Zalando Assortment by Browsing a Product Similarity Graph","publish_date":"2018-11-29 00:00:00","link":"https://engineering.zalando.com/posts/2018/11/exploring-fashion-catalog.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"452c4ce939248e717eb27adc8135e795","publish_timestamp":1542844800,"title":"Zalando Research Releases “Flair”","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/8496bef09500a26c88a6f42cb06576b99e7410f7_screen-shot-2018-11-22-at-5.47.19-pm.png?auto=compress,format","categories":["Zalando","Artificial Intelligence","Deep Learning","Machine Learning","Zalando Research"],"description":"Open sourcing machine learning research for natural language processing (NLP)","publish_date":"2018-11-22 00:00:00","link":"https://engineering.zalando.com/posts/2018/11/zalando-research-releases-flair.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"cfde4461fb0a74829610cc83e786f06a","publish_timestamp":1541635200,"title":"Train Deep Learning Models on AWS","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/10ebee3fca91065c05f120ae48aebcc6fc99880f_screen-shot-2018-11-06-at-4.07.37-pm.png?auto=compress,format","categories":["Zalando","AWS","Deep Learning","Docker","Machine Learning","TensorFlow"],"description":"A real-life example of how to train a Deep Learning model on an AWS Spot Instance using Spotty","publish_date":"2018-11-08 00:00:00","link":"https://engineering.zalando.com/posts/2018/11/train-deep-learning-models-aws.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"4779728541ea899ffb9d6c4f8d7b1f63","publish_timestamp":1541030400,"title":"#NoEstimates","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/2a74982d8e316bee3b98cebd72cd8f9c7cb5f293_screen-shot-2018-11-01-at-8.03.22-pm.png?auto=compress,format","categories":["Zalando","Estimates"],"description":"Why I advocate a practice of no estimates as a software engineer","publish_date":"2018-11-01 00:00:00","link":"https://engineering.zalando.com/posts/2018/11/no-estimates.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d948f212f25b18be143ebc7bb7308ec9","publish_timestamp":1540425600,"title":"Singleton Types","blogName":"Zalando","image":"","categories":["Zalando"],"description":"A Scala 3 Experiment","publish_date":"2018-10-25 00:00:00","link":"https://engineering.zalando.com/posts/2018/10/scala-three-experiment.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1f589235c264e8b91d9ed425cf5e518c","publish_timestamp":1539820800,"title":"Growing a Product Area at Zalando","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/210b631ffde6c599c4895166c20eee942ccd663d_screen-shot-2018-10-18-at-6.33.43-pm-1.png?auto=compress,format","categories":["Zalando","Project Management"],"description":"The six month journey of the customer inbox multi-disciplinary team","publish_date":"2018-10-18 00:00:00","link":"https://engineering.zalando.com/posts/2018/10/growing-product-area.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f197b4e7ab9d22962159c8c1044254b6","publish_timestamp":1539129600,"title":"A Team for Teams","blogName":"Zalando","image":"","categories":["Zalando"],"description":"How we revolutionized the way we worked agile","publish_date":"2018-10-10 00:00:00","link":"https://engineering.zalando.com/posts/2018/10/a-team-for-teams.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"909d57e9b0543b3eb5f4c947877c3d3d","publish_timestamp":1538611200,"title":"Four Pillars Of Leading People","blogName":"Zalando","image":"","categories":["Zalando","Leadership","Zalando Lisbon"],"description":"Essential building blocks for strong leadership that enables people to grow and achieve results","publish_date":"2018-10-04 00:00:00","link":"https://engineering.zalando.com/posts/2018/10/four-pillars-leadership.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"fe2e32e067c529e890e2990ac0f34557","publish_timestamp":1538006400,"title":"The Journey to Connecting Retail","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/eb8872a541bd06e77dacbcd0b992cc290c1ad05b_screen-shot-2018-09-26-at-10.40.53-am.png?auto=compress,format","categories":["Zalando","Product","Product Management","Zalando Helsinki"],"description":"Digitizing brick &amp; mortar fashion stores through Connected Retail","publish_date":"2018-09-27 00:00:00","link":"https://engineering.zalando.com/posts/2018/09/journey-connecting-retail.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"fbc34098cc6faa78a18deb9141f2e930","publish_timestamp":1536710400,"title":"Shop the Look with Deep Learning","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/db91decfc77b8748802ef383e2dc3fa5825130cc_screen-shot-2018-09-12-at-2.50.10-pm.png?auto=compress,format","categories":["Zalando","Artificial Intelligence","Computer Vision","Deep Learning","Machine Learning"],"description":"Retrieving fashion products based on a query image","publish_date":"2018-09-12 00:00:00","link":"https://engineering.zalando.com/posts/2018/09/shop-look-deep-learning.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5f615f9419293b1523ff1e10348206cb","publish_timestamp":1536192000,"title":"Visual Creation and Exploration at Zalando Research","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/d4c69c85c1ce205c79e27610c755eee7fd4a78e4_screen-shot-2018-09-06-at-18.03.01.png?auto=compress,format","categories":["Zalando"],"description":"Adversarial texture distribution learning as a tool of artistic expression","publish_date":"2018-09-06 00:00:00","link":"https://engineering.zalando.com/posts/2018/09/texture-distribution-artistic-expression.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b74c5ec66aa883b33cf3d06b83cfff22","publish_timestamp":1535587200,"title":"Three Years of our Helsinki Tech Hub","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/74c46fb53b1f4d370a4407a228de242eb78ef0ea_10.-aeaf5101-24e9-4436-98b9-8da195d44fdd.jpg?auto=compress,format","categories":["Zalando","Zalando Helsinki"],"description":"Getting to know our Finnish tech hub as it turns three","publish_date":"2018-08-30 00:00:00","link":"https://engineering.zalando.com/posts/2018/08/three-years-of-our-helsinki-tech-hub.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3be10d74e5b804bafc22e88ba058cb08","publish_timestamp":1534982400,"title":"Zalando at the DatSci Awards 2018","blogName":"Zalando","image":"","categories":["Zalando","DatScis","Zalando Dublin"],"description":"Building data science products in multi disciplinary teams","publish_date":"2018-08-23 00:00:00","link":"https://engineering.zalando.com/posts/2018/08/data-science-products-multi-disciplinary-teams.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e0d73d34d10175a79fe1107e43fbc9ff","publish_timestamp":1534377600,"title":"Battle of the Frameworks","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/bc2b7168c6597ec10b2fa46361cc2b3d4656352a_screen-shot-2018-08-15-at-11.26.37.png?auto=compress,format","categories":["Zalando","Frameworks"],"description":"How to choose a JavaScript framework?","publish_date":"2018-08-16 00:00:00","link":"https://engineering.zalando.com/posts/2018/08/battle-frameworks.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"fc9826020345f3f254056f68b4554e98","publish_timestamp":1533772800,"title":"The Future of Data Science","blogName":"Zalando","image":"","categories":["Zalando","Artificial Intelligence"],"description":"Debunking the myth of the data science bubble","publish_date":"2018-08-09 00:00:00","link":"https://engineering.zalando.com/posts/2018/08/future-data-science.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"fd30f6f9b32d3cd48f67bdc5d160e241","publish_timestamp":1533168000,"title":"Agile Principles Over Frameworks","blogName":"Zalando","image":"","categories":["Zalando"],"description":"Embracing the diverse in working agile","publish_date":"2018-08-02 00:00:00","link":"https://engineering.zalando.com/posts/2018/08/agile-principles-over-frameworks.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"7d9edb120d6d86bdaddcdff20aa8e722","publish_timestamp":1532563200,"title":"Agile in People Operations","blogName":"Zalando","image":"","categories":["Zalando"],"description":"Applying agile frameworks to HR processes","publish_date":"2018-07-26 00:00:00","link":"https://engineering.zalando.com/posts/2018/07/agile-people-operations.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"8bf0fe4a5a53b9141ac808c4918449e3","publish_timestamp":1531958400,"title":"Lean Testing, or Why Unit Tests are Worse than You Think","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/2050dbc47a11b89960c3656258b20a99b0e6ce46_1_8c333d_ynehg4q3udb1wta.jpeg?auto=compress,format","categories":["Zalando","TDD","Testing"],"description":"An economic perspective on testing","publish_date":"2018-07-19 00:00:00","link":"https://engineering.zalando.com/posts/2018/07/economic-perspective-testing.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"dd604200ab76ca467ec8fdf2ede57967","publish_timestamp":1531353600,"title":"Styling-API Reinvented","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/3469fa5373c726adc3bc41d813500d93d1899cc6_decoupled-styling-in-ui-components---diagram-1.png?auto=compress,format","categories":["Zalando","CSS","UI","UX"],"description":"Decoupled styling in UI components","publish_date":"2018-07-12 00:00:00","link":"https://engineering.zalando.com/posts/2018/07/decoupled-styling-ui-components.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"9c1954ee202fa8bf2b69980cd38322ab","publish_timestamp":1531180800,"title":"Dortmund Turns Six","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/2727433a01b5f136b381fd4ec19a441a6d794f02_dortmund-six.jpg?auto=compress,format","categories":["Zalando","Culture","Zalando Dortmund"],"description":"Zalando’s maiden tech hub celebrates in style","publish_date":"2018-07-10 00:00:00","link":"https://engineering.zalando.com/posts/2018/07/dortmund-turns-six.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"95571848a9c6fdedc34d09580ecdb78f","publish_timestamp":1530748800,"title":"Utilizing the Finite State Machine","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/5ed72f8d040105aef40e7a9fc533049a9ff38a65_image.png?auto=compress,format","categories":["Zalando","Frontend","NodeJS","Redux"],"description":"How using a State Machine saved our apps &amp; flows from refactoring","publish_date":"2018-07-05 00:00:00","link":"https://engineering.zalando.com/posts/2018/07/utilizing-finite-state-machine.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3964d4d2985f14354b2e24d953b0f4b2","publish_timestamp":1530144000,"title":"The State of Open Source","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/b68cc29b47f7cf3a876067182167021deea4254b_screen-shot-2018-06-27-at-18.13.15.png?auto=compress,format","categories":["Zalando","Connexion","Kubernetes","Nakadi","Open Source","Patroni"],"description":"The evolution and future of open source at Zalando","publish_date":"2018-06-28 00:00:00","link":"https://engineering.zalando.com/posts/2018/06/state-of-open-source.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3d4d13629d63c96d87834d11c281a7a7","publish_timestamp":1529539200,"title":"The Intrapreneurship Journey at Zalando","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/3aa26a46a683c22ec23f0336f5733b331ccac046_screen-shot-2018-06-21-at-18.54.08.png?auto=compress,format","categories":["Zalando","Culture","Hack Week","Innovation"],"description":"Sharing our innovation stories: success, failures, and learnings","publish_date":"2018-06-21 00:00:00","link":"https://engineering.zalando.com/posts/2018/06/innovation-intrapreneurship.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"746f32fff540d70622eef4063e071fdd","publish_timestamp":1528934400,"title":"All Aboard","blogName":"Zalando","image":"","categories":["Zalando","Guild","Onboarding"],"description":"What new tech employees can expect from Zalando onboarding","publish_date":"2018-06-14 00:00:00","link":"https://engineering.zalando.com/posts/2018/06/all-aboard.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b016cf707d3237d7bd8180782ad76bc3","publish_timestamp":1528675200,"title":"Loading Time Matters","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/a4f168469326f3b9bd35f3535c6f45b6523382e3_profiling-desktop-before.png?auto=compress,format","categories":["Zalando","JavaScript","SRE"],"description":"How Zalando's overall site speed improved by more than 25% in five months","publish_date":"2018-06-11 00:00:00","link":"https://engineering.zalando.com/posts/2018/06/loading-time-matters.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"6fd5e46e2d2750023e8dbccae7fba17f","publish_timestamp":1527724800,"title":"State Management in React","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/59db9c034220aaeeb0e0bc4da677bc8bb36ff850_screen-shot-2018-05-31-at-14.15.39.png?auto=compress,format","categories":["Zalando","MobX","React","Redux"],"description":"Comparing Redux, MobX &amp; setState in React","publish_date":"2018-05-31 00:00:00","link":"https://engineering.zalando.com/posts/2018/05/state-management-react.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b3f488bf26df080d19f4561dd70b88fb","publish_timestamp":1526515200,"title":"Scaling Agile at Zalando","blogName":"Zalando","image":"","categories":["Zalando"],"description":"Sharing successful large scale agile experiences","publish_date":"2018-05-17 00:00:00","link":"https://engineering.zalando.com/posts/2018/05/scaling-agile-zalando.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e74e1f9db2bdabdf92f8e87eb4d14250","publish_timestamp":1526342400,"title":"Dublin’s Data Science Guild","blogName":"Zalando","image":"","categories":["Zalando","Culture","Data Science","Guild","Zalando Dublin"],"description":"How to establish and evolve your data science community","publish_date":"2018-05-15 00:00:00","link":"https://engineering.zalando.com/posts/2018/05/dublin-data-science-guild.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f47cf4d446bed7bb7fb27ba2e7102da2","publish_timestamp":1525824000,"title":"How to Make Product Management for Enterprise Systems Work","blogName":"Zalando","image":"","categories":["Zalando","Product Management"],"description":"Moving from a more traditional internal IT setup to a product-driven culture","publish_date":"2018-05-09 00:00:00","link":"https://engineering.zalando.com/posts/2018/05/product-management-enterprise-systems.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e6b03e68ee17af2a098e788d51f90baa","publish_timestamp":1525737600,"title":"Many-to-Many Relationships Using Kafka","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/0d9717fd95a7798e3b15f7cb2d7d4cc049cd3342_screen-shot-2018-05-08-at-11.53.08.png?auto=compress,format","categories":["Zalando","Apache Kafka","Event Driven","Zalando Dublin"],"description":"Real-time joins in event-driven microservices","publish_date":"2018-05-08 00:00:00","link":"https://engineering.zalando.com/posts/2018/05/many-to-many-using-kafka.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"8340ad8faae2ebaa1e6322915cf7e358","publish_timestamp":1525305600,"title":"Investing in the Future of Engineering and Design","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/61eb6e69f0cfb937907da0cd5b4acc3821becfa3_code-students.jpg?auto=compress,format","categories":["Zalando","Hack Week"],"description":"Our cooperation with CODE University","publish_date":"2018-05-03 00:00:00","link":"https://engineering.zalando.com/posts/2018/05/code-university.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"18be44b49051eae265e68eac42e9e007","publish_timestamp":1525219200,"title":"Our Dublin Tech Hub Turns Three","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/4d8222c1350f3db74123155747dbcd746e2fe4a3_zalando-cakes.png?auto=compress,format","categories":["Zalando","Culture","Zalando Dublin"],"description":"Celebrating Zalando’s first international tech hub","publish_date":"2018-05-02 00:00:00","link":"https://engineering.zalando.com/posts/2018/05/dublin-tech-hub-three.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"163863755e31225097203799f445e2ca","publish_timestamp":1524700800,"title":"Short Story of a Long Migration","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/2204c2f0585e859f4225cd16ef9b07620f39d1d4_screen-shot-2018-04-22-at-19.07.48.png?auto=compress,format","categories":["Zalando","Java","Logistics","Monolith"],"description":"How we migrated the Zalando Logistics Operating Services to Java 8","publish_date":"2018-04-26 00:00:00","link":"https://engineering.zalando.com/posts/2018/04/migrating-java-8.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a1c3928c4c7f1e735f70c17c9c423cf6","publish_timestamp":1524528000,"title":"Improving Efficiency in Offline Campaigns","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/c989b44273b15b03999f91c15b45a618958eca46_screen-shot-2018-04-24-at-09.38.15.png?auto=compress,format","categories":["Zalando","APIs","Zalando Dortmund"],"description":"How Zalando uses an API to drive marketing profitability","publish_date":"2018-04-24 00:00:00","link":"https://engineering.zalando.com/posts/2018/04/improving-efficiency-offline-campaigns.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"fceafc2741224eb87d1a9f4b9f2cd298","publish_timestamp":1524096000,"title":"Distributed Cache","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/d46f8bfb09aab2c504e7b82b0e72a075141af11b_screen-shot-2018-02-25-at-3.10.04-pm.png?auto=compress,format","categories":["Zalando","Akka","Docker","Kubernetes","Scala"],"description":"Using Akka cluster-sharding and Akka HTTP on Kubernetes","publish_date":"2018-04-19 00:00:00","link":"https://engineering.zalando.com/posts/2018/04/distributed-cache-akka-kubernetes.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"693645a07dde21dfde768a5ad4cb31d2","publish_timestamp":1523923200,"title":"The Democratization of ‘Data Science As A Service’","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/d16a15532e04094a61e29a39c2b222e21109a8ca_datasciencequestions.png?auto=compress,format","categories":["Zalando","Data Science","Zalando Dublin"],"description":"How data science is becoming available ‘for the good of all’ businesses","publish_date":"2018-04-17 00:00:00","link":"https://engineering.zalando.com/posts/2018/04/democratization-data-science.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"52993182abda98ad33ac0d3abd42129b","publish_timestamp":1523491200,"title":"Discovering Design Sprints","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/74559b6dfcb28e7fe0e7024bad9ab86e575e8d09_screen-shot-2018-04-10-at-10.25.18.png?auto=compress,format","categories":["Zalando","Design Sprint","Product Management","Prototyping","UX"],"description":"Our experience of The Sprint","publish_date":"2018-04-12 00:00:00","link":"https://engineering.zalando.com/posts/2018/04/discovering-design-sprints.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"ece6067c738e5fa4dbf2efa90528d741","publish_timestamp":1523318400,"title":"Managing Personalized Products","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/99748b090362f7eb24dcefaf2a27936d5c385a14_screen-shot-2018-04-10-at-09.31.33.png?auto=compress,format","categories":["Zalando","Product Management","Zalando Helsinki"],"description":"A product manager's insights on customization","publish_date":"2018-04-10 00:00:00","link":"https://engineering.zalando.com/posts/2018/04/managing-personalized-products.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"09cc3cfba67681743d3e142b4abc977d","publish_timestamp":1522886400,"title":"The Perks of Being in a Hackathon","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/d1f12d5bf34406ab1afd4753d7c726a04774eaa9_screen-shot-2018-04-05-at-17.11.03.png?auto=compress,format","categories":["Zalando","Hackathon","Meetup"],"description":"How stepping out of our comfort zone led to a hackathon victory","publish_date":"2018-04-05 00:00:00","link":"https://engineering.zalando.com/posts/2018/04/perks-of-hackathon.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1c8e7617278a45128f5615915d35dd8e","publish_timestamp":1522281600,"title":"Cross-Department Hackathons at Zalando","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/f3b2168e3ebaa7848d4a0031048cd72aa52af2f3_zalando-hack-week---23-march-2018---image-copyright-dan-taylor---dandantaylorphotography.com-4.jpg?auto=compress,format","categories":["Zalando","Hack Week"],"description":"Making Innovative Ideas a Reality","publish_date":"2018-03-29 00:00:00","link":"https://engineering.zalando.com/posts/2018/03/cross-department-hackathons.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"6ab0ffa441d18c9f038a30f166c7acd6","publish_timestamp":1522108800,"title":"Discovering a Future in Tech","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/90096463cdc29607b777735b7664eb4fb9cb9249_image1.jpg?auto=compress,format","categories":["Zalando","Culture","Zalando Helsinki"],"description":"How former Zalando trainee, Anriika Kauppi, found her calling","publish_date":"2018-03-27 00:00:00","link":"https://engineering.zalando.com/posts/2018/03/discovering-future-tech.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"c6a05d692d7743033e6a4dcf24ccc6a0","publish_timestamp":1521676800,"title":"In Praise of TypeScript","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/e5f3879aec28d1e10e84581c6af8b67182a56de1_screen-shot-2018-03-22-at-14.02.02.png?auto=compress,format","categories":["Zalando","APIs","NodeJS","TypeScript"],"description":"Insights on making NodeJS APIs great","publish_date":"2018-03-22 00:00:00","link":"https://engineering.zalando.com/posts/2018/03/make-node-apis-great-typescript.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b4dc2dcb5c20e1f1ef6ee2fdd0cfce8c","publish_timestamp":1521504000,"title":"The Art of Ontology","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/d17083dedbdee3ad8824a4875ce038df35e45718_katariina.jpg?auto=compress,format","categories":["Zalando","Zalando Helsinki"],"description":"Introducing Semantic Web Technologies at Zalando","publish_date":"2018-03-20 00:00:00","link":"https://engineering.zalando.com/posts/2018/03/semantic-web-technologies.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3935662a6d6cda39622fb3a81d48e211","publish_timestamp":1521072000,"title":"Why MobX?","blogName":"Zalando","image":"","categories":["Zalando"],"description":"Removing the burden of state management","publish_date":"2018-03-15 00:00:00","link":"https://engineering.zalando.com/posts/2018/03/why-mobx.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"c73f139b2cb8d601f62bd7f00c4c36f3","publish_timestamp":1520899200,"title":"Zalando Tech: Lisbon","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/1a58748b79d065038e10dbf705ebea71ab82a301_2018-01-20-photo-00003432.jpg?auto=compress,format","categories":["Zalando","Culture","Recruiting","Zalando Lisbon"],"description":"Engineering Lead, Michael Duergner on the company’s newest tech hub","publish_date":"2018-03-13 00:00:00","link":"https://engineering.zalando.com/posts/2018/03/interview-michael-duergner-lisbon.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a8319538af50f1f0b2caab40efe86f62","publish_timestamp":1520467200,"title":"How to Spot a Bad Product","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/2c1f37b5fe3d4cbc671b4056f5260c95471b5a9b_screen-shot-2018-02-27-at-11.57.25.png?auto=compress,format","categories":["Zalando"],"description":"Red flags to look out for in badly written projects.","publish_date":"2018-03-08 00:00:00","link":"https://engineering.zalando.com/posts/2018/03/how-to-spot-bad-product.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"2b38de911cb6ffccd9d75ae3741e12d0","publish_timestamp":1520294400,"title":"Just Run a Game Day","blogName":"Zalando","image":"","categories":["Zalando","Testing","Zalando Dublin"],"description":"Scaling operational excellence","publish_date":"2018-03-06 00:00:00","link":"https://engineering.zalando.com/posts/2018/03/just-run-game-day.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"7ef2fa4edd13e57bc20213776b828b53","publish_timestamp":1519862400,"title":"Data Analysis with Spark","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/6df7c8b1bd1f26174a773f388fd68957b26f6faa_screen-shot-2018-02-26-at-12.10.53.png?auto=compress,format","categories":["Zalando","Apache Spark","Big Data","Data Analytics","Data Science","Hadoop"],"description":"Apache’s lightning fast engine for data analysis and machine learning","publish_date":"2018-03-01 00:00:00","link":"https://engineering.zalando.com/posts/2018/03/data-analysis-spark.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"482fe2d69609a6b088e6724beaf53774","publish_timestamp":1519257600,"title":"Zalando @ FOSDEM","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/6168e5849361304dd695688768ae8ae96683510b_screen-shot-2018-02-22-at-11.46.29.png?auto=compress,format","categories":["Zalando","FOSDEM","Nakadi","Open Source"],"description":"Why FOSDEM is not your average conference","publish_date":"2018-02-22 00:00:00","link":"https://engineering.zalando.com/posts/2018/02/fosdem-not-average-conference.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"30eb4b96f02f7b43c0fb007b455f6510","publish_timestamp":1519084800,"title":"Innovation in Digital Experience","blogName":"Zalando","image":"","categories":["Zalando","Design Sprint","Hack Week","Product Management","Zalando Dublin"],"description":"Multi-functional teams make for a greater customer journey","publish_date":"2018-02-20 00:00:00","link":"https://engineering.zalando.com/posts/2018/02/innovation-digital-experience.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"931460c505fb388ceff82f6d194b5bd4","publish_timestamp":1518652800,"title":"Five Minutes from Machine Learning to RESTful API","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/05160cb8669931e919845f54036181c8193f0db3_article_1.png?auto=compress,format","categories":["Zalando","APIs","Connexion"],"description":"The benefits of Connexion: Zalando’s open source API-First framework","publish_date":"2018-02-15 00:00:00","link":"https://engineering.zalando.com/posts/2018/02/connexion-zalando-open-source.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"70e9b68f43191f0e4dfda50488670b74","publish_timestamp":1518048000,"title":"Cross-Lingual End-to-End Product Search with Deep Learning","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/96c3241266029744b470f7914ef27e2e8e620cea_screen-shot-2018-01-25-at-18.12.13.png?auto=compress,format","categories":["Zalando","Data Science","Fashion Search"],"description":"How We Built the Next Generation Product Search from Scratch using a Deep Neural Network","publish_date":"2018-02-08 00:00:00","link":"https://engineering.zalando.com/posts/2018/02/search-deep-neural-network.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d933ef9520a5c3b08723e759bc88a1cc","publish_timestamp":1517875200,"title":"Crushing AVRO Small Files with Spark","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/a4a45a32fb7066a4db6ec5bfa4416b546f05faff_screen-shot-2018-02-06-at-13.12.57.png?auto=compress,format","categories":["Zalando","AWS","Big Data","Hadoop","Machine Learning","Zalando Dublin"],"description":"Solving the many small files problem for AVRO","publish_date":"2018-02-06 00:00:00","link":"https://engineering.zalando.com/posts/2018/02/solving-many-small-files-avro.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"022bae764b2038d4cf3fd4b68ca25d1e","publish_timestamp":1517443200,"title":"Rabbit in the Cloud","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/65f1f55b431a11679d2d6db3ecf6a5d57a169bd1_screen-shot-2018-01-11-at-11.48.50.png?auto=compress,format","categories":["Zalando","AWS"],"description":"How we deployed RabbitMQ on AWS","publish_date":"2018-02-01 00:00:00","link":"https://engineering.zalando.com/posts/2018/02/rabbit-in-the-cloud.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"32bdda0f23097785965d5cb605629d78","publish_timestamp":1516838400,"title":"Building a Better Tech Radar","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/237e128065613d0c7eac0390e41742c7de6f600b_fencing.jpg?auto=compress,format","categories":["Zalando","JavaScript","Tech Radar"],"description":"How Zalando helps its engineering teams navigate the tech landscape","publish_date":"2018-01-25 00:00:00","link":"https://engineering.zalando.com/posts/2018/01/building-tech-radar.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0c25e673540e7d9be92c35e53eda9638","publish_timestamp":1516665600,"title":"Simplicity by Distributing Complexity","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/f37a5ef01d580d415c1d9ae43e7bb66cd72990ba_hierarchy.jpg?auto=compress,format","categories":["Zalando","Apache Kafka","Event Driven","Zalando Dublin"],"description":"Building an aggregated view of data in the event-driven microservice architecture","publish_date":"2018-01-23 00:00:00","link":"https://engineering.zalando.com/posts/2018/01/simplicity-by-distributing-complexity.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"2ef44b0a68bb85b839c760e5b7628ef2","publish_timestamp":1516233600,"title":"Drawn Together","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/0b9c269492567dcc276106aa42f7078d8134530b_screen-shot-2018-01-12-at-13.29.30.png?auto=compress,format","categories":["Zalando","Culture","Design"],"description":"How to talk about design in the agile world","publish_date":"2018-01-18 00:00:00","link":"https://engineering.zalando.com/posts/2018/01/how-to-talk-about-design.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"389ae1387520b8ab3b86f7ffe857d8da","publish_timestamp":1515628800,"title":"The Faces Behind the Fashion-MNIST","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/01936e633c87d48b1a4f4fca368003764a778a59_2017-12-07-han-xiao--kashif-rasul-0147.jpg?auto=compress,format","categories":["Zalando","Computer Vision","Deep Learning","Machine Learning","Open Source"],"description":"The Faces Behind the Fashion-MNIST","publish_date":"2018-01-11 00:00:00","link":"https://engineering.zalando.com/posts/2018/01/faces-behind-fashion-mnist.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"efc6b23639ca3c2fc9e3b3d0302f7255","publish_timestamp":1515456000,"title":"Why We Do Scala in Zalando","blogName":"Zalando","image":"","categories":["Zalando","Zalando Dublin"],"description":"Leveraging the full power of a functional programming language","publish_date":"2018-01-09 00:00:00","link":"https://engineering.zalando.com/posts/2018/01/why-we-do-scala.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"92d2793798b7367dde27b12e1dd32c3d","publish_timestamp":1515024000,"title":"Rock Solid Kafka and ZooKeeper Ops on AWS","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/33128199ba2862e7b2da622b4eecf5e7327e0f96_screen-shot-2018-01-02-at-14.21.18.png?auto=compress,format","categories":["Zalando","Apache Kafka"],"description":"Reducing ops effort while maintaining Kafka and Zookeeper","publish_date":"2018-01-04 00:00:00","link":"https://engineering.zalando.com/posts/2018/01/rock-solid-kafka.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1abf6f766ee4204a78e1d8ab7c5c1dd6","publish_timestamp":1513814400,"title":"AngularConnect 2017","blogName":"Zalando","image":"","categories":["Zalando","Angular"],"description":"Highlights and Takeaways from Europe’s Largest Angular Conference","publish_date":"2017-12-21 00:00:00","link":"https://engineering.zalando.com/posts/2017/12/angularconnect-2017.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"471e429965081bb4c89624896c566e88","publish_timestamp":1513641600,"title":"Surviving Data Loss","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/7096f62ecc37e1773be9b8b1c424e83241505343_screen-shot-2017-12-14-at-13.47.06.png?auto=compress,format","categories":["Zalando","Apache Kafka","Zookeeper"],"description":"Backing up Apache Kafka and Zookeeper to S3","publish_date":"2017-12-19 00:00:00","link":"https://engineering.zalando.com/posts/2017/12/backing-up-kafka-zookeeper.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"afdbb33196188ea0a37b76cd22a1521c","publish_timestamp":1513209600,"title":"Constant Gardening","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/1008fca091fe9310bae176120e67f0c5073796e8_constant-gardening-oct-17-airshow.jpg?auto=compress,format","categories":["Zalando","Leadership","Management","Product Management"],"description":"Effective management is not an end goal, but a process.","publish_date":"2017-12-14 00:00:00","link":"https://engineering.zalando.com/posts/2017/12/constant-gardening.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b8ab1ab0fd4060b470544785fcaded60","publish_timestamp":1512604800,"title":"Introducing: Helsinki’s 100th Employee","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/e6db91340235e0d4f04c58bf6423c642cbf626db_maksim-helsinki.jpg?auto=compress,format","categories":["Zalando","Employer Branding","Zalando Helsinki"],"description":"In conversation with Full Stack Engineer, Maksim Ekimovskii","publish_date":"2017-12-07 00:00:00","link":"https://engineering.zalando.com/posts/2017/12/helsinki-100-employee.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"582067b0ae636788e2f054d11ad6ff2a","publish_timestamp":1512432000,"title":"A Recipe for Kafka Lag Monitoring","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/6b01f59fb7f262c93c5a902ea858a718bc93c1e6_screen-shot-2017-12-04-at-12.19.57.png?auto=compress,format","categories":["Zalando","Apache Kafka","Zalando Dublin"],"description":"A closer look at the ingredients needed for ultimate stability","publish_date":"2017-12-05 00:00:00","link":"https://engineering.zalando.com/posts/2017/12/recipe-for-kafka-lag-monitoring.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1f6693671285e38bc4048605e40de748","publish_timestamp":1512000000,"title":"Running Kafka Streams applications in AWS","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/63b7f3790416f6933fb1f0faf4058067ee033ec5_screen-shot-2017-11-28-at-10.12.44.png?auto=compress,format","categories":["Zalando","Apache Kafka","Zalando Dublin"],"description":"Second in our series about the use of Apache Kafka’s Streams API by Zalando","publish_date":"2017-11-30 00:00:00","link":"https://engineering.zalando.com/posts/2017/11/running-kafka-streams-applications-aws.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"672f03a8f1f80e2382f15be138ffc492","publish_timestamp":1511395200,"title":"Real-time Ranking with Apache Kafka’s Streams API","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/63fb0106e79a7db22606f43697f8ff96781b14c2_screen-shot-2017-11-23-at-09.04.33.png?auto=compress,format","categories":["Zalando","APIs","Apache Kafka"],"description":"Using Apache and the Kafka Streams API with Scala on AWS for real-time fashion insights","publish_date":"2017-11-23 00:00:00","link":"https://engineering.zalando.com/posts/2017/11/real-time-ranking-kafka.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0a33aeb412f0153fb745e9b84fdff07b","publish_timestamp":1511222400,"title":"Why Event Driven?","blogName":"Zalando","image":"","categories":["Zalando","Zalando Dublin"],"description":"Zalando is using an event-driven approach for its new Fashion Platform. Conor Clifford examines why","publish_date":"2017-11-21 00:00:00","link":"https://engineering.zalando.com/posts/2017/11/why-event-driven.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"69b3988d4c0bbf8af759c2940cd4ec21","publish_timestamp":1510790400,"title":"Do We Really Need UI Tests?","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/4fd507fb406a584dd96052be66da87f69965462b_screen-shot-2017-11-13-at-10.23.06.png?auto=compress,format","categories":["Zalando","Apps"],"description":"Two brothers examine the pros and cons of UI testing","publish_date":"2017-11-16 00:00:00","link":"https://engineering.zalando.com/posts/2017/11/do-we-really-need-ui-testing.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f98ec0bfe9d2a0b106e59fc6d8804796","publish_timestamp":1510185600,"title":"Dedicated Ownership for Teams at Zalon","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/58e1a5fdf7a0d5ee993d6d2a1d54dd31f5f826a3_screen-shot-2017-11-07-at-17.42.08.png?auto=compress,format","categories":["Zalando","Management"],"description":"Agile Lead and Software Engineer at Zalon, Jan Helwich on how to work well","publish_date":"2017-11-09 00:00:00","link":"https://engineering.zalando.com/posts/2017/11/dedicated-ownership-for-teams-at-zalon.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"2e6bbab5154e4d7407ffec1024e3ee68","publish_timestamp":1510012800,"title":"Zalando Wins Big in Dublin","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/8d4dca1cbcb7b64904bf0c3487251386293f5a64_screen-shot-2017-11-06-at-09.28.25.png?auto=compress,format","categories":["Zalando","DatScis","Data Science","Zalando Dublin"],"description":"Ana Peleteiro Ramallo takes ‘Data Scientist of the Year’ award at the DatSci’s","publish_date":"2017-11-07 00:00:00","link":"https://engineering.zalando.com/posts/2017/11/datsci-awards-2017.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"302ba08e637bfa415b1d853a7d133651","publish_timestamp":1509580800,"title":"Agile Fails","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/9109dde6625cfdf7a8de435d6df419ee5c6740c7_screen-shot-2017-10-23-at-13.45.43.png?auto=compress,format","categories":["Zalando","Culture","Management"],"description":"\"To be pressure-proof, a process needs to become second nature.\" We look at learning from and overcoming the issues that slow our teams down.","publish_date":"2017-11-02 00:00:00","link":"https://engineering.zalando.com/posts/2017/11/agile-fails.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e784404359ee48e8bafe48f1a80bff84","publish_timestamp":1508976000,"title":"Reattaching Kafka EBS in AWS","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/17445b2ad643da351e47e7cf06b75bb2bb77f7b3_ebs-1.png?auto=compress,format","categories":["Zalando","AWS","Apache Kafka","Microservices","Nakadi"],"description":"Stop worrying about losing your Apache Kafka broker without copying a large amount data.","publish_date":"2017-10-26 00:00:00","link":"https://engineering.zalando.com/posts/2017/10/reattaching-kafka-ebs-in-aws.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0eee3d8fc7fd4b2e10722a99dcc53922","publish_timestamp":1508803200,"title":"Zalando's Smart Product Platform","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/4f2c385b8efc58679bbc0423285e77f202af83cf_screen-shot-2017-10-23-at-08.39.57.png?auto=compress,format","categories":["Zalando","Fashion Insights Centre","Product Management","Zalando Dublin"],"description":"What's the craic? We'll tell you: it's our SPP and it happens where fashion meets tech in our Dublin hub.","publish_date":"2017-10-24 00:00:00","link":"https://engineering.zalando.com/posts/2017/10/zalando-smart-product-platform.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"76a3442efd979a048fdc580141809530","publish_timestamp":1508371200,"title":"All Systems Go","blogName":"Zalando","image":"","categories":["Zalando","Tech Events"],"description":"Zalando flies the fashion flag at Recsys 2017. Research engineer, Humberto Corona reports back.","publish_date":"2017-10-19 00:00:00","link":"https://engineering.zalando.com/posts/2017/10/recsys-2017.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"50011f07c4bcae697a843ba0e4fa1a9b","publish_timestamp":1508198400,"title":"A Plea For Small Pull Requests","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/a380a517ecac72b2eb841fccf1e34016a04b8a64_selection_001.png?auto=compress,format","categories":["Zalando","Git","GitHub","Open Source"],"description":"Size matters when we're talking about Pull Requests, especially when it comes to best practices in software development.","publish_date":"2017-10-17 00:00:00","link":"https://engineering.zalando.com/posts/2017/10/a-plea-for-small-pull-requests.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"c13e736632d9f68a071dd01306eb8615","publish_timestamp":1507593600,"title":"Event First Development - Moving Towards Kafka Pipeline Applications","blogName":"Zalando","image":"","categories":["Zalando","APIs","Apache Kafka","Data Science","Zalando Dublin"],"description":"Read how we went about building our primarily event driven system for better access to data for users of our Smart Product Platform.","publish_date":"2017-10-10 00:00:00","link":"https://engineering.zalando.com/posts/2017/10/event-first-development---moving-towards-kafka-pipeline-applications.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"346665c713a6b5115c77971490e53d83","publish_timestamp":1507075200,"title":"On the Road to Full Stack Responsibility","blogName":"Zalando","image":"","categories":["Zalando","Frontend"],"description":"Learnings from a team's journey to making their product foolproof, regardless of team switches or roles.","publish_date":"2017-10-04 00:00:00","link":"https://engineering.zalando.com/posts/2017/10/on-the-road-to-full-stack-responsibility.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"26b5fae0160c661a1345eebe7590f00f","publish_timestamp":1506470400,"title":"A State-of-the-Art Method for Generating Photo-Realistic Textures in Real Time","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/472c699d2d6f0b0e9e0348449c49ba9c9745b102_figure1zresearch1.png?auto=compress,format","categories":["Zalando","Artificial Intelligence","Computer Vision","Data Science","Deep Learning","Machine Learning","Neural Networks","Zalando Research"],"description":"Our Zalando Research team give an overview of the latest work in image generation using machine learning.","publish_date":"2017-09-27 00:00:00","link":"https://engineering.zalando.com/posts/2017/09/a-state-of-the-art-method-for-generating-photo-realistic-textures-in-real-time.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"6b03f3aa5851f6ee5b2ce5a34376c566","publish_timestamp":1506384000,"title":"Zalando Dublin Welcomes Their 100th Employee","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/684d8aa1c7bc5027c3063216d005f243106b3763_dublin_team_old_office.jpg?auto=compress,format","categories":["Zalando","Employer Branding","Zalando Dublin"],"description":"We've hit an important milestone at our Fashion Insights Centre in Dublin, Ireland!","publish_date":"2017-09-26 00:00:00","link":"https://engineering.zalando.com/posts/2017/09/zalando-dublin-welcomes-their-100th-employee.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1c4b60c50806b6f33d5c4bf765bfb34c","publish_timestamp":1505952000,"title":"Zalando Fulfillment Solutions and our FAST Replenishment Algorithm","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/f42f21399494643c6bf952245e230dbad6fdab57_screen-shot-2017-06-27-at-13.30.39.png?auto=compress,format","categories":["Zalando","Logistics"],"description":"Better availability of products is regarded as extremely important, which is where Zalando Fulfillment Solutions comes in.","publish_date":"2017-09-21 00:00:00","link":"https://engineering.zalando.com/posts/2017/09/zalando-fulfillment-solutions-and-our-fast-replenishment-algorithm.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"06c39cdd52eff6ed4f8a49f2590e7a73","publish_timestamp":1505088000,"title":"IT-Compliance in the 21st Century","blogName":"Zalando","image":"","categories":["Zalando","Platform Engineering","Security"],"description":"Read about how our world-class team manages the biggest challenge in Europe.","publish_date":"2017-09-11 00:00:00","link":"https://engineering.zalando.com/posts/2017/09/it-compliance-in-the-21st-century.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"db6929d8e3783aa3dd46d258fe94245f","publish_timestamp":1504051200,"title":"InnerSource Do’s and Don’ts out of Dortmund","blogName":"Zalando","image":"","categories":["Zalando","Code Quality","InnerSource","Open Source","Zalando Dortmund"],"description":"Communication is the key to success when it comes to InnerSource across teams, offices, and locations","publish_date":"2017-08-30 00:00:00","link":"https://engineering.zalando.com/posts/2017/08/innersource-dos-and-donts-out-of-dortmund.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"7a404b95bf7a61d2ca0e4e7a777eb143","publish_timestamp":1503532800,"title":"Spaghetti and Marshmallows at Zalando: An Exercise to Inspire Deep Learning","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/44bc40d4b18681e30a500b081ce7ef473095495c_imag0756.jpg?auto=compress,format","categories":["Zalando","Deep Learning","Producer","Zalando Dortmund"],"description":"Encourage teams to experience simple yet profound lessons in collaboration, innovation, and creativity.","publish_date":"2017-08-24 00:00:00","link":"https://engineering.zalando.com/posts/2017/08/spaghetti-and-marshmallows-at-zalando.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f4196b1859254a9323fd0500c4a26f5d","publish_timestamp":1502841600,"title":"Data For All: An Introduction to Product Analytics at Zalando","blogName":"Zalando","image":"","categories":["Zalando","Data Analytics","Product","Product Management"],"description":"Embedding a true data and experimentation culture at Zalando.","publish_date":"2017-08-16 00:00:00","link":"https://engineering.zalando.com/posts/2017/08/data-for-all-an-introduction-to-product-analytics-at-zalando.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d1b5138636db7b086ff3eebc215b5993","publish_timestamp":1502668800,"title":"How Tech Candidate Feedback Helped Improve our Candidate Net Promoter Score","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/9cadea84af0b0ea6fe68abdf6caec70b6bb74121_improvement-suggestions.jpg?auto=compress,format","categories":["Zalando","Recruiting"],"description":"Read about the combination of actions that led to an incredible improvement of our cNPS.","publish_date":"2017-08-14 00:00:00","link":"https://engineering.zalando.com/posts/2017/08/how-tech-candidate-feedback-helped-improve-our-candidate-net-promoter-score.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"fb652d6033a542d6f298239f7c6345fd","publish_timestamp":1501632000,"title":"Community Group Hug: Techspert Loves Open Source","blogName":"Zalando","image":"","categories":["Zalando","Open Source","Zalando Techspert Series"],"description":"Touching on issues of diversity, non-code contributions, and bigger market players in open source.","publish_date":"2017-08-02 00:00:00","link":"https://engineering.zalando.com/posts/2017/08/techspert-loves-open-source.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"18b8af429c522000313f185635bebef4","publish_timestamp":1501027200,"title":"The Purpose of JWT: Stateless Authentication","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/9b61b2b68ab55fe9baf00eb0f92a9536ad431ae8_jsonwebtoken.png?auto=compress,format","categories":["Zalando"],"description":"The true purpose of JSON Web Tokens and a comparison of two approaches in authentication.","publish_date":"2017-07-26 00:00:00","link":"https://engineering.zalando.com/posts/2017/07/the-purpose-of-jwt-stateless-authentication.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"9ec528b137591db2b3ba26a72ac413e6","publish_timestamp":1500336000,"title":"Closing the Data-Quality Loop","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/179ac8bb932707077ae3f28e8e76d54c5ac1f545_initialrunpanel.png?auto=compress,format","categories":["Zalando","Data Science","Microservices","Zalando Dublin"],"description":"Producing high quality validation corpora without the traditional time and cost inefficiencies.","publish_date":"2017-07-18 00:00:00","link":"https://engineering.zalando.com/posts/2017/07/closing-the-data-quality-loop.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0e2e2bcb4919e4bf2f11544b71e63412","publish_timestamp":1499904000,"title":"Complex Event Generation for Business Process Monitoring using Apache Flink","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/1505b80e0ee2e6eb767b8c2ab99cab1ac9f14259_order_created__all_parcels_shipped.png?auto=compress,format","categories":["Zalando","Apache Flink","Apache Spark","Big Data","Monitoring"],"description":"We look at the design, implementation, and generation of complex events.","publish_date":"2017-07-13 00:00:00","link":"https://engineering.zalando.com/posts/2017/07/complex-event-generation-for-business-process-monitoring-using-apache-flink.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"348bb240bce406ab8da3637855fd51d8","publish_timestamp":1498694400,"title":"Autonomous Motivation in Technology Organizations","blogName":"Zalando","image":"","categories":["Zalando","Management","Microservices Day"],"description":"We've been living autonomy for 2 years – have we been able to make it work at Zalando?","publish_date":"2017-06-29 00:00:00","link":"https://engineering.zalando.com/posts/2017/06/autonomous-motivation-in-technology-organizations.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f9aa3445c2cdbf35fd25fe99f213f51f","publish_timestamp":1498521600,"title":"The Modern Architecture of Search","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/db5e75c13ebd99d4719a793e9a28778c1560c953_architecture.png?auto=compress,format","categories":["Zalando","Data Science","Fashion Search","Machine Learning","Search"],"description":"Discussing the components needed to solve the problems of IR in web platforms.","publish_date":"2017-06-27 00:00:00","link":"https://engineering.zalando.com/posts/2017/06/the-modern-architecture-of-search.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"bb4445cfaac304d15485c51a8b5aedf7","publish_timestamp":1498003200,"title":"PostgreSQL in a time of Kubernetes","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/5bb124543023a757c41dc7d56822892fa464b22d_postgreskubernetes1.png?auto=compress,format","categories":["Zalando","AWS","Kubernetes","Open Source","Patroni","PostgreSQL","STUPS","Postgres Operator"],"description":"Read about how we created a useful set of projects for operating PostgreSQL on Kubernetes.","publish_date":"2017-06-21 00:00:00","link":"https://engineering.zalando.com/posts/2017/06/postgresql-in-a-time-of-kubernetes.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"c83e8054e892a2ad5a37402d64d17c4b","publish_timestamp":1497484800,"title":"Quantitative UX Research – How Can it Complement our Customer Insights?","blogName":"Zalando","image":"","categories":["Zalando","Customer Centricity","UX","User Experience","User Research"],"description":"Our UX research team has started to uncover the possibilities that quantitative UX research offers.","publish_date":"2017-06-15 00:00:00","link":"https://engineering.zalando.com/posts/2017/06/quantitative-ux-research--how-can-it-complement-our-customer-insights.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"4b27d09e4e19ca80baaa7628d058d5f7","publish_timestamp":1496880000,"title":"Signalling Your Jenkins Build Status with a Mini USB Traffic Light","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/00bbc4a66572e91b970fb721e6f41a38c2d5f06c_img_20160630_155142.jpg?auto=compress,format","categories":["Zalando","Code Quality","Continuous Delivery"],"description":"Raise awareness of your failing builds, like we are, with this handy tutorial.","publish_date":"2017-06-08 00:00:00","link":"https://engineering.zalando.com/posts/2017/06/signalling-your-jenkins-build-status-with-a-mini-usb-traffic-light.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"9a3ff46eca0e6107a5352b07e4b4ec7f","publish_timestamp":1496102400,"title":"Behind Project Deadlines: Estimations for a Shared Understanding","blogName":"Zalando","image":"","categories":["Zalando","Estimates","Producer","Testing"],"description":"Managing stakeholder expectations for the good of your team and project.","publish_date":"2017-05-30 00:00:00","link":"https://engineering.zalando.com/posts/2017/05/behind-project-deadlines-estimations-for-a-shared-understanding.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"dc6ca4f12cdc5237175294a1a2fdac66","publish_timestamp":1495756800,"title":"Platform Engineering and Third Generation Microservices in Dublin","blogName":"Zalando","image":"","categories":["Zalando","Data Science","Machine Learning","Microservices","Platform Engineering","Platform Strategy","Zalando Dublin"],"description":"Closing the gap between systems of record and intelligence is a team vision.","publish_date":"2017-05-26 00:00:00","link":"https://engineering.zalando.com/posts/2017/05/platform-engineering-and-third-generation-microservices-in-dublin.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a14c2270eafe648a1b1b782ea76d9557","publish_timestamp":1494979200,"title":"Hack Around The Clock – Hack Night @ Zalando Hamburg","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/e96f7d5ddf6724bbf60f3ff21a7d35f784aea263_img_1775.jpg?auto=compress,format","categories":["Zalando","Hack Week","Innovation","Zalando Hamburg"],"description":"Hacking has a long tradition and it’s not merely limited to writing lines of code.","publish_date":"2017-05-17 00:00:00","link":"https://engineering.zalando.com/posts/2017/05/hack-around-the-clock--hack-night--zalando-hamburg.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1c64a8ebcb20f689d5d8c4b89b4baecd","publish_timestamp":1494892800,"title":"Personalization For The Good Of All","blogName":"Zalando","image":"","categories":["Zalando","Zalando Helsinki"],"description":"Building an advanced personalization system by creating a fully curated experience.","publish_date":"2017-05-16 00:00:00","link":"https://engineering.zalando.com/posts/2017/05/personalization-for-the-good-of-all.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3bc06419e115933a50c10402f86ec35c","publish_timestamp":1494374400,"title":"Detecting List Items Observed by User","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/5fd3ddd1db05f0d878171fd664daf568c00a63ff_medium-image-tracking.png?auto=compress,format","categories":["Zalando","Android","Java","Mobile","Mobile First","RxJava","UI"],"description":"Improve your knowledge of RxJava and RecyclerView APIs with this tutorial.","publish_date":"2017-05-10 00:00:00","link":"https://engineering.zalando.com/posts/2017/05/detecting-list-items-observed-by-user.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"76d563a4e36a2f3f9b5d0cafe58e66fa","publish_timestamp":1494201600,"title":"How to Dress Code – The Creation of Fashion for Tech","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/227ffb34f7e39949baf95746afebf3d927c71704_wdc-environment.003.jpeg?auto=compress,format","categories":["Zalando","Startups","Tech Culture","Tech Entrepreneurs","Tech Startups"],"description":"We dress code. The brand for tech enthusiasts, programmers, and startup whizkids.","publish_date":"2017-05-08 00:00:00","link":"https://engineering.zalando.com/posts/2017/05/how-to-dress-code.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"70ca2a6073de9c753f082a65cffb4c62","publish_timestamp":1493856000,"title":"Selenium Conf Gets a Dose of Zalenium","blogName":"Zalando","image":"","categories":["Zalando","Docker","DockerSelenium","Sauce Labs","Selenium","Testing","Zalenium"],"description":"Our Zalando developers recently presented Zalenium to the world in Austin, Texas.","publish_date":"2017-05-04 00:00:00","link":"https://engineering.zalando.com/posts/2017/05/selenium-conf-gets-a-dose-of-zalenium.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a11b7cb636f8de82f7476a6ba8f6d057","publish_timestamp":1493164800,"title":"Taking a Walk in a Producer's Shoes","blogName":"Zalando","image":"","categories":["Zalando","Producer","Zalando Dortmund"],"description":"Every Producer has their own style at Zalando, but the agile mindset is what unites them.","publish_date":"2017-04-26 00:00:00","link":"https://engineering.zalando.com/posts/2017/04/taking-a-walk-in-a-producers-shoes.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d8ada90766c0c32e12089f36054a316d","publish_timestamp":1492560000,"title":"Achieving 3.2x Faster Scala Compile Time","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/401c64aecdd367500255a99c585a6fec5f1fd4f0_initial-main-test-chart1.png?auto=compress,format","categories":["Zalando","Grafter","Scala","Shapeless"],"description":"Reducing compilation time of Scala programs can be challenging, but we got some great help.","publish_date":"2017-04-19 00:00:00","link":"https://engineering.zalando.com/posts/2017/04/achieving-3.2x-faster-scala-compile-time.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a4e0b7e7fa62f1502dd029b6735e72c2","publish_timestamp":1492473600,"title":"Adapting to the Mobile Consumer","blogName":"Zalando","image":"","categories":["Zalando","Android","Apps","Mobile","Mobile First","Tech Culture","iOS"],"description":"Our Mobile Apps Production Manager shares how Zalando refashioned its approach to mobile.","publish_date":"2017-04-18 00:00:00","link":"https://engineering.zalando.com/posts/2017/04/adapting-to-the-mobile-consumer.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"30ee61e68c588d9290abd4500df6d44e","publish_timestamp":1492041600,"title":"Parallel Computing with Scala","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/2d0650500b1748fc351a5e8db9742e25a16da79d_fotorcreated2.jpg?auto=compress,format","categories":["Zalando","Scala"],"description":"What can we do to solve problems that don’t fit on a single core CPU?","publish_date":"2017-04-13 00:00:00","link":"https://engineering.zalando.com/posts/2017/04/parallel-computing-with-scala.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"09056e7d7db13889556423207305857d","publish_timestamp":1491955200,"title":"Improving Swift Compilation Times from 12 to 2 Minutes","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/db5d3110d4b42f3dec754f4df7af561a89e298f6_swifttime1.png?auto=compress,format","categories":["Zalando","Swift","Xcode","Zalando Helsinki","iOS"],"description":"Module optimization and cutting compilation times for a better app experience overall.","publish_date":"2017-04-12 00:00:00","link":"https://engineering.zalando.com/posts/2017/04/improving-swift-compilation-times-from-12-to-2-minutes.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"4a589342e9a2ca786113bceebb99dd03","publish_timestamp":1491782400,"title":"Tech Destination: Berlin","blogName":"Zalando","image":"","categories":["Zalando","Startups","Tech Culture","Tech Entrepreneurs","Tech Startups"],"description":"Attracting great talent is key to growing Berlin's tech ecosystem, and we're up to the challenge.","publish_date":"2017-04-10 00:00:00","link":"https://engineering.zalando.com/posts/2017/04/tech-destination-berlin.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0a2ce766a51265526b748599e2e80f6b","publish_timestamp":1491436800,"title":"Nine Tips for Planning User Research in Foreign Countries","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/f4dc0b30d5858f09994819097569db5f0c6a845d_planninguserresearch.png?auto=compress,format","categories":["Zalando","Research","UX","User Experience","User Research"],"description":"Check out our advice for planning field research in different countries for Zalando overall.","publish_date":"2017-04-06 00:00:00","link":"https://engineering.zalando.com/posts/2017/04/nine-tips-for-planning-user-research-in-foreign-countries.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"c7ac5333b6cfe871c0f0ba5a521c70b5","publish_timestamp":1491264000,"title":"Crafting a Digital Fashion Vocabulary","blogName":"Zalando","image":"","categories":["Zalando","Data Science","Fashion Insights Centre","Research","Zalando Dublin"],"description":"Fashion and tech make a powerful partnership, you just need to know the lingo.","publish_date":"2017-04-04 00:00:00","link":"https://engineering.zalando.com/posts/2017/04/crafting-a-digital-fashion-vocabulary.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0d60c4de30ca7fd044f2b9e3eb19f787","publish_timestamp":1490832000,"title":"An Open Source Pulse Check at Zalando for 2017","blogName":"Zalando","image":"","categories":["Zalando","Connexion","GitHub","Grafter","Patroni","Skipper"],"description":"Zalando depends on open source technologies to exist, so where are we at with some of our projects?","publish_date":"2017-03-30 00:00:00","link":"https://engineering.zalando.com/posts/2017/03/an-open-source-pulse-check-at-zalando-for-2017.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5c9a15daa409a3f3a4fee8398924c3e2","publish_timestamp":1490745600,"title":"HMM PySpark Implementation: A Zalando Hack Week Project","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/c6a76cde467cb53361a5980d504a35691c761ab4_chart.png?auto=compress,format","categories":["Zalando","Big Data","Data Science","Hack Week","Python"],"description":"Training models using larger amounts of data for a great Zalando Hack Week project.","publish_date":"2017-03-29 00:00:00","link":"https://engineering.zalando.com/posts/2017/03/hmm-pyspark-implementation-a-zalando-hack-week-project.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5cbb244aa92009613710b223c31b5f3a","publish_timestamp":1490140800,"title":"Deep Learning in Production for Predicting Consumer Behavior","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/2e48238be0de136d12875c44bf32d5d87ba48c8a_rnn.png?auto=compress,format","categories":["Zalando","AdTech","Deep Learning","Machine Learning","Recurrent Neural Networks","Zalando Hamburg"],"description":"We are excited to see how user experiences in e-commerce will evolve to personalized encounters.","publish_date":"2017-03-22 00:00:00","link":"https://engineering.zalando.com/posts/2017/03/deep-learning-in-production-for-predicting-consumer-behavior.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"add2038dc2b9ed588fb3278e0cce2a6a","publish_timestamp":1490054400,"title":"Practical Challenges For RxJava Learners","blogName":"Zalando","image":"","categories":["Zalando","Android","Java","Mobile","RxJava","TDD"],"description":"If you're an Android developer, you can learn RxJava by coding with this useful guide.","publish_date":"2017-03-21 00:00:00","link":"https://engineering.zalando.com/posts/2017/03/practical-challenges-for-rxjava-learners.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"68343006bc2170052aee919201357318","publish_timestamp":1489622400,"title":"Linting and ESLint: Write Better Code","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/beba80be1e67db99ef8397122c644a46e23b9f06_1-wi-3pi1main3yczn7rz_xq.png?auto=compress,format","categories":["Zalando","Code Review","JavaScript"],"description":"Why should you spend the time required to lint your code? Up your code quality game right here.","publish_date":"2017-03-16 00:00:00","link":"https://engineering.zalando.com/posts/2017/03/linting-and-eslint-write-better-code.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"6b7942d6387e5bb481bcda66157f9e81","publish_timestamp":1489449600,"title":"One-click Deployments for iOS Apps using Xcode 8 and More","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/f6479641c836922a7d1f867bed679a3d112f424e_xcodeservercreateuser.png?auto=compress,format","categories":["Zalando","Mobile","Xcode","iOS"],"description":"Exploring tools to provide a seamless Continuous Delivery experience for your iOS team.","publish_date":"2017-03-14 00:00:00","link":"https://engineering.zalando.com/posts/2017/03/one-click-deployments-for-ios-apps.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5f33aeb191be1c005d9886e4fe27d681","publish_timestamp":1487721600,"title":"How the Zalando iOS App Abandoned CocoaPods and Reduced Build Time","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/d7aee681eb425c9fdf4d2c86166e7d1f938cab2e_cocoapods1.png?auto=compress,format","categories":["Zalando","Mobile","Swift","iOS"],"description":"Read the takeaways from our adoption of manual dependency management.","publish_date":"2017-02-22 00:00:00","link":"https://engineering.zalando.com/posts/2017/02/how-the-zalando-ios-app-abandoned-cocoapods-and-reduced-build-time.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"02acd432e730e732b10e2117cf2bb433","publish_timestamp":1487635200,"title":"Dress Code: An In-house Style Guide for Zalando’s Solution Center","blogName":"Zalando","image":"","categories":["Zalando","Design","Design Patterns","Design Thinking","UX"],"description":"How do we work when we Dress Code? Discover our pattern library and style guide.","publish_date":"2017-02-21 00:00:00","link":"https://engineering.zalando.com/posts/2017/02/dress-code-an-in-house-style-guide-for-zalandos-solution-center.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e24a82988f51ba1392a949e010d3324a","publish_timestamp":1487116800,"title":"Riding the Scalawave in 2016","blogName":"Zalando","image":"","categories":["Zalando","Akka","Scala","Shapeless"],"description":"Scala and Akka are mind-bending in this conference overview from Gdańsk, Poland.","publish_date":"2017-02-15 00:00:00","link":"https://engineering.zalando.com/posts/2017/02/riding-the-scalawave-in-2016.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"18cb8965fdd5eb2fb58af0dbd3cdf6f4","publish_timestamp":1487030400,"title":"Zalenium: A Disposable and Flexible Selenium Grid Infrastructure","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite%2Fed47df08-3c62-4cb4-9b27-a523128b3212_how_it_works.gif?auto=compress,format","categories":["Zalando","Docker","DockerSelenium","Sauce Labs","Selenium","Testing","Zalenium"],"description":"Let us help you scale your local grid dynamically with Docker containers via open source software.","publish_date":"2017-02-14 00:00:00","link":"https://engineering.zalando.com/posts/2017/02/zalenium-a-disposable-and-flexible-selenium-grid-infrastructure.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"6f426642f2b3413c5875ea63fb1c3c41","publish_timestamp":1486080000,"title":"Building a Relay-compatible GraphQL Server","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/41f86097610056809243b4eddaca6903d9e223e0_graphql1.png?auto=compress,format","categories":["Zalando","ES6","GraphQL","OpenAPI","React","Redux"],"description":"Experimenting with our technology stack via GraphQL and Relay – ES6 and React knowledge required!","publish_date":"2017-02-03 00:00:00","link":"https://engineering.zalando.com/posts/2017/02/building-a-relay-compatible-graphql-server.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"25390b67ef5863af91f17b6c8d519c18","publish_timestamp":1485993600,"title":"Using Microservices to Power Fashion Search and Discovery","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/5152fcc5755e43a6432bb736197b5ec1b2eebbda_usingmicroservicestopowerfashionsearchimage1.png?auto=compress,format","categories":["Zalando","Fashion Search","Microservices","Zalando Helsinki"],"description":"Focusing on our customer's search solution that targets a consumer facing application.","publish_date":"2017-02-02 00:00:00","link":"https://engineering.zalando.com/posts/2017/02/using-microservices-to-power-fashion-search-and-discovery.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"c50cc25c5439fa032e37dc1dc330b8e9","publish_timestamp":1485475200,"title":"Your Lifelike Hologram using Structure and HoloLens for Hack Week","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/e8192eed8674ad294f6189ff0e76e23046565412_vrify_scanning.jpeg?auto=compress,format","categories":["Zalando","Augmented Reality","Hack Week"],"description":"More Hack Week fun with VRify, an immersive conference call experience for remote colleagues.","publish_date":"2017-01-27 00:00:00","link":"https://engineering.zalando.com/posts/2017/01/your-lifelike-hologram-using-structure-and-hololens-for-hack-week.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a46dc21e7a1301a5e7026a92f5cf7884","publish_timestamp":1485302400,"title":"The Role of UX in Hack Week","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/97c7be97742a43a83f819e7ebecaa366c2997f50_31431720640_cde6b2b296_o.jpg?auto=compress,format","categories":["Zalando","Design","Design Thinking","Hack Week","UI","UX","User Experience","User Interaction"],"description":"When UX-savvy people are involved in Hack Week, design-related problems get solved.","publish_date":"2017-01-25 00:00:00","link":"https://engineering.zalando.com/posts/2017/01/the-role-of-ux-in-hack-week.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d6e55eec4eb0c938c555691bd210911d","publish_timestamp":1484784000,"title":"About Akka Streams","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/54e9486a657ec3f67015c6b89007598262d4018a_compose_composites1.png?auto=compress,format","categories":["Zalando","Akka","Erlang","Reactive Streams","Scala","Zalando Helsinki"],"description":"Pipeline processing on a high level with Akka Streams and our Helsinki Engineering Team.","publish_date":"2017-01-19 00:00:00","link":"https://engineering.zalando.com/posts/2017/01/about-akka-streams.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"c3c989eff81edaa69c76be150891c770","publish_timestamp":1484697600,"title":"Rule Over Your Angular2 State Machine","blogName":"Zalando","image":"","categories":["Zalando","Angular","Frontend","JavaScript","TypeScript"],"description":"Handle your Angular 2 application state better and make your frontend developer life that much easier.","publish_date":"2017-01-18 00:00:00","link":"https://engineering.zalando.com/posts/2017/01/rule-over-your-angular2-state-machine.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b3a1e42980eec016975d388496c3e7f6","publish_timestamp":1484179200,"title":"In Search of the Perfect Fit – Insights from the UX Job Title Survey","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/3b8fa53eb008a10e786ec1e57ae9305f1db0a8a3_fig1uxjobsurvey.png?auto=compress,format","categories":["Zalando","Design","Interaction Design","Research","Tech Jobs","UI","UX","User Experience","User Interaction","User Research"],"description":"The results are in! One in three UX professionals aren’t happy with their job title.","publish_date":"2017-01-12 00:00:00","link":"https://engineering.zalando.com/posts/2017/01/in-search-of-the-perfect-fit--insights-from-the-ux-job-title-survey.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"80483c94683605f40ce9d81252db2099","publish_timestamp":1484092800,"title":"What is Hardcore Data Science – In Practice?","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/923f13007bfcb801f4ba6bd6e5d3fbba0daea21c_image_1-24655720fbf19c663573aa6bbd0b2a58.jpg?auto=compress,format","categories":["Zalando","Data Platforms","Data Science","Research"],"description":"Originally a research topic, data science has proven to add real business value for Zalando.","publish_date":"2017-01-11 00:00:00","link":"https://engineering.zalando.com/posts/2017/01/what-is-hardcore-data-science--in-practice.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"ee4746900dc5296f5c1679e4cdac0f10","publish_timestamp":1483660800,"title":"App Migration to Swift 3","blogName":"Zalando","image":"","categories":["Zalando","Mobile","Swift","Zalando Helsinki"],"description":"We’re happy to share our experiences with teams needing to migrate their own apps.","publish_date":"2017-01-06 00:00:00","link":"https://engineering.zalando.com/posts/2017/01/app-migration-to-swift-3.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"c1b24d8481f22c67f30ee1747027dd70","publish_timestamp":1483488000,"title":"Sapphire Deep Learning Upskilling","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/4465118368f8c6d8a165a1a9591bbcba0823a52c_upskilling1.png?auto=compress,format","categories":["Zalando","Data Science","Deep Learning","Natural Language Processing","TensorFlow","Tour of Mastery","Zalando Dublin"],"description":"Read about how our Dublin Tech Hub balances delivery with data science upskilling.","publish_date":"2017-01-04 00:00:00","link":"https://engineering.zalando.com/posts/2017/01/sapphire-deep-learning-upskilling.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"510ae6063b382a97bff7ee26dc88f36f","publish_timestamp":1482969600,"title":"Our Android App wins Editor’s Choice in the Google Play Store","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/619ec9419d2b62b308107590c10c1f54eb41cd9b_screenshotandroid1.png?auto=compress,format","categories":["Zalando","Android","Apps"],"description":"We’re incredibly proud to share this tremendous feat on behalf of the fashion e-commerce industry.","publish_date":"2016-12-29 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/our-android-app-wins-editors-choice-in-the-google-play-store.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"40342e0c80140cdb04684ece6d36112c","publish_timestamp":1482883200,"title":"Top 5 Career Tips of 2016: UX and Beyond","blogName":"Zalando","image":"","categories":["Zalando","Tech Jobs","UX"],"description":"We’ve been drawing from our hiring experiences this past year to deliver useful career advice.","publish_date":"2016-12-28 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/top-5-career-tips-of-2016-ux-and-beyond.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e43f7337e23b8c421d5712a76a157097","publish_timestamp":1482796800,"title":"Zalando and the Docker Global Mentor Week","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/955de04680b6dc63908d3c35983ce73ecee4d7f6_mentorweek.jpg?auto=compress,format","categories":["Zalando","Docker","Linux","Zalando Dortmund"],"description":"Docker had its first Global Mentor Week this year and Zalando Dortmund joined the fun.","publish_date":"2016-12-27 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/zalando-and-the-docker-global-mentor-week.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a73b9cb08b40e6bc61fe2bfc364060e9","publish_timestamp":1482451200,"title":"The Finish Line – Hack Week #5 Awards and More!","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/2ff8fd2db2d04dc3a839c12213b84ece69a3f325_16_053_zalandomaxthrelfallphoto-_xth5342.jpg?auto=compress,format","categories":["Zalando","Hack Week","Innovation","Zalando Dortmund","Zalando Dublin","Zalando Helsinki"],"description":"It's a wrap on this year's Hack Week – see which projects were praised and rewarded!","publish_date":"2016-12-23 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/the-finish-line--hack-week-5-awards-and-more.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e550b0a50565f31b2a86928889e00644","publish_timestamp":1482364800,"title":"Hack Week #5 – The ajudando Project","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/5d0a17526f7c4c0558a6eb8e0b5f7a1ecff97a4b_16_053_zalandomaxthrelfallphoto-_xth5644.jpg?auto=compress,format","categories":["Zalando","Experimentation","Hack Week","Innovation"],"description":"Connecting one another with the right expertise via knowledge exchange for Hack Week.","publish_date":"2016-12-22 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/hack-week-5--the-ajudando-project.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"81c59c0652893d0198541bad32e40735","publish_timestamp":1482278400,"title":"Introducing Kids to Tech for Hack Week","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/215be7819c0fb92151d3e9f1b91a687b50136a1b_kidsintech1.jpg?auto=compress,format","categories":["Zalando","Diversity in Tech","Hack Week","Innovation","Tech Entrepreneurs"],"description":"Exploring ways we can give back to the community via technology for future generations.","publish_date":"2016-12-21 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/introducing-kids-to-tech-for-hack-week.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"7995769aab3148f8ae4aaf25c2895dde","publish_timestamp":1482105600,"title":"Hack Week #5 is Live!","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/808b40f8ef15f595b760c2657bc483a9f92ad464_dortmundhackweek.jpg?auto=compress,format","categories":["Zalando","Hack Week","Innovation","Zalando Dortmund","Zalando Dublin","Zalando Helsinki"],"description":"Our annual Zalando Hack Week is here, coming to you bigger and better than ever!","publish_date":"2016-12-19 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/hack-week-5-is-live.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"38ca9e63357604e9a6823686eca8e555","publish_timestamp":1481846400,"title":"Zalando meets Technology Foundation Berlin at Techsperts","blogName":"Zalando","image":"","categories":["Zalando","Tech Culture","Zalando Techspert Series"],"description":"Some healthy discussion about expectations of the future workplace and agile.","publish_date":"2016-12-16 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/zalando-meets-technology-foundation-berlin-at-techsperts.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"06b7bb280e724faec92fda7ae13aee00","publish_timestamp":1481673600,"title":"Talking to Elasticsearch","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/a1c77f2fb37caf671756389757e36787a4eb63c2_talkingtoelasticsearch1.png?auto=compress,format","categories":["Zalando","Data Analytics","Distributed Data","Elasticsearch","Search"],"description":"Good communication between applications and clusters is an essential requirement for scaling.","publish_date":"2016-12-14 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/get-your-application-or-cluster-talking-to-elasticsearch.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"4e88fba9bd4dbb03f2c2aeb643445ab8","publish_timestamp":1481587200,"title":"Zalando lands at EuroClojure 2016","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/de4027db640ee249deb0564f9eb9989864434754_ec2.jpg?auto=compress,format","categories":["Zalando","Clojure","Scala"],"description":"Clojure's growing popularity at Zalando saw us attend the largest Clojure conference in Europe.","publish_date":"2016-12-13 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/zalando-lands-at-euroclojure-2016.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"dababae13093c7807e1f3500ecefbba2","publish_timestamp":1481241600,"title":"Zalando Continues Being Part of the React Ecosystem at ReactNL 2016","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/b38cdcaf1541353b01720ef32432a08d79b8932b_1-_2unf9rqvrydxpthrztycg.jpeg?auto=compress,format","categories":["Zalando","Elm","JavaScript","React"],"description":"Developer Tony Saad shares his insights and learnings from the inaugural ReactNL Conference.","publish_date":"2016-12-09 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/zalando-continues-being-part-of-the-react-ecosystem-at-reactnl-2016.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"ca5333026fdf44da00ddfd6d64353f59","publish_timestamp":1481155200,"title":"An Introduction to the Producer Role at Zalando","blogName":"Zalando","image":"","categories":["Zalando","OKRs","Producer","Product Management"],"description":"Imagine a Producer to be a mix of Scrum Master, Project Manager, and Agile Coach.","publish_date":"2016-12-08 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/an-introduction-to-the-producer-role-at-zalando.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"43cd6dc9ce0491e5887aade32ff44518","publish_timestamp":1481068800,"title":"Hack Like a Girl with Zalando Tech","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/e65530897ad83e6c8de7dbde97886adad0a07c78_hacklikeagirl1.jpg?auto=compress,format","categories":["Zalando","APIs","Diversity in Tech","Hackathon","React"],"description":"Get involved in hacking with Geek Girls Carrots Berlin and the Zalando Tech Shop API.","publish_date":"2016-12-07 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/hack-like-a-girl-with-zalando-tech.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"54d8f8c88995363f34a311ee4bc2f4ab","publish_timestamp":1480636800,"title":"Recommendations Galore: How Zalando Tech Makes It Happen","blogName":"Zalando","image":"","categories":["Zalando","Data Science","Recommender Systems"],"description":"Diving deeper into the tools that allow us to power our recommendation engines.","publish_date":"2016-12-02 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/recommendations-galore-how-zalando-tech-makes-it-happen.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1bc147213b0a213ee1d431e64bac34dc","publish_timestamp":1480550400,"title":"Crafting Effective Microservices in Python","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/7160a92c2a115cde37eddc0af6b8953663a9c8f9_api_console.png?auto=compress,format","categories":["Zalando","APIs","Connexion","Microservices","Open Source","OpenAPI","Python"],"description":"The API-first approach combined with Connexion are powerful tools to create effective microservices.","publish_date":"2016-12-01 00:00:00","link":"https://engineering.zalando.com/posts/2016/12/crafting-effective-microservices-in-python.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d71a82cff9e8092623ba458f0a6b2030","publish_timestamp":1480377600,"title":"Getting Down to Business with our Techsperts","blogName":"Zalando","image":"","categories":["Zalando","Tech Entrepreneurs","Zalando Techspert Series"],"description":"Missed our last Techspert Panel? Check out the exclusive interview with this month's guests.","publish_date":"2016-11-29 00:00:00","link":"https://engineering.zalando.com/posts/2016/11/getting-down-to-business-with-our-techsperts.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"8294615b416e9f5180b5cca1b4dc4a3c","publish_timestamp":1479945600,"title":"A Closer Look at Elasticsearch Express","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/e13151fa20c729f56e0b30d70dada464a7790179_eexpressarticle1.png?auto=compress,format","categories":["Zalando","Data Analytics","Distributed Data","Elasticsearch","Search"],"description":"An applicance allowing teams to serve and retrieve data at a large scale, used in production.","publish_date":"2016-11-24 00:00:00","link":"https://engineering.zalando.com/posts/2016/11/a-closer-look-at-elasticsearch-express.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1d7de2a815ce569ff851893d9bcc69a7","publish_timestamp":1479859200,"title":"Bread&Butter 2016: The Livestreaming Rollercoaster","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/e6d571acd0b1b4cd29b3955d669815ebce6ea530_12_bb-preview_credits-nils-krager.jpg?auto=compress,format","categories":["Zalando","APIs","Facebook"],"description":"We took social media broadcasting to the next level at this year's B&amp;&amp;B showcase.","publish_date":"2016-11-23 00:00:00","link":"https://engineering.zalando.com/posts/2016/11/bb-the-livestreaming-rollercoaster.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"bfa9b9b78657c643d635eccb0bc32354","publish_timestamp":1479772800,"title":"The Art of Mob Programming","blogName":"Zalando","image":"","categories":["Zalando","Producer","Product Management"],"description":"One of our Dortmund teams have tried out the relatively new concept of Mob Programming.","publish_date":"2016-11-22 00:00:00","link":"https://engineering.zalando.com/posts/2016/11/the-art-of-mob-programming.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"56126da4087c56cab4e7ac6a2824206e","publish_timestamp":1479340800,"title":"Why Did I Decide to Become a Producer?","blogName":"Zalando","image":"","categories":["Zalando","Producer","Product","Product Management"],"description":"As a Producer, you are instrumental in the organization of teams at Zalando Tech.","publish_date":"2016-11-17 00:00:00","link":"https://engineering.zalando.com/posts/2016/11/why-did-i-decide-to-become-a-producer.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"55298a4db9f62656f8c5aead45de3e64","publish_timestamp":1479254400,"title":"The State of Frontend at Zalando 2016","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/427d2dd4fcb049de43592514a791616bc8168817_elmsurveyresults.png?auto=compress,format","categories":["Zalando","Angular","ES6","Elm","Frameworks","Frontend","JavaScript","React"],"description":"What is the flavour of JavaScript and frontend at Zalando Tech? Find out here!","publish_date":"2016-11-16 00:00:00","link":"https://engineering.zalando.com/posts/2016/11/the-state-of-frontend-at-zalando-2016.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e2795e50e726c384eea77864f22f0c10","publish_timestamp":1479168000,"title":"How InnerSource bolstered integration for Local Order Fulfillment","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/b0e2d84460b103dc6e6839e1fead4b2af0c3ea12_161005_local-order-fulfillment_process_zalando-hires.jpg?auto=compress,format","categories":["Zalando","GitHub","InnerSource","Logistics"],"description":"We're piloting InnerSource here at Zalando and have already had some early success.","publish_date":"2016-11-15 00:00:00","link":"https://engineering.zalando.com/posts/2016/11/how-innersource-bolstered-integration-for-local-order-fulfillment.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1a000cc38fc216080cddd1b857bbbc15","publish_timestamp":1478736000,"title":"The RecSys’16 Review","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/9faefcc1c51d2dc7960bcbf93888eca00a6c9cca_img_0601.jpg?auto=compress,format","categories":["Zalando","Data Science","Deep Learning","Machine Learning","Neural Networks","Recommender Systems","Zalando Dublin"],"description":"All the learnings we brought back from the ACM Conference on Recommender Systems.","publish_date":"2016-11-10 00:00:00","link":"https://engineering.zalando.com/posts/2016/11/the-recsys-16-review.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f3fd685b8caac9e9283e20e8a5745e5f","publish_timestamp":1478649600,"title":"Angular2: Final Release Unit Test Migration Guide","blogName":"Zalando","image":"","categories":["Zalando","Angular","Frontend","JavaScript","Testing"],"description":"About to use Angular2? Need to refactor your codebase? Take a read of this handy guide.","publish_date":"2016-11-09 00:00:00","link":"https://engineering.zalando.com/posts/2016/11/angular2-rc4-to-rc5-unit-test-migration-guide.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"c62f3986128ad15009c317863d27fce7","publish_timestamp":1478563200,"title":"Bootcamp for Fashpreneurs – Reimagining Online Fashion","blogName":"Zalando","image":"","categories":["Zalando","Mobile","Mobile First","Startups","Tech Entrepreneurs","Tech Startups"],"description":"Zalando will run its first bootcamp dedicated to Fashpreneurs in Berlin from 25-27 November.","publish_date":"2016-11-08 00:00:00","link":"https://engineering.zalando.com/posts/2016/11/bootcamp-for-fashpreneurs-reimagining-online-fashion.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"6bf4cf99d6dfc833d09092320610eb49","publish_timestamp":1478476800,"title":"Delivering a Cross-Site Project","blogName":"Zalando","image":"","categories":["Zalando","OKRs","Zalando Dublin"],"description":"Align your goals, produce software at speed, and be agile enough to incorporate change.","publish_date":"2016-11-07 00:00:00","link":"https://engineering.zalando.com/posts/2016/11/delivering-a-cross-site-project.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f490ba21dbc377e21ea9132670e5b2e5","publish_timestamp":1478217600,"title":"Doing Data Science the Cloud and Distributed Way","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/ad71dc6290b586f7a67d2413a1d8b23a305ebb2e_zeppelin-datascience-blogpost-2.png?auto=compress,format","categories":["Zalando","Data Science","Python","Zalando Dublin"],"description":"See how we're iterating the way we do early stage exploratory data science with large datasets.","publish_date":"2016-11-04 00:00:00","link":"https://engineering.zalando.com/posts/2016/11/doing-data-science-the-cloud-and-distributed-way.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a024bdecd49452d6d2f50d3a53be25d7","publish_timestamp":1478131200,"title":"The Sprint Exposed – How we Use it at Zalando","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/2f6a52dff0c39f0adfc0249bc0e363d1fc04832d_ebde1b47795447d66f88d80d28dcd7b9.jpg?auto=compress,format","categories":["Zalando","Design Sprint","Design Thinking","Product"],"description":"How we go about answering the most critical project questions and doubts in just one week.","publish_date":"2016-11-03 00:00:00","link":"https://engineering.zalando.com/posts/2016/11/the-sprint-exposed--how-we-use-it-at-zalando.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"2a5562257cbd18423ebe5c92a1b8f2ed","publish_timestamp":1477612800,"title":"How Failing Fast Drives us Forward at Zalando Tech","blogName":"Zalando","image":"","categories":["Zalando"],"description":"Failing fast, getting the right feedback, and remembering to test everything you're working on.","publish_date":"2016-10-28 00:00:00","link":"https://engineering.zalando.com/posts/2016/10/how-failing-fast-drives-us-forward-at-zalando-tech.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"edca35b363826fb81786c3cb773327f6","publish_timestamp":1477526400,"title":"Building an Open Source Culture at Europe’s Largest Fashion Platform","blogName":"Zalando","image":"","categories":["Zalando","GitHub","InnerSource","Open Source"],"description":"Lauri Apple discusses Zalando's open source transformation and how it's shaped the company.","publish_date":"2016-10-27 00:00:00","link":"https://engineering.zalando.com/posts/2016/10/building-an-open-source-culture-at-europes-largest-fashion-platform.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"30111915f0a9b7c6fe3e74697e6730a5","publish_timestamp":1477440000,"title":"Data Science and AI in the Spotlight with our VP, Alex Rahin","blogName":"Zalando","image":"","categories":["Zalando","Artificial Intelligence","Data Platforms","Data Science","Machine Learning"],"description":"Introducing our new VP of Data and Machine Learning Platforms to the world, Alex Rahin.","publish_date":"2016-10-26 00:00:00","link":"https://engineering.zalando.com/posts/2016/10/data-science-and-ai-in-the-spotlight-with-our-vp-alex-rahin.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"ecae5cbe12977821a0c1790c3df4960e","publish_timestamp":1477353600,"title":"Deep Learning for Understanding Consumer Histories","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/2ac1f20f2c8beeb95a9d28988aa1966a077dd3ba_consumer_history.png?auto=compress,format","categories":["Zalando","AdTech","Data Science","Deep Learning","Machine Learning","Recurrent Neural Networks","Zalando Hamburg"],"description":"See how we're delivering better consumer experiences with recurrent neural networks.","publish_date":"2016-10-25 00:00:00","link":"https://engineering.zalando.com/posts/2016/10/deep-learning-for-understanding-consumer-histories.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3c2342d2cc95e7f752a91718824f6282","publish_timestamp":1476835200,"title":"Zalando Tech x Strange Loop 2016","blogName":"Zalando","image":"","categories":["Zalando","Akka","Apache Kafka","Cassandra","Debugging","Linux","Scala"],"description":"Take a look at our rundown of Strange Loop 2016 all the way from the USA.","publish_date":"2016-10-19 00:00:00","link":"https://engineering.zalando.com/posts/2016/10/zalando-tech-x-strange-loop-2016.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"c434ea628c511b1dc2b461f483453a94","publish_timestamp":1476748800,"title":"Research Roles at Zalando Research: What You Need To Know","blogName":"Zalando","image":"","categories":["Zalando","Artificial Intelligence","Data Science","Deep Learning","Machine Learning","Natural Language Processing","Research","Zalando Research"],"description":"We have created three job profiles that outline how we organize research within Zalando.","publish_date":"2016-10-18 00:00:00","link":"https://engineering.zalando.com/posts/2016/10/research-roles-at-zalando-research.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"733f7976068297a3d7f92f59098e539e","publish_timestamp":1476403200,"title":"Copywriting for Emotion","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/97353c896e68f5911149f75044cd4ba94d7b9be2_piramid_new.png?auto=compress,format","categories":["Zalando","UI","UX","User Experience"],"description":"With users in 15 countries, we cater to a variety of shopping behaviours and needs.","publish_date":"2016-10-14 00:00:00","link":"https://engineering.zalando.com/posts/2016/10/copywriting-for-emotion.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"730567fe0483895b408a96a511e0427c","publish_timestamp":1476230400,"title":"Techsperts at Scale: Tips to Grow Your Business","blogName":"Zalando","image":"","categories":["Zalando","Scalability","Startups","Tech Startups","Zalando Techspert Series"],"description":"We sat down with this month's Techsperts to chat about the business of businesses.","publish_date":"2016-10-12 00:00:00","link":"https://engineering.zalando.com/posts/2016/10/techsperts-at-scale-tips-to-grow-your-business.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"dad22d7e0e606dadcd3168f437347520","publish_timestamp":1476144000,"title":"Key Talks and Takeaways from the AnDevCon Conference","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/1f413fc4f833a8a2cf6dcb041c1019784596ebe8_adc_show_floor.jpg-t1472680389567width710height362nameadc_show_floor.jpg?auto=compress,format","categories":["Zalando","Android","Apps","Java","Mobile","Mobile First"],"description":"A great resource of information and insight into Android technology and development.","publish_date":"2016-10-11 00:00:00","link":"https://engineering.zalando.com/posts/2016/10/key-talks-and-takeaways-from-the-andevcon-conference.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a785ff0c6d38543d8201426ddc8ad58b","publish_timestamp":1475712000,"title":"5G: The Future of Wireless Networks","blogName":"Zalando","image":"","categories":["Zalando","Mobile","Mobile First"],"description":"Want to know what's in store for the world with the approach of 5G connectivity?","publish_date":"2016-10-06 00:00:00","link":"https://engineering.zalando.com/posts/2016/10/5g-the-future-of-wireless-networks.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"92d941f299b1e7e9e661ecf48a862eb0","publish_timestamp":1475625600,"title":"Jimmy to Microservices – The Journey One Year Later","blogName":"Zalando","image":"","categories":["Zalando","Clojure","Elm","Innkeeper","Microservices","Skipper"],"description":"Read about our microservices adventure more than a year after making the jump.","publish_date":"2016-10-05 00:00:00","link":"https://engineering.zalando.com/posts/2016/10/jimmy-to-microservices-the-journey-one-year-later.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"9f20431dcd5ca1f48469b022e05e9887","publish_timestamp":1475539200,"title":"How a Summer University for Women Makes a Difference","blogName":"Zalando","image":"","categories":["Zalando","Git","Scala","Women in Tech"],"description":"The German Summer University for women in Bremen tackles the issue of Women in Tech.","publish_date":"2016-10-04 00:00:00","link":"https://engineering.zalando.com/posts/2016/10/how-a-summer-university-for-women-makes-a-difference.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1322f8a7cf59c055cd3755f5ac6a8c68","publish_timestamp":1475193600,"title":"Our Engineers get Hands-On at Flow Festival","blogName":"Zalando","image":"","categories":["Zalando","Zalando Helsinki"],"description":"Read about how our Zelsinkis created audio-reactive visuals for the club setup of Flow Festival.","publish_date":"2016-09-30 00:00:00","link":"https://engineering.zalando.com/posts/2016/09/our-engineers-get-hands-on-at-flow-festival.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3daeac5b4c43793ae96821d257e0158f","publish_timestamp":1475020800,"title":"Zalando Launches Research Lab","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/164ce4c12f5b29bc926b7cb2aec1ef41d71e1e88_2016-07-25-research-team-8056.jpg?auto=compress,format","categories":["Zalando","Artificial Intelligence","Data Science","Machine Learning","Research","Zalando Research"],"description":"Zalando Tech is embarking on an exciting new chapter in its evolution as a platform.","publish_date":"2016-09-28 00:00:00","link":"https://engineering.zalando.com/posts/2016/09/zalando-launches-research-lab.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"62a375bcc5110ab07ac96f7e447373fe","publish_timestamp":1474588800,"title":"User Story Mapping from a Backend Perspective","blogName":"Zalando","image":"","categories":["Zalando","Product","UX","User Experience","User Research"],"description":"We think it's important that our developers get a clear picture of our users, too.","publish_date":"2016-09-23 00:00:00","link":"https://engineering.zalando.com/posts/2016/09/user-story-mapping-from-a-backend-perspective.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e4f05687026ae8eef0288ced78b0fcb9","publish_timestamp":1474416000,"title":"Our ReactEurope Recap","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/7bb160cdc8ea8a5ca5927cc5a4dddc7de0d35081_reacteurope2.jpg?auto=compress,format","categories":["Zalando","Facebook","GraphQL","Mobile","Open Source","React"],"description":"Zalando's attendance at this year’s ReactEurope Conference in Paris was a no-brainer.","publish_date":"2016-09-21 00:00:00","link":"https://engineering.zalando.com/posts/2016/09/our-reacteurope-recap.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"dc372f12720a8739f788c933fc5002e5","publish_timestamp":1474329600,"title":"GOTO 2016 – From Monolith to Microservices at Zalando","blogName":"Zalando","image":"","categories":["Zalando","AWS","Docker","GOTO Conference","Microservices","STUPS"],"description":"Rodrigue Schaefer at GOTO 2016 on the challenges of our transition from monolith to microservices.","publish_date":"2016-09-20 00:00:00","link":"https://engineering.zalando.com/posts/2016/09/goto-2016-from-monolith-to-microservices.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"37754eafb6bf3b63f7b9810d0d8569e1","publish_timestamp":1473897600,"title":"Juggling Expectations and Reality in UX Job Ads","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/48eea6059dba091cfde83f815b7750c17c57c792_buttonsurv.png?auto=compress,format","categories":["Zalando","Design","Interaction Design","Research","Tech Jobs","UI","UX","User Experience","User Interaction","User Research"],"description":"Help us reduce frustration and inspire the UX community to create well fitting job ads.","publish_date":"2016-09-15 00:00:00","link":"https://engineering.zalando.com/posts/2016/09/juggling-expectations-and-reality-in-ux-job-ads.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"357122bf5154c5bfea70c01c308180a8","publish_timestamp":1473811200,"title":"Pass props and keeping the DOM neat in a React Isomorphic App","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/c86f6bf97d9dde6d05ca325187e11bc536339e1c_screen-shot-2016-08-10-at-11.33.42.png?auto=compress,format","categories":["Zalando","React","Redux"],"description":"Want a nicer DOM when debugging HTML in the inspector? Read more right here.","publish_date":"2016-09-14 00:00:00","link":"https://engineering.zalando.com/posts/2016/09/keeping-the-dom-neat-in-a-react-isomorphic-app.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"00a7a752e72513d0c47d82cc699e4c76","publish_timestamp":1473724800,"title":"All About Startups At The Latest Techspert Panel","blogName":"Zalando","image":"","categories":["Zalando","Tech Culture","Tech Entrepreneurs","Tech Events","Tech Startups","Zalando Techspert Series"],"description":"A recap with this month's Techspert Panelists on the hottest topic in Berlin: Startups.","publish_date":"2016-09-13 00:00:00","link":"https://engineering.zalando.com/posts/2016/09/all-about-startups-at-the-latest-techspert-panel.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b84553b3bf2ec1c4d6cc859bc98503ca","publish_timestamp":1473292800,"title":"How Can Your Company Radically Curb Insider Threat?","blogName":"Zalando","image":"","categories":["Zalando","Security"],"description":"Security needs to stop being an afterthought of the production line when building software.","publish_date":"2016-09-08 00:00:00","link":"https://engineering.zalando.com/posts/2016/09/how-can-your-company-radically-curb-insider-threat.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0773f6fb6c072ab8c076fb9e1be5c7c8","publish_timestamp":1471996800,"title":"What knowledge should you have to be a frontend developer?","blogName":"Zalando","image":"","categories":["Zalando","Angular","CSS","Continuous Delivery","Frontend","JavaScript","React","TDD","Testing"],"description":"Read on to find out which skills we think a well-rounded frontend developer should possess.","publish_date":"2016-08-24 00:00:00","link":"https://engineering.zalando.com/posts/2016/08/what-knowledge-should-you-have-to-be-a-good-frontend-developer.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"8f261fe2970cc0dbf451e5e91c47372b","publish_timestamp":1471910400,"title":"Why Do We Have Autonomous Teams?","blogName":"Zalando","image":"","categories":["Zalando","Management","Zalando Dublin"],"description":"You're invited to catch a glimpse into what makes autonomous teams tick at Zalando.","publish_date":"2016-08-23 00:00:00","link":"https://engineering.zalando.com/posts/2016/08/why-do-we-have-autonomous-teams.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"70d81cc0d4186832d803c0c31c21d389","publish_timestamp":1471392000,"title":"Radical Agility 101: Study Notes","blogName":"Zalando","image":"","categories":["Zalando","APIs","AWS","Microservices"],"description":"Looking for a quick breakdown of all things Radical Agility? We've got you covered.","publish_date":"2016-08-17 00:00:00","link":"https://engineering.zalando.com/posts/2016/08/radical-agility-study-notes.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3995f546d5c16017214f1078b51dc8f7","publish_timestamp":1471305600,"title":"Emerging Tech Hubs Around The World","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/ee7f60039b2a349942d607753f24bae81db653ce_moscow-1556561_1920.jpg?auto=compress,format","categories":["Zalando","Tech Culture","Tech Entrepreneurs","Tech Jobs","Tech Startups","Zalando Dublin","Zalando Helsinki"],"description":"Check out our assessment of the emerging tech hubs that are trending worldwide right now.","publish_date":"2016-08-16 00:00:00","link":"https://engineering.zalando.com/posts/2016/08/emerging-tech-hubs-around-the-world.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a869b093f822d05296c58e779e9445a8","publish_timestamp":1471219200,"title":"End-to-End Latency Challenges for Microservices","blogName":"Zalando","image":"","categories":["Zalando","APIs","Erlang","Functional Programming","Microservices","Open Source","Scalability","Zalando Helsinki"],"description":"Read about Typhoon, our open source project to assess distributed software architecture.","publish_date":"2016-08-15 00:00:00","link":"https://engineering.zalando.com/posts/2016/08/end-to-end-latency-challenges-for-microservices.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3950637c920a77678fce3a092edc0e1f","publish_timestamp":1470873600,"title":"A closer look at the ClassNames npm package","blogName":"Zalando","image":"","categories":["Zalando","JavaScript","React"],"description":"Looking at a useful package that all teams using React should be familiar with.","publish_date":"2016-08-11 00:00:00","link":"https://engineering.zalando.com/posts/2016/08/a-closer-look-at-the-classnames-npm-package.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"51d0410111a15e3d1d5597d6fcb407e4","publish_timestamp":1470787200,"title":"Can interviewing people make you a better conversationalist?","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/8fb786b6cc647cdcba89064e958707efef227c0a_img_0045.jpg?auto=compress,format","categories":["Zalando","Customer Centricity","Education","Teaching","User Experience","User Research"],"description":"If your job involves interacting with people to get to know them, you need to read this.","publish_date":"2016-08-10 00:00:00","link":"https://engineering.zalando.com/posts/2016/08/can-interviewing-people-can-make-you-a-better-conversationalist.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"109421f94d2a3cd28192ffdf232086eb","publish_timestamp":1470700800,"title":"Welcome to the family, Zalando AdTech Lab Hamburg!","blogName":"Zalando","image":"","categories":["Zalando","AdTech","Apache Spark","Data Science","Machine Learning","Platform Strategy","TensorFlow","Zalando Hamburg"],"description":"We're introducing you to another member of the Zalando family: Zalando AdTech Lab Hamburg.","publish_date":"2016-08-09 00:00:00","link":"https://engineering.zalando.com/posts/2016/08/zalando-adtech-lab-hamburg.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"52ee50be6badfe4962459159096c48e0","publish_timestamp":1470355200,"title":"Talking to Techsperts: The Price of Employee Freedom","blogName":"Zalando","image":"","categories":["Zalando","Tech Culture","Tech Entrepreneurs","Tech Events","Zalando Techspert Series"],"description":"We sat down with this month's Techsperts to expand on ideas about autonomy in the workplace.","publish_date":"2016-08-05 00:00:00","link":"https://engineering.zalando.com/posts/2016/08/talking-to-techsperts.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5a0a3839ad2130d88dfc48c9b869279c","publish_timestamp":1470182400,"title":"Zalando Dortmund's RuhrJS Journal","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/a55fe80e95a82e89e05b5e66097e1bb810b50499_sacrificial.jpg?auto=compress,format","categories":["Zalando","Angular","JavaScript","React","Redux","Zalando Dortmund"],"description":"A summary of the most interesting talks we attended at RuhrJS, plus our learnings and takeaways.","publish_date":"2016-08-03 00:00:00","link":"https://engineering.zalando.com/posts/2016/08/zalando-dortmunds-ruhrjs-journal.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5bf8dd4372c30654404b9f378a332c96","publish_timestamp":1470096000,"title":"An Obsession with Design Patterns: Redux","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/ff54323021b8177c0c6cf0a03bfbd1edcc08c276_redux-flow.png?auto=compress,format","categories":["Zalando","Design Patterns","Education","JavaScript","React","Redux"],"description":"Our deep dive into design patterns continues with Redux, the State Tree, and the Connect Method.","publish_date":"2016-08-02 00:00:00","link":"https://engineering.zalando.com/posts/2016/08/design-patterns-redux.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"232f0d30529c2196f491280c095d7526","publish_timestamp":1470009600,"title":"JAX Finance Learnings from London","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/8bc39a5b06a30f8ac9deba1754f84341cf3cb6a3_img_0089.jpg?auto=compress,format","categories":["Zalando","Continuous Delivery","Java","Zalando Dortmund"],"description":"Read about Zalando Dortmund's travels to London to hear about payment topics at JAX Finance.","publish_date":"2016-08-01 00:00:00","link":"https://engineering.zalando.com/posts/2016/08/jax-finance-learnings-from-london.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e8fb550ff66ead2745843b2547ddd9ae","publish_timestamp":1469664000,"title":"Best Practices for Android Developer Productivity","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/f3283ff761186baf95d8638226d319d73128096a_pasted-image-0.png?auto=compress,format","categories":["Zalando","Android","Droidcon","Gradle","Mobile","Mobile First"],"description":"Level up your Android development skills with these tried and true tips from Sergii Zhuk.","publish_date":"2016-07-28 00:00:00","link":"https://engineering.zalando.com/posts/2016/07/best-practices-for-android-developer-productivity.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b7ea09deb0618ee98d40e891830ed0dc","publish_timestamp":1468972800,"title":"Zalando makes a Connexion: Our interview with Tony Tam","blogName":"Zalando","image":"","categories":["Zalando","APIs","Clojure","Connexion","Flask","OpenAPI","Play Framework","PlaySwagger","Python"],"description":"We chat to the creator of Swagger, Tony Tam, to hear his thoughts on Zalando's Connexion.","publish_date":"2016-07-20 00:00:00","link":"https://engineering.zalando.com/posts/2016/07/connexion-interview-with-tony-tam.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3cf42f5aac79b021e9313dd8a32c6006","publish_timestamp":1468886400,"title":"The Factory Pattern in React","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/f355e73881cbc9a16d494def7bb3a98c94523c16_screen-shot-2016-07-06-at-09.20.32.png?auto=compress,format","categories":["Zalando","JavaScript","React","Teaching"],"description":"One of our developers shares her learnings and takeaways as she dives deeper into React.","publish_date":"2016-07-19 00:00:00","link":"https://engineering.zalando.com/posts/2016/07/the-factory-pattern-in-react.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"9070389a770bce8cdb60b804bb9c41e9","publish_timestamp":1468540800,"title":"Dynamic App Content: An Introduction to Truly Native Apps","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/57ba1aebaeaddf3c6baa5d6640e6b8da283a58d4_tna-elements.png?auto=compress,format","categories":["Zalando","Android","Mobile","Mobile First","React","Tailor","Zalando App"],"description":"Read about the challenges we experienced when designing the home screen of the Zalando App.","publish_date":"2016-07-15 00:00:00","link":"https://engineering.zalando.com/posts/2016/07/an-introduction-to-truly-native-apps.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"bf0a42387aa16b139ea832c6f3cdc93a","publish_timestamp":1468368000,"title":"Scaling Our Tech Organization and Architecture","blogName":"Zalando","image":"","categories":["Zalando","Microservices","Scalability"],"description":"Zalando Tech likes to set itself big challenges – hear more about them with Dan Persa.","publish_date":"2016-07-13 00:00:00","link":"https://engineering.zalando.com/posts/2016/07/scaling-our-tech-organization-and-architecture.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"7f6600846dc8e5ff867712f4109ca8d5","publish_timestamp":1468281600,"title":"Building services with the Akamai API Open API using Go","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/5871c24952c99f5c0e6fd1f0ea720d9cf06ba109_akamaishot.png?auto=compress,format","categories":["Zalando","APIs","GitHub","Golang","Skipper"],"description":"Read about our new Go library that Akamai GitHub uses as their default Go implementation.","publish_date":"2016-07-12 00:00:00","link":"https://engineering.zalando.com/posts/2016/07/building-services-with-the-akamai-api-open-api-using-go.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"efcf2433342869f513e0a5ad276aa2a4","publish_timestamp":1467936000,"title":"Top 5 Techpreneurs Revolutionising Tech Culture","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/c2647db444018266d5f565b2a57125205bc41dca_tony-hsieh-net-worth.jpg?auto=compress,format","categories":["Zalando","Stack Overflow","Tech Culture","Tech Entrepreneurs"],"description":"We take a look at who's responsible for reshaping the definition of modern tech culture.","publish_date":"2016-07-08 00:00:00","link":"https://engineering.zalando.com/posts/2016/07/top-5-techpreneurs-revolutionising-tech-culture.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"c2fe91887e421d2440fbe7cb4ef6b958","publish_timestamp":1467849600,"title":"Proper Use of CellForRowAtIndexPath and WillDisplayCell","blogName":"Zalando","image":"","categories":["Zalando","Mobile","Mobile First","iOS"],"description":"Yunus Güzel gives us his take on Alexander Orlov's iOS scrolling performance arguments.","publish_date":"2016-07-07 00:00:00","link":"https://engineering.zalando.com/posts/2016/07/proper-use-of-cellforrowatindexpath-and-willdisplaycell.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d5f3cbce05644c9bdb1ef1399eb46dfc","publish_timestamp":1467763200,"title":"Zalando’s Tech Academy gets cosy with GitHub","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/8b48097b4eb92e11aef749247e7313b568cda8d4_pngbase64679999866ae3d5da.png?auto=compress,format","categories":["Zalando","GitHub"],"description":"Want to hear about how our recent workshops with GitHub went at Zalando Tech?","publish_date":"2016-07-06 00:00:00","link":"https://engineering.zalando.com/posts/2016/07/zalandos-tech-academy-gets-cosy-with-github.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0caacd22b3362aa62ea44a80a4893243","publish_timestamp":1467331200,"title":"Healthy habits every software engineer should adopt","blogName":"Zalando","image":"","categories":["Zalando","Debugging","Tech Culture"],"description":"We’ve outlined some of the healthiest habits that every developer should start practicing.","publish_date":"2016-07-01 00:00:00","link":"https://engineering.zalando.com/posts/2016/07/healthy-habits-every-software-engineer-should-adopt.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"8040f98f8598a7c3cfdbf3ba869ed825","publish_timestamp":1467244800,"title":"Highlights of the CASI conference","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/0dec234c11ea3ba534f2b4f1824f59f82ac7647c_casi-conf-1.jpeg?auto=compress,format","categories":["Zalando","Data Science","Research","Zalando Dublin"],"description":"Our Dublin Data Scientists have been making themselves heard in the wider statistics community.","publish_date":"2016-06-30 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/highlights-of-the-casi-conference.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3b2d37e72e7898459680ceb1ffbe850e","publish_timestamp":1467158400,"title":"Which shoe fits you? Comparing Akka Streams, Actors, and Plain Futures","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/29a6a9659793da991275c77a2126bc2b0740aa36_benchmarking-results-jhofer.png?auto=compress,format","categories":["Zalando","Akka","Play Framework","Reactive Streams","Scala"],"description":"We explore which architecture to implement for a component that is critical to our platform.","publish_date":"2016-06-29 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/comparing-akka-streams-actors-and-plain-futures.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0cebcb50be1d94ec00ea055b064c2e5b","publish_timestamp":1467072000,"title":"Revolutionising fashion at our Dublin HQ","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/1d78ecca3a9b522fb9057ab441645028c3ff6611_img_4638.jpg?auto=compress,format","categories":["Zalando","Big Data","Data Science","Fashion Insights Centre","Tech Jobs","Zalando Dublin"],"description":"Zalando Tech’s Dublin Hub is getting some attention with the help of Tech/Life Ireland.","publish_date":"2016-06-28 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/revolutionising-fashion-at-our-dublin-hq.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"176c777047bfda8c38da084cc4bb231a","publish_timestamp":1466640000,"title":"Feature Extraction: Science or Engineering?","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/df0d47562bbfe1d272f44d6942a9602213df998b_fart2ml----blog.png?auto=compress,format","categories":["Zalando","Data Science","Machine Learning","Recommender Systems"],"description":"It's time for feature engineering to stop being neglected in the machine learning literature.","publish_date":"2016-06-23 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/feature-extraction-science-or-engineering.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"2756dd9dd8f8783e1dd2f2d4a4b11c5e","publish_timestamp":1466553600,"title":"Interview preparation tips for Java developers","blogName":"Zalando","image":"","categories":["Zalando","HackerRank","Java","Microservices","Stack Overflow","Tech Jobs"],"description":"Do you program in Java? Are you preparing for an interview? Then you should read this.","publish_date":"2016-06-22 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/interview-preparation-tips-for-java-developers.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"fb2dbddf0f74ba3150a523ed2efce162","publish_timestamp":1466467200,"title":"Integrated Commerce and our Merchant Center rebuild","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/519af30f9da9be2bca4aaa5d7759dc2a1a3546b8_integrated-commerce_en.jpg?auto=compress,format","categories":["Zalando","AWS","Akka","Angular","ES6","Microservices","PostgreSQL","Scala"],"description":"Get an insight from Brand Solutions into our work behind the Zalando Merchant Center rebuild.","publish_date":"2016-06-21 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/integrated-commerce-merchant-centre-rebuild.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"37ed45687a0a40f80b76d29c8b08d86d","publish_timestamp":1465948800,"title":"The Product Specialist role in a Distributed Team Setup","blogName":"Zalando","image":"","categories":["Zalando","Product","Product Management","Zalando Helsinki"],"description":"Hear all about Product Management before and after Radical Agility at Zalando Tech.","publish_date":"2016-06-15 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/the-product-specialist-role-in-a-distributed-team-setup.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b378f1138421fc60ba9d6914b5b0dd46","publish_timestamp":1465862400,"title":"One year of Radical Agility – goodbye Angular (1), hello React","blogName":"Zalando","image":"","categories":["Zalando","Angular","ES6","JavaScript","React","Redux","Zalando Dortmund"],"description":"It's out with the old, in with the new for Team Phrasemongers at Zalando Dortmund.","publish_date":"2016-06-14 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/goodbye-angular-1-hello-react.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"811331401443c02823f14eea313df419","publish_timestamp":1465776000,"title":"Falling in Love with Tech in Helsinki","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/703e5e415101a2ff8531ea2355fffd786aefbf09_rails-girls-2016-helsinki-175.jpg?auto=compress,format","categories":["Zalando","Education","Women in Tech","Zalando Helsinki"],"description":"How a non-dev techie took a radical approach to understand her working environment.","publish_date":"2016-06-13 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/falling-in-love-with-tech-in-helsinki.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"06f213badab2ee22a35422ffa53490fa","publish_timestamp":1465516800,"title":"Pushing the boundaries: Human interaction with technology","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/69996ceb6fcb8a67e41080e7b8be22459e5f7b86_img-20160523-wa0001.jpg?auto=compress,format","categories":["Zalando","JSConf","JSConf EU","JavaScript","Women in Tech"],"description":"At JSConf 2016 in Budapest, we gave some kickass insights into tech and human interaction.","publish_date":"2016-06-10 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/human-interaction-with-technology.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b3f6732c09fcec9f46ad0c730cad4b8d","publish_timestamp":1465430400,"title":"Why should your kid code?","blogName":"Zalando","image":"","categories":["Zalando","Education","Innovation","Tech Culture"],"description":"To take part in the societies of tomorrow, kids should learn to code today.","publish_date":"2016-06-09 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/why-should-your-kid-code.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"10f741992e5eaecfc0908aeb3c3c2ec3","publish_timestamp":1465344000,"title":"Zappr – Enhancing your GitHub workflow","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/5b3ed00a42ec81e5e67b6c3e358a30140520e7d0_zappr-image.png?auto=compress,format","categories":["Zalando","Code Review","GitHub","Open Source"],"description":"An open source tool to guarantee effective code reviews on GitHub, using only GitHub.","publish_date":"2016-06-08 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/zappr--enhancing-your-github-workflow.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a4f0743fa3f05909966ed8a7b7b3295c","publish_timestamp":1465257600,"title":"Five Tech Jobs That Didn’t Exist Five Years Ago","blogName":"Zalando","image":"","categories":["Zalando","Big Data","Data Science","Golang","Tech Jobs","UX"],"description":"What tech jobs are seen as the norm today that barely existed five years ago?","publish_date":"2016-06-07 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/five-tech-jobs-that-didnt-exist-five-years-ago.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"4fcb6132abf9671162ad1645520895e7","publish_timestamp":1464825600,"title":"Better streaming layouts for frontend microservices with Tailor","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/db20bec7a14fa70cbc8f8cb11d4dadfa1088b8ab_router-layout-image.jpg?auto=compress,format","categories":["Zalando","Akka","Frontend","Microservices","Open Source","Scala","Tailor"],"description":"Learn about a straightforward approach to frontend microservices with Open Source and Tailor.","publish_date":"2016-06-02 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/frontend-microservices-tailor.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"aa5b2723dd542544e2e663ea01b0a53f","publish_timestamp":1464739200,"title":"The nuts and bolts of the Docker-Selenium project","blogName":"Zalando","image":"","categories":["Zalando","Docker","DockerSelenium","Open Source","Selenium"],"description":"Our Open Source Evangelist sits down to chat with the creator of Docker-Selenium.","publish_date":"2016-06-01 00:00:00","link":"https://engineering.zalando.com/posts/2016/06/docker-selenium-open-source.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"4f45a20d0d22493b40806db445fedbf4","publish_timestamp":1464652800,"title":"Scalable Fraud Detection for Zalando's Fashion Platform","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/1befeeb60af4c1ced452c642ea91f64504089370_dataflows.png?auto=compress,format","categories":["Zalando","Apache Spark","Big Data","Play Framework","Python","Scala"],"description":"Team Payana have been busy: Read about their migration efforts for the Zalando platform.","publish_date":"2016-05-31 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/scalable-fraud-detection-fashion-platform.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"902a8d1d6af14cd2dfad966a42a430da","publish_timestamp":1464220800,"title":"Zalando Tech are the new unicorns at Microservices Day","blogName":"Zalando","image":"","categories":["Zalando","Microservices","Microservices Day"],"description":"Check out how Zalando Tech made their mark at Microservices Day, London.","publish_date":"2016-05-26 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/zalando-tech-microservices-day-2016.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"9cff7f01a8dd90f759ebcb7567dbfb6b","publish_timestamp":1464134400,"title":"Can you hack it? Yes you can!","blogName":"Zalando","image":"","categories":["Zalando","Hackathon","HackerRank","Tech Culture"],"description":"Learn more about why Zalando's developers contributed to our upcoming online hackathon.","publish_date":"2016-05-25 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/zalando-tech-codesprint.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f3f7635f93609475a67869a21508ec46","publish_timestamp":1464048000,"title":"The Scala Travel Diary","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/58b9b97898d3cb9fc312f9bd573453bad00c46be_thumb_img_0725_1024.jpg?auto=compress,format","categories":["Zalando","Scala"],"description":"What's in Zalando’s travel itinerary for the Scala world? Find out about our travel plans here.","publish_date":"2016-05-24 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/the-scala-travel-diary.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"2cf60f59e414b7d55bf54848d3a4b95b","publish_timestamp":1464048000,"title":"Zalando Techspert Series: How to foster an innovative culture","blogName":"Zalando","image":"","categories":["Zalando","Tech Culture","Zalando Techspert Series"],"description":"A Zalando-led initiative to encourage discussions about soft tech with leading organisations.","publish_date":"2016-05-24 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/zalando-techspert-series-launch.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f3f52f3e99f21884cf15496a47b119dc","publish_timestamp":1463702400,"title":"Our polyglot approach: Getting started with Rust","blogName":"Zalando","image":"","categories":["Zalando","Docker","Microservices","Rust"],"description":"Dan Persa shares his first dive into the multi-paradigm programming language Rust.","publish_date":"2016-05-20 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/getting-started-with-rust.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"51f93c5fb249601fc67140efad56a0d4","publish_timestamp":1463616000,"title":"We couldn't get enough: Stack Overflow, Round 2","blogName":"Zalando","image":"","categories":["Zalando","Stack Overflow","Tech Culture"],"description":"We chat once again with the Stack Overflow team about developers, recruiting, and more.","publish_date":"2016-05-19 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/stack-overflow-round-2.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"52caaedf667e1bce4f6043609c3ed273","publish_timestamp":1463529600,"title":"A resilient, Zookeeper-less Solr architecture on AWS","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/aa60ef61da5cadde75f223ec47179491ec0687dd_solr-aws-image-1.png?auto=compress,format","categories":["Zalando","AWS","Java","Zookeeper"],"description":"The Recommendations team is back, providing a design for deploying Solr on AWS.","publish_date":"2016-05-18 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/zookeeper-less-solr-architecture-aws.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"909ae8f2d6dc8dfdc186ba88e1bd695b","publish_timestamp":1463443200,"title":"Zalando's Tech Radar: All you need to know","blogName":"Zalando","image":"","categories":["Zalando","Angular","Apache Flink","Apache Kafka","Scala"],"description":"All about our Tech Radar and its assessment of technologies used in software development.","publish_date":"2016-05-17 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/zalando-tech-radar.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f2593e676ec679d487db5f30682156eb","publish_timestamp":1463097600,"title":"When do you involve users in a user-centered design process?","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/364f519c19eea369cd99114e602c3c558e71846b_image-1.png?auto=compress,format","categories":["Zalando","Design","UX","User Experience"],"description":"Read about Clementine's preferred framework for considering users in digital product design.","publish_date":"2016-05-13 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/involve-users-user-centered-design-process.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e25bfafbf2eb9b4bec8883e9f29cb791","publish_timestamp":1463011200,"title":"How Radical Agility Helped us Stay on Track","blogName":"Zalando","image":"","categories":["Zalando","AWS","Retrospective"],"description":"See how data wizards Team Saiki got their s#!t together and made their project work.","publish_date":"2016-05-12 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/how-radical-agility-helped-us-stay-on-track.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"c26e35a1d26505c5b6253e384fa115c3","publish_timestamp":1462924800,"title":"How to deliver millions of personalised newsletter emails on AWS","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/5e51b53d84c20de1a6a92a314e989b2df945e000_newsletter-aws-1.png?auto=compress,format","categories":["Zalando","AWS","Java","Recommender Systems","Redis"],"description":"We look at the challenges around personalised recommendations in Zalando's newsletters.","publish_date":"2016-05-11 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/personalised-newsletter-emails-aws.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0c24ecbea4c0278bc21beb1991b55212","publish_timestamp":1462838400,"title":"Dortmund Docker Meetup – A cooperation between Zalando and Docker","blogName":"Zalando","image":"","categories":["Zalando","AWS","Docker","Meetup","Zalando Dortmund"],"description":"Join us in Dortmund for our new meetup group that's all things Docker.","publish_date":"2016-05-10 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/docker-meetup-group-dortmund.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"33573f109760e2ed8d7d5e848af43919","publish_timestamp":1462752000,"title":"How to avoid tapping the “Back” button in an interface design","blogName":"Zalando","image":"https://prismic-io.s3.amazonaws.com/zalando-jobsite%2F05c98c12-09c6-4476-8ac3-b3a3b8ab6c9f_1+fancy+back+back+240x427.gif","categories":["Zalando","Interaction Design","Mobile","Mobile First","Mobile Responsive Design","UX"],"description":"\"Back\" button interaction is essential for navigation, but it can also become counter-intuitive.","publish_date":"2016-05-09 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/avoid-back-button-interface-design.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d0078a9cf3c2fedb139fb89ab92ab71b","publish_timestamp":1462492800,"title":"Zalando explores the Hadoop Summit 2016","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/799719abd08496c51c35468da79d507a244fe833_img_1375.jpg?auto=compress,format","categories":["Zalando","Apache Flink","Apache Spark","Data Science","TensorFlow","Zalando Dublin"],"description":"Get the lowdown from this year's Hadoop Summit from Zalando's Dublin crew.","publish_date":"2016-05-06 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/zalando-hadoop-summit-2016.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"79e1698cfe38280c4ffc04c63c19a28b","publish_timestamp":1462233600,"title":"Migrating from Spray to Akka HTTP","blogName":"Zalando","image":"","categories":["Zalando","Akka","Scala"],"description":"We cover the trickiest changes when it comes to migrating from Spray to Akka HTTP.","publish_date":"2016-05-03 00:00:00","link":"https://engineering.zalando.com/posts/2016/05/migrating-spray-akka-http.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b67fed1b71ef39e050f900780e9aead9","publish_timestamp":1461888000,"title":"Teaching React: A different approach","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/b3fa5148fca5125dd3927b8ba24bae753860bacb_component-diagram.png?auto=compress,format","categories":["Zalando","Diversity in Tech","React","Teaching","Tech Culture"],"description":"Finding your latest beginners workshop too fast-paced? Andra did something about hers.","publish_date":"2016-04-29 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/andra-teaching-react.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5233d8c0cf3f2c375926cc1d55248e2b","publish_timestamp":1461888000,"title":"Why is Girls’ Day so important to Zalando Tech?","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/e042bf29a4af1c68c4b01c2010facc6df29a777f_girls-day-bmo.jpg?auto=compress,format","categories":["Zalando","Tech Culture"],"description":"Educating girls about Zalando and Technology is high on our company's To-Do list.","publish_date":"2016-04-29 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/girls-day-zalando-tech.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b0bc02014d62b304e29a97661d3b94b7","publish_timestamp":1461715200,"title":"Four lessons learned when working with Microservices","blogName":"Zalando","image":"","categories":["Zalando","APIs","Cassandra","Microservices","Zalando Dortmund"],"description":"Read about what our Dortmund team has learned on the road to implementing Microservices.","publish_date":"2016-04-27 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/four-lessons-with-microservices.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"fb1678be27c4fe4ccba98b1eb7037e95","publish_timestamp":1461542400,"title":"Radical Agility Profile: Eric Torreborre","blogName":"Zalando","image":"","categories":["Zalando","Dependency Injection","Scala"],"description":"The final video in our Radical Agility Series takes a look at one of our Scala superstars.","publish_date":"2016-04-25 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/ra-profile-eric-torreborre.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"fe6cb202d05b69a091b6cc37593e7aa8","publish_timestamp":1461283200,"title":"Progress recap: Elm Street 404","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/beead74c936055591e12629289f401ccedfe4ec3_01-screenshot.png?auto=compress,format","categories":["Zalando","Elm","Functional Programming","Open Source"],"description":"The guys behind Elm Street 404 give us an update on the game's progress.","publish_date":"2016-04-22 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/progress-recap-elm-street-404.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"77d0fabeead6a234071ef3ae34f3e75b","publish_timestamp":1461196800,"title":"Introducing the Zalando Web Guild","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/7b02e937cbbc960f24b2fbd6d0374f93c33ec1f6_thumb_img_0703_1024.jpg?auto=compress,format","categories":["Zalando","Frontend","JavaScript","Microservices"],"description":"A quick dive into the Web Guild's latest activities and announcing their first external meetup.","publish_date":"2016-04-21 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/zalando-web-guild.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"00b0c86ed50cc61030384855883782ba","publish_timestamp":1461110400,"title":"Radical Agility Profile: Princiya Marina Sequeira","blogName":"Zalando","image":"","categories":["Zalando","Frontend","Tech Culture","Tech Jobs"],"description":"Frontend development and Radical Agility are a match made in Zalando heaven.","publish_date":"2016-04-20 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/radical-agility-profile-princiya.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"2a5603cb6bd1c52090638f4fb07d9a54","publish_timestamp":1461024000,"title":"My onboarding experience, with love from Dortmund","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/ec01a6399a1b4fe744fbb21b65b607ec7844f2aa_mass-blog-photo-1.jpg?auto=compress,format","categories":["Zalando","Onboarding","Tech Culture","Zalando Dortmund"],"description":"Hear from one of our Producers in Dortmund all about his onboarding experiences.","publish_date":"2016-04-19 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/my-onboarding-experience.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"406b5f9836a87bb8d2082932df203f5c","publish_timestamp":1461024000,"title":"An interview with Dublin's Startup Commissioner Niamh Bushnell","blogName":"Zalando","image":"","categories":["Zalando","Diversity in Tech","Tech Culture","Tech Entrepreneurs","Tech Startups","Zalando Dublin"],"description":"We chat with Dublin's Startup Commissioner before our Tech Culture Panel event.","publish_date":"2016-04-19 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/niamh-bushnell-interview.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"cf6e8ad476bb7544da2e479e50ea5eba","publish_timestamp":1460937600,"title":"Zester – Unit Tests on Steroids","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/6acc33cb1e8e3f3eef5e7d963bb34b66b6d48927_zester-image-1.png?auto=compress,format","categories":["Zalando","Testing"],"description":"Are the tests you write good at catching bugs? Sebastian Monte introduces Zalando's Zester.","publish_date":"2016-04-18 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/zester-mutation-testing.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"95559442bc3d2a7fad7b4abcdce172b7","publish_timestamp":1460678400,"title":"Stack Overflow questions you should read if you program in Java","blogName":"Zalando","image":"","categories":["Zalando","Java","Stack Overflow"],"description":"Peter Lawrey shares some advice as the man with the most answers for Java on Stack Overflow.","publish_date":"2016-04-15 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/stack-overflow-questions-java.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"92a9d81d8ac3b8ba0d2aee894548309a","publish_timestamp":1460592000,"title":"The best apps for productivity","blogName":"Zalando","image":"","categories":["Zalando","Mobile","Mobile First"],"description":"Get your s#!t done with these helpful apps that have Zalando's tick of approval.","publish_date":"2016-04-14 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/best-productivity-apps.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"96fadefc20ee23adfa9bd3ca3171a259","publish_timestamp":1460505600,"title":"Radical Agility Profile: Vijaya Prakash Kandel","blogName":"Zalando","image":"","categories":["Zalando","Mobile","Mobile First","Tech Jobs","iOS"],"description":"The newest addition to our Radical Agility profile series puts the spotlight on iOS.","publish_date":"2016-04-13 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/radical-agility-profile-vijaya-prakash-kandel.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"62e68f61aad1236472b7ea936814346c","publish_timestamp":1460419200,"title":"Ana Peleteiro takes us on a data science tour of Dublin","blogName":"Zalando","image":"","categories":["Zalando","Data Science","Machine Learning","Zalando Dublin"],"description":"Get the rundown from one of our data scientists about her conference journey across Dublin.","publish_date":"2016-04-12 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/dublin-data-science-tour.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"2cd42a11b1603543ff2f0d11d616e3fa","publish_timestamp":1460332800,"title":"Continuous Delivery pipelines of ghe-backups with Lizzy","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/c6f3350f32932f67f7adefb6ed7652b2dc877664_ci-pipeline-1.png?auto=compress,format","categories":["Zalando","AWS","Continuous Delivery","GitHub"],"description":"Lothar Schulz is back to focus on the Continuous Delivery of ghe-backup instances on AWS.","publish_date":"2016-04-11 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/ci-pipelines-with-lizzy.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a73a4385682732b323030cb692f07874","publish_timestamp":1460073600,"title":"We’re finalists for the 2016 SAP HANA Innovation Award!","blogName":"Zalando","image":"","categories":["Zalando","Innovation"],"description":"Cast your vote now to ensure Zalando remains a world class innovator.","publish_date":"2016-04-08 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/sap-innovation-award-finalist.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3a47d07a8dc25e8d8b3f0dcb713df9ae","publish_timestamp":1459987200,"title":"EasyDI – Who wants some cake?","blogName":"Zalando","image":"","categories":["Zalando","Dependency Injection","Scala"],"description":"Eric Torreborre presents another approach for DI that is both simple and flexible.","publish_date":"2016-04-07 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/easy-di-library.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"178e2c7b3d7c1f3d6261128ba9c46c89","publish_timestamp":1459900800,"title":"Radical Agility – One Year On","blogName":"Zalando","image":"","categories":["Zalando"],"description":"One year on, what does Radical Agility mean to the team at Zalando Tech?","publish_date":"2016-04-06 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/radical-agility--one-year-on.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a5b6f7b0cdc8d411e5a38562732c378a","publish_timestamp":1459814400,"title":"Our multi AWS account GitHub Enterprise backup","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/4bf04b8d77cb5fee76f8a06551b7a34665c72e75_slide1.png?auto=compress,format","categories":["Zalando","AWS","GitHub"],"description":"A look at our GitHub Enterprise backup with a High Availability configuration based on AWS.","publish_date":"2016-04-05 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/multi-aws-github-enterprise-backup.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"9b2d99a9ce4e1cdc496e3a8864ab8a46","publish_timestamp":1459728000,"title":"Job Profile: Delivery Lead – Brand Solutions","blogName":"Zalando","image":"","categories":["Zalando","Tech Jobs"],"description":"The first edition in our Job Profile video series takes a look at Brand Solutions.","publish_date":"2016-04-04 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/job-profile-delivery-lead.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"45b82523153591f5755eb0ae41b0bb7e","publish_timestamp":1459468800,"title":"Joel Spolsky holds the fort at Zalando Tech","blogName":"Zalando","image":"","categories":["Zalando","Stack Overflow","Tech Culture"],"description":"The CEO of Stack Overflow and creator of Trello gets cosy with Zalando Tech in Berlin.","publish_date":"2016-04-01 00:00:00","link":"https://engineering.zalando.com/posts/2016/04/joel-spolsky-at-zalando-tech.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f57725e439b3e5250dcec2d1b771929e","publish_timestamp":1459382400,"title":"Apache Showdown: Flink vs. Spark","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/7fab50904edff18f00a54f3f27866ce8d8a62040_saiki-blog-image-1.png?auto=compress,format","categories":["Zalando","Apache Flink","Apache Spark","Big Data"],"description":"Why we chose Apache Flink for the Saiki Data Integration Platform.","publish_date":"2016-03-31 00:00:00","link":"https://engineering.zalando.com/posts/2016/03/apache-showdown-flink-vs.-spark.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d0d0c1a4e0fd2a88599f4ecbf82c266d","publish_timestamp":1459209600,"title":"Your favourite franchises are having an open source love affair with tech","blogName":"Zalando","image":"","categories":["Zalando","APIs","Open Source","Tech Culture"],"description":"The coupling of pop culture and tech via accessible data is on our radar.","publish_date":"2016-03-29 00:00:00","link":"https://engineering.zalando.com/posts/2016/03/your-favourite-franchises-are-having-an-open-source-love-affair-with-tech.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1c32f9a2518030219fdba5c668f0f62f","publish_timestamp":1458691200,"title":"How far will Apps take the shopping experience?","blogName":"Zalando","image":"","categories":["Zalando","Android","Apple","Mobile","Mobile First","iOS"],"description":"Apps are pushing the envelope when it comes to a richer shopping experience.","publish_date":"2016-03-23 00:00:00","link":"https://engineering.zalando.com/posts/2016/03/how-far-will-apps-take-the-shopping-experience.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"504db8e823ed81230de7acc8d5ebcac7","publish_timestamp":1458086400,"title":"Selenium WebDriver Explained","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/acefbd7a02daec98d94a828012a6af2c3d8964a7_screen-shot-2016-03-16-at-10.22.16.png?auto=compress,format","categories":["Zalando","Java","JavaScript","Selenium"],"description":"Providing the best UI automation support for development teams.","publish_date":"2016-03-16 00:00:00","link":"https://engineering.zalando.com/posts/2016/03/selenium-webdriver-explained.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"8af04c22f327f592014400d83500187b","publish_timestamp":1458000000,"title":"Portfolio advice for UX Interaction Designers","blogName":"Zalando","image":"","categories":["Zalando","Interaction Design","UX"],"description":"Top-line tips for Interaction Designers to convey what they do.","publish_date":"2016-03-15 00:00:00","link":"https://engineering.zalando.com/posts/2016/03/portfolio-advice-for-ux-interaction-designers.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"ec091568a111826e0359ccb6752aecf9","publish_timestamp":1457481600,"title":"Building a self-organized team in a radically agile company.","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/122cbd9ee4ed000490e76b1f75321417976656d1_blogpost.png?auto=compress,format","categories":["Zalando","Management"],"description":"Insights from the Business Excellence team at Zalando Technology.","publish_date":"2016-03-09 00:00:00","link":"https://engineering.zalando.com/posts/2016/03/building-a-self-organized-team-in-a-radically-agile-company..html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5c6cd31e07d3d10f5e1f5d4b59f5930c","publish_timestamp":1456185600,"title":"Streaming Huge Databases Using Logical Decoding","blogName":"Zalando","image":"","categories":["Zalando","Docker","FOSDEM","PostgreSQL"],"description":"Practical aspects of extracting consistent data snapshots from a PostgreSQL database.","publish_date":"2016-02-23 00:00:00","link":"https://engineering.zalando.com/posts/2016/02/streaming-huge-databases-using-logical-decoding.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"9feaed1d88e2d6d867a4be78bd353680","publish_timestamp":1456185600,"title":"Student CVs for UX careers: Tips & tricks","blogName":"Zalando","image":"","categories":["Zalando","Design","UX"],"description":"Here we’d like to share some insights from our experience reviewing myriad UX applications.","publish_date":"2016-02-23 00:00:00","link":"https://engineering.zalando.com/posts/2016/02/student-cvs-for-ux-careers-tips--tricks.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e0ba8866b92dd1f0d9a7e76c058362a1","publish_timestamp":1455926400,"title":"Integrating Amazon DynamoDB into your development process","blogName":"Zalando","image":"","categories":["Zalando","AWS","Gradle"],"description":"How to start development with DynamoDB, Gradle and Spring.","publish_date":"2016-02-20 00:00:00","link":"https://engineering.zalando.com/posts/2016/02/integrating-amazon-dynamodb-into-your-development-process.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"4c55b512fa0755fe8626919d23c6a0e0","publish_timestamp":1455235200,"title":"How Agile Coaches Scale Continuous Improvement","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/8f4c26a33350e87c8222ae390c08091ef9802ff0_handful.jpg?auto=compress,format","categories":["Zalando","Coaching","Innovation","Retrospective"],"description":"Zalando’s Agile coaches deep dive into how they scale continuous improvement.","publish_date":"2016-02-12 00:00:00","link":"https://engineering.zalando.com/posts/2016/02/how-agile-coaches-scale-continuous-improvement.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d3d78950b7b1dba415dfaf7ac82c7db2","publish_timestamp":1455062400,"title":"From Monolith to Microservices (Video)","blogName":"Zalando","image":"","categories":["Zalando","AWS","STUPS"],"description":"Managing our growing Microservices Ecosystem.","publish_date":"2016-02-10 00:00:00","link":"https://engineering.zalando.com/posts/2016/02/from-monolith-to-microservices-video.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"15a81905670f63d72c1310570b76b4e5","publish_timestamp":1454457600,"title":"Zalando’s Patroni: a Template for High Availability PostgreSQL","blogName":"Zalando","image":"","categories":["Zalando","Open Source","Patroni","PostgreSQL"],"description":"A customized, high-availability PostgreSQL solution using Python and a distributed configuration store.","publish_date":"2016-02-03 00:00:00","link":"https://engineering.zalando.com/posts/2016/02/zalandos-patroni-a-template-for-high-availability-postgresql.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"6417078b23d181189aef58fade5ece3a","publish_timestamp":1453939200,"title":"Trust Instead of Control","blogName":"Zalando","image":"","categories":["Zalando","Tech Culture"],"description":"Making Security Awareness the Fun Factor in a Tech Organisation","publish_date":"2016-01-28 00:00:00","link":"https://engineering.zalando.com/posts/2016/01/trust-instead-of-control.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"453aa2004ea644a5d8862f01ecbfa022","publish_timestamp":1453766400,"title":"Oh Appy Day!","blogName":"Zalando","image":"","categories":["Zalando","Android","Mobile","Mobile First","Zalando App","iOS"],"description":"The revamp of our fashion store app is ready to install.","publish_date":"2016-01-26 00:00:00","link":"https://engineering.zalando.com/posts/2016/01/oh-appy-day.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"35d83ab86682f11e6a57baddf5609743","publish_timestamp":1453248000,"title":"Hack Week #4 - the video!","blogName":"Zalando","image":"","categories":["Zalando","Hack Week","Innovation","Tech Culture"],"description":"Highlights of a full week of Hacking at Zalando Tech.","publish_date":"2016-01-20 00:00:00","link":"https://engineering.zalando.com/posts/2016/01/hack-week-4---the-video.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"6eda11bab66af902d02d13b5745d921c","publish_timestamp":1453161600,"title":"Reactive Design Patterns","blogName":"Zalando","image":"","categories":["Zalando","Tech Events","Typesafe"],"description":"A Talk by Typesafe’s Roland Kuhn (Slides)","publish_date":"2016-01-19 00:00:00","link":"https://engineering.zalando.com/posts/2016/01/reactive-design-patterns.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0bff39dfb3451be0ead08bac658efef2","publish_timestamp":1452470400,"title":"Meet Connexion: Our REST Framework for Python","blogName":"Zalando","image":"","categories":["Zalando","APIs","Connexion","Flask","OpenAPI","Python"],"description":"Automagically handle your REST API requests based on Swagger/OpenAPI 2.0 Specification files in YAML.","publish_date":"2016-01-11 00:00:00","link":"https://engineering.zalando.com/posts/2016/01/meet-connexion-our-rest-framework-for-python.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"87e1c5934a49b9e669eb8e4152b2850b","publish_timestamp":1452124800,"title":"Using Elm to Create a Fun Game in Just Five Days","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/805bc167467ec5eacb265685aca9020ff726975e_elm-street-2.png?auto=compress,format","categories":["Zalando","Elm"],"description":"Learn about 404 Elm Street, our open-source browser game made with Elm.","publish_date":"2016-01-07 00:00:00","link":"https://engineering.zalando.com/posts/2016/01/using-elm-to-create-a-fun-game-in-just-five-days.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"60b8180c7989c6e6ed91eb673d13bf48","publish_timestamp":1450742400,"title":"Mobile Trends for 2016","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/ad16de398feb11016884b20ccdb29d32f6dc283a_shutterstock_169338014.jpg?auto=compress,format","categories":["Zalando","Mobile","Mobile First"],"description":"An overview of mobile trends for the coming year, and how Zalando is tackling these","publish_date":"2015-12-22 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/mobile-trends-for-2016.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"99921ec0df12215557e96197c31ba22f","publish_timestamp":1450656000,"title":"Hack Week #4: Let’s Talk About Code, Baby!","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/430c9264720e13ab4728666dc031c39feb109785_pasted-image-0.png?auto=compress,format","categories":["Zalando","Hack Week"],"description":"How to integrate non-devs into our Zalando Tech world.","publish_date":"2015-12-21 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/hack-week-4-lets-talk-about-code-baby.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"57cffc6fad9e95a4ef71a8bf1af0bc6c","publish_timestamp":1450396800,"title":"Hack Week #4: From Dublin to Dortmund","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/7afdc96bec552760219f6ed278fcd86290f13752_image-18-12-15-at-11.00.jpg?auto=compress,format","categories":["Zalando","Hack Week","Zalando Dortmund"],"description":"Hack Week is like a big reunion with your friends.","publish_date":"2015-12-18 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/hack-week-4-from-dublin-to-dortmund.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3ae97120e557cf1974ca5fb2e1891f5c","publish_timestamp":1450396800,"title":"Hack Week #4: Hacking for Social Good","blogName":"Zalando","image":"","categories":["Zalando","Hack Week"],"description":"How Hack Week helps to address social needs.","publish_date":"2015-12-18 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/hack-week-4-hacking-for-social-good.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"259b668a2b77195be2a6a471432b0bb7","publish_timestamp":1450396800,"title":"Hack Week #4: Turn it up to Eleven","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/f81a31684b1442e19e44278b18741a6b61a44b85_rockbandz.jpg?auto=compress,format","categories":["Zalando","Hack Week","Zalando Dortmund"],"description":"Hack Week is not all about projects and hard work. Check out what else goes on during Hack Week here:","publish_date":"2015-12-18 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/hack-week-4-turn-it-up-to-eleven.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"4cafcf503f393c92e10c6e0e91210622","publish_timestamp":1450396800,"title":"Hack Week #4: Zinder","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/efb264b4e2ad61959234fca7b7a16a1e2b1a1fd5_zinder.jpg?auto=compress,format","categories":["Zalando","Hack Week"],"description":"Zalando - Connecting people!","publish_date":"2015-12-18 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/hack-week-4-zinder.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"9be8f3774d8949b5d0f4702e1e78cda3","publish_timestamp":1450310400,"title":"Hack Week #4: #DortmundWillHackThis","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/18fb9bae3f92d77e29dce24ce7c70355e1fd5236_img_3285.jpg?auto=compress,format","categories":["Zalando","Hack Week"],"description":"How Hack Week hit Dortmund.","publish_date":"2015-12-17 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/hack-week-4-dortmundwillhackthis.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"29add817b1836901640aeb71bcada2d5","publish_timestamp":1450310400,"title":"Hack Week #4: Hack Week How-To","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/5596cfdda26674c83ed7e9f06f166b48bf74d3cc_ellenandbastian.jpg?auto=compress,format","categories":["Zalando","Hack Week"],"description":"Tips From Our Organizers","publish_date":"2015-12-17 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/hack-week-4-hack-week-how-to.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"ba2af1b2a91ffffc3c687d07d5405c77","publish_timestamp":1450310400,"title":"Hack Week #4: The Knitting Machine","blogName":"Zalando","image":"","categories":["Zalando","Hack Week"],"description":"How to turn an old knitting machine into a yarn-based printer.","publish_date":"2015-12-17 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/hack-week-4-the-knitting-machine.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a8ad771c7945a7f2093f886e1e922c29","publish_timestamp":1450224000,"title":"Hack Week #4: Building the Best Conference App","blogName":"Zalando","image":"","categories":["Zalando","Hack Week"],"description":"How we are building a high-performance conference app during Hack Week.","publish_date":"2015-12-16 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/hack-week-4-building-the-best-conference-app.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5ebf56bfe4049ed91cbb279ba08367cd","publish_timestamp":1450224000,"title":"Hack Week #4: Onboarding Goes Hack Week!","blogName":"Zalando","image":"","categories":["Zalando","Hack Week"],"description":"How a Hack Week project helps Zalando Tech newbies get on board.","publish_date":"2015-12-16 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/hack-week-4-onboarding-goes-hack-week.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b47a6230cc61c3bfcfbda782c26b1a3d","publish_timestamp":1450137600,"title":"Accelerating Warehouse Operations with Neural Networks","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/622c2a6801c88f95465328aa0e551ed3c187f67a_calvin-seward-post.png?auto=compress,format","categories":["Zalando"],"description":"How we've been using deep neural networks to steer operations at Zalando’s fashion warehouses.","publish_date":"2015-12-15 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/accelerating-warehouse-operations-with-neural-networks.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"9cb392672997a991932a6dc769df67f9","publish_timestamp":1450137600,"title":"Hack Week #4: Awards","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/be0cf91f21426f017368fb25c674591641e3655c_protectors.jpg?auto=compress,format","categories":["Zalando","Hack Week"],"description":"Zalando's Head of Innovation and Enablement explains the Hack Week judging process.","publish_date":"2015-12-15 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/hack-week-4-awards.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1ae957c165675db92bbddaed98e91363","publish_timestamp":1450051200,"title":"Hack Week #4 Begins!","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite%2Fc1b8b24c-e9a9-4a17-abae-c914357676f1_vr2o6.gif?auto=compress,format","categories":["Zalando","Hack Week"],"description":"We will, we will hack you!","publish_date":"2015-12-14 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/hack-week-4-begins.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"06af5ac1376a904bb4264bba4314f9c3","publish_timestamp":1449705600,"title":"One Last Thing Before We Call It a Year: Hack Week #4","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/ad256b2a5b995c279753fd33442d86dcc0230a1d_mollstrtechnology.jpg?auto=compress,format","categories":["Zalando","Augmented Reality","Experimentation","Hack Week","Tech Events"],"description":"Zalando Tech's annual innovation week takes place this December. Bigger and more relevant than ever!","publish_date":"2015-12-10 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/one-last-thing-before-we-call-it-a-year-hack-week-4.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d3029441f801c4d4772e9b74fe36a1d9","publish_timestamp":1449532800,"title":"Video: Reactive RESTful APIs with Akka HTTP and Slick","blogName":"Zalando","image":"","categories":["Zalando","Akka","Open Source","Skipper","Zalando Dortmund"],"description":"Why we're building on top of reactive technologies like Akka HTTP and Slick.","publish_date":"2015-12-08 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/video-reactive-restful-apis-with-akka-http-and-slick.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"c43badd20e3fe664284e6d3bf149346a","publish_timestamp":1449446400,"title":"Building System Packages from Python Modules (with Dependencies Included)","blogName":"Zalando","image":"","categories":["Zalando","Python"],"description":"Learn about the benefits of wrapping Python’s virtualenvs in system packages.","publish_date":"2015-12-07 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/building-system-packages-from-python-modules-with-dependencies-included.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"83835030b4436cb0b73cd1e96b1e8c8e","publish_timestamp":1449014400,"title":"Video: “Scala Microservices at Zalando”","blogName":"Zalando","image":"","categories":["Zalando","Open Source","PlaySwagger","Scala","Typesafe"],"description":"A Zalando delivery lead describes his team's work with Scala.","publish_date":"2015-12-02 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/video-scala-microservices-at-zalando.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"58f3a7a68089188f65d19c6120577bb9","publish_timestamp":1448928000,"title":"Building Our Own Open-Source HTTP Routing Solution","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/b5561084b6f469054a73b340021309cd4ebe30cf_skipper-image.png?auto=compress,format","categories":["Zalando","Golang","Innkeeper","Open Source","Skipper"],"description":"Enabling microservices deployment while decoupling routing from service logic.","publish_date":"2015-12-01 00:00:00","link":"https://engineering.zalando.com/posts/2015/12/building-our-own-open-source-http-routing-solution.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"9bc23b8af629e0bcb4346f4db133e529","publish_timestamp":1448582400,"title":"Read About Zalando UX in Smashing Magazine","blogName":"Zalando","image":"","categories":["Zalando","Smashing Magazine"],"description":"To attract motivated designers and user researchers, keep your eye on the why.","publish_date":"2015-11-27 00:00:00","link":"https://engineering.zalando.com/posts/2015/11/read-about-zalando-ux-in-smashing-magazine.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"4271faaf5835285a3dbf59ae52ec2182","publish_timestamp":1448409600,"title":"Video: Swagger Creator Mentions Zalando Open Source","blogName":"Zalando","image":"","categories":["Zalando","APIs","Flask","Open Source","OpenAPI","Python"],"description":"Tony Tam talks about our Connexion, our open-source Swagger framework.","publish_date":"2015-11-25 00:00:00","link":"https://engineering.zalando.com/posts/2015/11/video-swagger-creator-mentions-zalando-open-source.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5bf4f6948088a67c5d364f4a6702cde9","publish_timestamp":1448323200,"title":"How to Prepare for Your Zalando Tech Interview","blogName":"Zalando","image":"","categories":["Zalando"],"description":"Our VP Engineering shares some advice to help you succeed at your Zalando Tech interview.","publish_date":"2015-11-24 00:00:00","link":"https://engineering.zalando.com/posts/2015/11/how-to-prepare-for-your-zalando-tech-interview.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e38dfa17f615852ef48ba02508f7b9fe","publish_timestamp":1448236800,"title":"How I Created My Own Ecommerce App Without Leaving Zalando","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/0ba6283c9927526e8867c09e8498eae61025411e_meteor-berlin-meetup.jpg?auto=compress,format","categories":["Zalando","Mobile First"],"description":"Want to work at Zalando, but also start your own company? It can be done!","publish_date":"2015-11-23 00:00:00","link":"https://engineering.zalando.com/posts/2015/11/how-i-created-my-own-ecommerce-app-without-leaving-zalando.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0bdc3ac8ac304d8c04904a75f7559973","publish_timestamp":1447977600,"title":"How Zalando's Using Clojure+Spark (Slides)","blogName":"Zalando","image":"","categories":["Zalando","Apache Spark","Clojure","Fashion Insights Centre","Zalando Dublin"],"description":"Our talk from this year's Clojure/Conj, which we sponsored.","publish_date":"2015-11-20 00:00:00","link":"https://engineering.zalando.com/posts/2015/11/how-zalandos-using-clojurespark-slides.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1fd9bdf372de10290f4b3e1802ed2895","publish_timestamp":1447718400,"title":"Why \"Open Source First\"","blogName":"Zalando","image":"","categories":["Zalando","Golang","Open Source"],"description":"The sooner you think about open-sourcing, the better.","publish_date":"2015-11-17 00:00:00","link":"https://engineering.zalando.com/posts/2015/11/why-open-source-first.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"723c2778df56acc8640f562457dfc919","publish_timestamp":1447632000,"title":"Video: Radical Agility with Autonomous Teams and Microservices","blogName":"Zalando","image":"","categories":["Zalando","Management","STUPS"],"description":"Talking Radical Agility and more at the code.talks conference.","publish_date":"2015-11-16 00:00:00","link":"https://engineering.zalando.com/posts/2015/11/video-radical-agility-with-autonomous-teams-and-microservices.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3dc8ada6b59ebc626016d3e093d1f1d5","publish_timestamp":1447372800,"title":"Achieving Correct Bloat Estimates of JSON Data in PostgreSQL","blogName":"Zalando","image":"","categories":["Zalando","PostgreSQL"],"description":"How we improved PostgreSQL's perception of space occupied by JSON data.","publish_date":"2015-11-13 00:00:00","link":"https://engineering.zalando.com/posts/2015/11/achieving-correct-bloat-estimates-of-json-data-in-postgresql.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"243f138acdcafa1fc0846947bc5f6ff9","publish_timestamp":1447200000,"title":"Doing Data Science for Social Good","blogName":"Zalando","image":"","categories":["Zalando","Data Science"],"description":"A weekend DataDive inside Zalando Tech!","publish_date":"2015-11-11 00:00:00","link":"https://engineering.zalando.com/posts/2015/11/doing-data-science-for-social-good.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"12c8fbd02a1bb3dac8d548c9df64fea6","publish_timestamp":1447113600,"title":"Zalando Tech's New Open Source Principles","blogName":"Zalando","image":"","categories":["Zalando","Open Source"],"description":"Zalando Tech's Open Source Guild outlines a vision for promoting open source development.","publish_date":"2015-11-10 00:00:00","link":"https://engineering.zalando.com/posts/2015/11/zalando-techs-new-open-source-principles.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b699dc23224f4941f85570557280ff12","publish_timestamp":1447027200,"title":"Video: \"A Tale of Automation and Legacy\"","blogName":"Zalando","image":"","categories":["Zalando","STUPS"],"description":"Learn how we're tackling Identity and Access Management in the cloud.","publish_date":"2015-11-09 00:00:00","link":"https://engineering.zalando.com/posts/2015/11/video-a-tale-of-automation-and-legacy.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5300dec8ac4854fbc8df79a4e36586c7","publish_timestamp":1446681600,"title":"Watch: \"How to Auto-Scale Your API\" (Video)","blogName":"Zalando","image":"","categories":["Zalando","APIs"],"description":"Learn about our API strategy from two Zalando software engineers.","publish_date":"2015-11-05 00:00:00","link":"https://engineering.zalando.com/posts/2015/11/watch-how-to-auto-scale-your-api-video.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1baada74a9c03099309323eeff2bb0b4","publish_timestamp":1446595200,"title":"Zalando Tech Screens \"Big Dream\" on Nov. 10","blogName":"Zalando","image":"","categories":["Zalando"],"description":"Zalando screens this women in tech documentary for free in Berlin.","publish_date":"2015-11-04 00:00:00","link":"https://engineering.zalando.com/posts/2015/11/zalando-tech-screens-big-dream-on-nov.-10.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"344caaccb42688a2d9b4c6a98a2abb13","publish_timestamp":1446422400,"title":"How to Web Summit","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/b2e131100e977f85065f1046b061c4c0c351ca5b_screen-shot-2015-11-02-at-19.21.46.png?auto=compress,format","categories":["Zalando","Tech Events","Zalando Dublin"],"description":"A native Dubliner and Web Summit alumni shows us the way.","publish_date":"2015-11-02 00:00:00","link":"https://engineering.zalando.com/posts/2015/11/how-to-web-summit.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"ea921dbd7212dd1c19db640616f1baac","publish_timestamp":1445990400,"title":"Attention Tech Entrepreneurs!","blogName":"Zalando","image":"","categories":["Zalando","Tech Entrepreneurs","Tech Startups","Zalando Helsinki"],"description":"Zalando-Helsinki launches a new residency for startups that connect people to fashion.","publish_date":"2015-10-28 00:00:00","link":"https://engineering.zalando.com/posts/2015/10/attention-tech-entrepreneurs.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d77d9c294c566049b07f20c8b401e26d","publish_timestamp":1444953600,"title":"From Jimmy to Microservices: Rebuilding Zalando’s Fashion Store","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/f55c07d732742810c966c319ded205dc58e635fc_the-spearheads.jpg?auto=compress,format","categories":["Zalando","APIs","Monolith","Open Source","Skipper"],"description":"How Zalando's Spearheads team is facilitating our monolith-to-microservices evolution.","publish_date":"2015-10-16 00:00:00","link":"https://engineering.zalando.com/posts/2015/10/from-jimmy-to-microservices-rebuilding-zalandos-fashion-store.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"052d91f662bbb84ba5e4e043d58f3ca4","publish_timestamp":1444867200,"title":"\"Choosing the Right Components\": Zalando at HelsinkiJS","blogName":"Zalando","image":"","categories":["Zalando"],"description":"Zalando Senior Frontend Engineer Dmitriy Kubyshkin helps you to avoid picking the wrong ones.","publish_date":"2015-10-15 00:00:00","link":"https://engineering.zalando.com/posts/2015/10/choosing-the-right-components-zalando-at-helsinkijs.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b14b4b15b96220f0e5a4f8a79ae6ad56","publish_timestamp":1444867200,"title":"How to Use Parameter Names in SQL Functions","blogName":"Zalando","image":"","categories":["Zalando","PostgreSQL"],"description":"Use a 'new' PostgreSQL 9.2 feature to make your life better!","publish_date":"2015-10-15 00:00:00","link":"https://engineering.zalando.com/posts/2015/10/how-to-use-parameter-names-in-sql-functions.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"c39e73f5944431e556cef707a8b6b304","publish_timestamp":1444694400,"title":"How Zalando’s App Makes Instagram Images Shoppable","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/5346fe0b9f3b8e84840637015cac33b1ffa45de9_screen-shot-2015-10-13-at-12.17.14.png?auto=compress,format","categories":["Zalando","Mobile","Mobile First","iOS"],"description":"Delivering inspiration through user-generated content.","publish_date":"2015-10-13 00:00:00","link":"https://engineering.zalando.com/posts/2015/10/how-zalandos-app-makes-instagram-images-shoppable.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1839dbcde7b96a5d5abd6967fa889e63","publish_timestamp":1444348800,"title":"Migrating from Legacy IAM to ForgeRock: Lessons (slides)","blogName":"Zalando","image":"","categories":["Zalando","AWS"],"description":"Our transition to a cloud-based infrastructure that still supports communication with legacy systems.","publish_date":"2015-10-09 00:00:00","link":"https://engineering.zalando.com/posts/2015/10/migrating-from-legacy-iam-to-forgerock-lessons-slides.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"8fe531575f52b4584e81f1456e457cc8","publish_timestamp":1444348800,"title":"My Droidcon Greece Experience","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/27b0ebccc4840f94fb0f1aa05708546033d84c03_arch_galerius2-2.jpg?auto=compress,format","categories":["Zalando","Android","Droidcon","Java","RxJava"],"description":"Eating, touring Thessaloniki, and presenting a talk about our Android app fails.","publish_date":"2015-10-09 00:00:00","link":"https://engineering.zalando.com/posts/2015/10/my-droidcon-greece-experience.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"451bc0a251fc814e50e97ecfa393ddd4","publish_timestamp":1444003200,"title":"What We Liked and Learned at JSConf.eu","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/7d7f38603895ba1ee27d5c0dddb9a6c3ed7aa6a1_microsoft-edge.jpg?auto=compress,format","categories":["Zalando","ES6","JSConf","JSConf EU","JavaScript","Smashing Magazine"],"description":"Microsoft Edge, Mozilla Flame, WYSISWYM, JSCodeShift, and more.","publish_date":"2015-10-05 00:00:00","link":"https://engineering.zalando.com/posts/2015/10/what-we-liked-and-learned-at-jsconf.eu.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e99eb098dc5857927f4264043273609d","publish_timestamp":1443657600,"title":"Building an OpenVPN Cluster, Zalando-Style","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/dc7dba759c4ce8f8503cf97be84c2e15a27ed00b_building-a-vpn-cluster-zalando.jpg?auto=compress,format","categories":["Zalando"],"description":"A Zalando DevOps Engineer describes how we did it.","publish_date":"2015-10-01 00:00:00","link":"https://engineering.zalando.com/posts/2015/10/building-an-openvpn-cluster-zalando-style.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5ed43b6347091963c23e578665cdd645","publish_timestamp":1443052800,"title":"My First Weeks as a Zalando Tech Engineer in Dortmund","blogName":"Zalando","image":"","categories":["Zalando","Java","OpenAPI","STUPS","Zalando Dortmund"],"description":"Zalando-Dortmund newbie Malte Pickhan describes getting acquainted with his new work digs.","publish_date":"2015-09-24 00:00:00","link":"https://engineering.zalando.com/posts/2015/09/my-first-weeks-as-a-zalando-tech-engineer-in-dortmund.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"64675642a969808a2b0ca0b6e7a164be","publish_timestamp":1442966400,"title":"Order Processing at Scale with Camunda (Slides)","blogName":"Zalando","image":"","categories":["Zalando","Cassandra"],"description":"Learn more about how Zalando's using the Camunda engine.","publish_date":"2015-09-23 00:00:00","link":"https://engineering.zalando.com/posts/2015/09/order-processing-at-scale-with-camunda-slides.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"2dd3b58329a8abe8b73227552a00ed38","publish_timestamp":1442966400,"title":"Why Zalando Is Celebrating “Mobile First Day”","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/f5759839cd996b4e22c471b58032f34f6312e0a9_screen-shot-2015-09-23-at-10.03.27.png?auto=compress,format","categories":["Zalando","Android","Mobile First","Mobile Responsive Design","Tech Events","iOS"],"description":"The many steps we’re taking to become a #MobileFirst company.","publish_date":"2015-09-23 00:00:00","link":"https://engineering.zalando.com/posts/2015/09/why-zalando-is-celebrating-mobile-first-day.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"04a44cf1e9f5b6370293e195e199524c","publish_timestamp":1442793600,"title":"Data Integration in a World of Microservices","blogName":"Zalando","image":"","categories":["Zalando","Apache Kafka","PostgreSQL"],"description":"Read about Saiki: our open-source, cloud-based, microservices-friendly data integration infrastructure.","publish_date":"2015-09-21 00:00:00","link":"https://engineering.zalando.com/posts/2015/09/data-integration-in-a-world-of-microservices.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"0fe53002348dc7f9907c4bd54700dd57","publish_timestamp":1442534400,"title":"Working at Zalando Dublin","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/5c21337b32c61427873c85b8ed5c0d6eb3b4abc8_screen-shot-2015-09-18-at-18.17.38.png?auto=compress,format","categories":["Zalando","Big Data","Data Science","Silicon Docks","Tech Jobs","Zalando Dublin"],"description":"An inside look at our Fashion Insights Centre.","publish_date":"2015-09-18 00:00:00","link":"https://engineering.zalando.com/posts/2015/09/working-at-zalando-dublin.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"de980d656690c87005313217f7f2fc00","publish_timestamp":1442188800,"title":"Watch the Fireside Chats from Our Helsinki Office Opening","blogName":"Zalando","image":"","categories":["Zalando","Mobile","Mobile First","Tech Jobs","Zalando Helsinki"],"description":"Why did Zalando go north? Watch the videos to find out.","publish_date":"2015-09-14 00:00:00","link":"https://engineering.zalando.com/posts/2015/09/watch-the-fireside-chats-from-our-helsinki-office-opening.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"da90b2c3596d6f871a16ff1a808396cb","publish_timestamp":1441929600,"title":"Delivering Radical Agility to Dublin","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/77c55b8263a9f7604e6504d0626b76db5fdfe916_graham-osullivan-dublin-zalando-team.jpg?auto=compress,format","categories":["Zalando","Fashion Insights Centre","Zalando Dublin"],"description":"A Zalando Delivery Lead and his team make Radical Agility happen in Dublin.","publish_date":"2015-09-11 00:00:00","link":"https://engineering.zalando.com/posts/2015/09/delivering-radical-agility-to-dublin.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f47b10594bf9495ea14b90f9950a7884","publish_timestamp":1441843200,"title":"ZMON: Zalando's Open Source Monitoring Tool (Slides)","blogName":"Zalando","image":"","categories":["Zalando","Meetup","Open Source","Zalando Dublin"],"description":"The slides from our recent talk at DevOps Dublin.","publish_date":"2015-09-10 00:00:00","link":"https://engineering.zalando.com/posts/2015/09/zmon-zalandos-open-source-monitoring-tool-slides.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"375dd01145955be91ba4ccf55ec5e232","publish_timestamp":1441756800,"title":"Meet Zalando Tech at Career Zoo","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/d8130b6b862cb28bfee9f562065af5dea32e734d_screen-shot-2015-09-09-at-01.19.48.png?auto=compress,format","categories":["Zalando","Big Data","Fashion Insights Centre","Machine Learning","Recruiting","Tech Jobs","Zalando Dublin"],"description":"Zalando is a main sponsor of this career and networking event for Dublin technologists.","publish_date":"2015-09-09 00:00:00","link":"https://engineering.zalando.com/posts/2015/09/meet-zalando-tech-at-career-zoo.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f11e463293f18c9cb2e7fbb18df73f48","publish_timestamp":1441756800,"title":"Zalando Goes to GOTO","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/4ee185d2e3a0271674ba2611ffc835c28d01e0b6_zalando-at-goto.png?auto=compress,format","categories":["Zalando","GOTO Conference"],"description":"Zalando technologists will speak at GOTO London and Copenhagen.","publish_date":"2015-09-09 00:00:00","link":"https://engineering.zalando.com/posts/2015/09/zalando-goes-to-goto.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"ca76f1acebfd3daff74356460f10980e","publish_timestamp":1441324800,"title":"Zalando Opens New Playground for Tech Innovation","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/544c0e5ac1247b50506aaac232d78ab385355ca2_kopie-von-zalando_lab_0261.jpg?auto=compress,format","categories":["Zalando"],"description":"“The Shuttle” launches inside our Tech HQ","publish_date":"2015-09-04 00:00:00","link":"https://engineering.zalando.com/posts/2015/09/zalando-opens-new-playground-for-tech-innovation.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"cbc29b7760c29d2b571f43b2ade564e0","publish_timestamp":1441238400,"title":"On APIs and the Zalando API Guild","blogName":"Zalando","image":"","categories":["Zalando","APIs"],"description":"Building high-quality, long-lasting APIs has never been more important.","publish_date":"2015-09-03 00:00:00","link":"https://engineering.zalando.com/posts/2015/09/on-apis-and-the-zalando-api-guild.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3fe1f2993832020a64cedfc9f5a4dfec","publish_timestamp":1441152000,"title":"A Zalando Tops “Most Read Data Science Articles” List","blogName":"Zalando","image":"","categories":["Zalando","Big Data","Data Science","Machine Learning","Recommender Systems"],"description":"A Zalando Delivery Lead’s post comes in at #1!","publish_date":"2015-09-02 00:00:00","link":"https://engineering.zalando.com/posts/2015/09/a-zalando-tops-most-read-data-science-articles-list.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"fa8b38267a7fb42ad22da09e2ab52f94","publish_timestamp":1440720000,"title":"Hello, Helsinki!","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/b6141726b0dd5a1af39e8a1e1b65ee72f7d8d931_hka_6667.jpg?auto=compress,format","categories":["Zalando","Mobile","Mobile First","Tech Events","Zalando Helsinki"],"description":"Zalando opens another international tech hub —this time, in Scandinavia!","publish_date":"2015-08-28 00:00:00","link":"https://engineering.zalando.com/posts/2015/08/hello-helsinki.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"bc2578414ca17ae4e210d35dffd71179","publish_timestamp":1440115200,"title":"Tech.EU Catches up with Zalando","blogName":"Zalando","image":"","categories":["Zalando","Tech Open Air"],"description":"Zalando Cofounder Robert Gentz chats with Tech.EU about our growth.","publish_date":"2015-08-21 00:00:00","link":"https://engineering.zalando.com/posts/2015/08/tech.eu-catches-up-with-zalando.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"56aa04550ffe1c21a0149342e490482f","publish_timestamp":1440028800,"title":"Designing RESTful APIs: A Zalando Coder Dojo","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/c60677c9f00c971ce71ff053ede6bb524f645037_zalando-api-first-coder-dojo-2.jpg?auto=compress,format","categories":["Zalando","APIs"],"description":"How we helped our technology team to learn API design. Plus: Tips!","publish_date":"2015-08-20 00:00:00","link":"https://engineering.zalando.com/posts/2015/08/designing-restful-apis-a-zalando-coder-dojo.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e3f2e7298cb601fe0d50fed23c99e208","publish_timestamp":1439856000,"title":"August 24: Zalando Hosts Microservices Meetup Berlin","blogName":"Zalando","image":"","categories":["Zalando","Meetup"],"description":"Two Zalandos to co-present on how we manage language diversity.","publish_date":"2015-08-18 00:00:00","link":"https://engineering.zalando.com/posts/2015/08/august-24-zalando-hosts-microservices-meetup-berlin.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e29f5bfc36c2319b63b1dd76b87f78f7","publish_timestamp":1439510400,"title":"PostgreSQL Backups Done Right (Video)","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/bf533f501f397f2597b4bf38c00aa6fd4bd79dd9_img_0230-1.jpg?auto=compress,format","categories":["Zalando","Meetup","PostgreSQL"],"description":"Longtime PostgreSQL contributor Devrim Gündüz speaks at Zalando’s Sky Lounge.","publish_date":"2015-08-14 00:00:00","link":"https://engineering.zalando.com/posts/2015/08/postgresql-backups-done-right-video.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e5d856f48be2ca610ddbb522d0610194","publish_timestamp":1439337600,"title":"How Zalando Helps Brands to Win Online (Video)","blogName":"Zalando","image":"","categories":["Zalando"],"description":"Zalando's VP Brand Solutions presents at the July 2015 Fashtech-Konferenz.","publish_date":"2015-08-12 00:00:00","link":"https://engineering.zalando.com/posts/2015/08/zalandos-vp-brand-solutions-presents-at-the-july-2015-fashtech-konferenz..html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5836b7428cf43b9606cf7aaeee03a806","publish_timestamp":1439251200,"title":"Gearing up for Zalando’s Mario Kart Championship","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/b9e621a544c0de4485ae7f4963a9dfee860b0b96_screen-shot-2015-08-11-at-18.12.34.png?auto=compress,format","categories":["Zalando","Tech Jobs"],"description":"Hot tarmac, screeching tires and banana peels on the speedway!","publish_date":"2015-08-11 00:00:00","link":"https://engineering.zalando.com/posts/2015/08/gearing-up-for-zalandos-mario-kart-championship.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"6831c1c94d736768d4ea7336aa89108a","publish_timestamp":1438732800,"title":"Meet Zalando at the First OpenTechSchool Conference","blogName":"Zalando","image":"","categories":["Zalando","Tech Events","Zalando Dortmund"],"description":"Go back to school with us this August 15-16 in Dortmund, Germany","publish_date":"2015-08-05 00:00:00","link":"https://engineering.zalando.com/posts/2015/08/meet-zalando-at-the-first-opentechschool-conference.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"cf10648ca4304d6cb0c389511b385fd8","publish_timestamp":1438300800,"title":"Mobile Testing Challenges at Zalando + 6Wunderkinder","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/c3d0d592927210f4e48a552cced71fadb5bce86a_mqc_20150723_06.jpg?auto=compress,format","categories":["Zalando","Android","Mobile","Swift","Tech Jobs","iOS"],"description":"The Mobile Quality Crew Meetup","publish_date":"2015-07-31 00:00:00","link":"https://engineering.zalando.com/posts/2015/07/mobile-testing-challenges-at-zalando--6wunderkinder.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"15ed0c1450fc840fe14a7437dab90871","publish_timestamp":1438214400,"title":"Analyzing Extreme Distributions in PostgreSQL","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/714d5139cf5e65e3cfc5ee5ac53261513e580a79_pic1.png?auto=compress,format","categories":["Zalando","PostgreSQL"],"description":"The rare things matter.","publish_date":"2015-07-30 00:00:00","link":"https://engineering.zalando.com/posts/2015/07/analyzing-extreme-distributions-in-postgresql.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"ca552928fa2830b3183166efb5ad3683","publish_timestamp":1438041600,"title":"Zalando's Traveling Prototyping Team","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/29139f2ec440ff149f989d8ec3bae250b20bf48c_dublin2.jpg?auto=compress,format","categories":["Zalando","Prototyping","Silicon Docks","Zalando Dortmund","Zalando Dublin"],"description":"From Dublin to Dortmund, Zalando's prototyping team is on the move—and mapping our future.","publish_date":"2015-07-28 00:00:00","link":"https://engineering.zalando.com/posts/2015/07/zalandos-traveling-prototyping-team.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1f31f4a95e2abad4e725927b45048cba","publish_timestamp":1437696000,"title":"So, You've Heard About \"Radical Agility\"... (Video)","blogName":"Zalando","image":"","categories":["Zalando"],"description":"The day Zalando Tech officially became Radical.","publish_date":"2015-07-24 00:00:00","link":"https://engineering.zalando.com/posts/2015/07/so-youve-heard-about-radical-agility...-video.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b0f8603968ed3311f7a68bf0a15ea767","publish_timestamp":1437609600,"title":"\"Using Git Hooks to Help Your Team Work Autonomously\" (Video)","blogName":"Zalando","image":"","categories":["Zalando","Git","Python"],"description":"Watch the video from Zalando's presentation at EuroPython 2015!","publish_date":"2015-07-23 00:00:00","link":"https://engineering.zalando.com/posts/2015/07/using-git-hooks-to-help-your-team-work-autonomously-video.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"8bf18afb09581629cf5de31843dd90b6","publish_timestamp":1437436800,"title":"Zalando Did Tech Open Air","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/9a93e1b800b5c6002892bdcca268012058035934_img_0058.jpg?auto=compress,format","categories":["Zalando","Design","STUPS","Tech Events","Tech Jobs","Tech Open Air"],"description":"The Recap.","publish_date":"2015-07-21 00:00:00","link":"https://engineering.zalando.com/posts/2015/07/zalando-did-tech-open-air.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"6df7ef40824729b0915a0c9be0d28687","publish_timestamp":1436918400,"title":"\"The Polyglot Platform\": Zalando at PolyConf (Video)","blogName":"Zalando","image":"","categories":["Zalando"],"description":"Zalando presents \"The Polyglot Platform: How Zalando Manages Language Diversity\" at PolyConf 2015!","publish_date":"2015-07-15 00:00:00","link":"https://engineering.zalando.com/posts/2015/07/the-polyglot-platform-zalando-at-polyconf-video.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"da5090aa74dad69cabb6deb3772f5d62","publish_timestamp":1436832000,"title":"The Perils of Modifying PostgreSQL System Catalogs","blogName":"Zalando","image":"","categories":["Zalando","PostgreSQL"],"description":"You shouldn’t modify tables under the pg_catalog schema without first consulting the pgsql-hackers mailing list.","publish_date":"2015-07-14 00:00:00","link":"https://engineering.zalando.com/posts/2015/07/the-perils-of-modifying-postgresql-system-catalogs.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"aeb7174b038648e4349e638a0e2dc250","publish_timestamp":1436400000,"title":"Zalando Does Tech Open Air","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/5761c621e6f9169d58f1bc9db9348ca585a3d515_10580136_635373156559366_6483002187383474509_n.jpg?auto=compress,format","categories":["Zalando","Design","Open Source","Tech Events","Tech Jobs"],"description":"Superpower Revelations at Tech Open Air.","publish_date":"2015-07-09 00:00:00","link":"https://engineering.zalando.com/posts/2015/07/zalando-does-tech-open-air.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"412a2fd1eb86487ea01c81041bdecd50","publish_timestamp":1436227200,"title":"Radical Agility on AWS (Video)","blogName":"Zalando","image":"","categories":["Zalando","AWS","STUPS"],"description":"Zalando's VP Engineering tells the AWS Summit crowd how we're using AWS.","publish_date":"2015-07-07 00:00:00","link":"https://engineering.zalando.com/posts/2015/07/radical-agility-on-aws-video.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"18a2293d8913b79dc88084c2028f7d5d","publish_timestamp":1436227200,"title":"Zalando goes to ReactEurope Paris","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/efc449519bfb586fd4b5bf92b998ca1f5976bdd5_img_20150702_164242-1.jpg?auto=compress,format","categories":["Zalando","React"],"description":"3 Zalandos withstand 35° with no AC for the love of React","publish_date":"2015-07-07 00:00:00","link":"https://engineering.zalando.com/posts/2015/07/zalando-goes-to-reacteurope-paris.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"39a606998a04bb8eebb8b90d0a1cc0ec","publish_timestamp":1435881600,"title":"Watch: \"From Java to Scala in Less Than Three Months\"","blogName":"Zalando","image":"","categories":["Zalando","Scala"],"description":"Where can you find video from Zalando's Scala Days 2015 presentation? We know the answer.","publish_date":"2015-07-03 00:00:00","link":"https://engineering.zalando.com/posts/2015/07/watch-from-java-to-scala-in-less-than-three-months.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"4e0d7e7a3a0341b782db31173803daed","publish_timestamp":1435708800,"title":"Explore Zalando's \"Tour of Mastery\" With Our Alien Guides!","blogName":"Zalando","image":"","categories":["Zalando","Tour of Mastery"],"description":"Space creatures and astronauts go on Zalando's Tour of Mastery. Why not you, too?","publish_date":"2015-07-01 00:00:00","link":"https://engineering.zalando.com/posts/2015/07/explore-zalandos-tour-of-mastery-with-our-alien-guides.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"f3efc76c1d8710468b0a7d274cc480a1","publish_timestamp":1435622400,"title":"Auto-Scaling Your API: Tips from Zalando (Slides)","blogName":"Zalando","image":"","categories":["Zalando","Java"],"description":"Learn more about Zalando's \"API First\" approach — and find out who Jimmy is.","publish_date":"2015-06-30 00:00:00","link":"https://engineering.zalando.com/posts/2015/06/auto-scaling-your-api-tips-from-zalando-slides.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"62fdcb6f74d1bf2981216d85ce3311e5","publish_timestamp":1435276800,"title":"Docker: Powering Radical Agility (slides)","blogName":"Zalando","image":"","categories":["Zalando","Docker","Meetup"],"description":"Check out the slides from our June 2015 Docker Berlin presentation!","publish_date":"2015-06-26 00:00:00","link":"https://engineering.zalando.com/posts/2015/06/docker-powering-radical-agility-slides.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"04df53ed1cd8e6492aa51eb71028eb58","publish_timestamp":1435017600,"title":"RSVP for Recommenders.ie’s July Meetup at Zalando-Dublin","blogName":"Zalando","image":"","categories":["Zalando","Silicon Docks","Zalando Dublin"],"description":"Zalando's Dublin office hosts our second meetup.","publish_date":"2015-06-23 00:00:00","link":"https://engineering.zalando.com/posts/2015/06/rsvp-for-recommenders.ies-july-meetup-at-zalando-dublin.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"7a21516fefcb05ce0ddc9e9b7f74bbf8","publish_timestamp":1435017600,"title":"What We Learned While Making Zalando's Apple Watch App","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/8296e1bb7a650155d026567bbc023160905e93d1_apple-watch.jpg?auto=compress,format","categories":["Zalando","Apple","iOS"],"description":"Zalando Mobile's tips and shortcuts to help you develop with Apple's new WatchKit framework.","publish_date":"2015-06-23 00:00:00","link":"https://engineering.zalando.com/posts/2015/06/what-we-learned-while-making-zalandos-apple-watch-app.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"432b7aceb1b7480c02079ad726b6058f","publish_timestamp":1434499200,"title":"Zalando Goes to GOTO Amsterdam 2015","blogName":"Zalando","image":"","categories":["Zalando","STUPS"],"description":"Zalando is a proud silver sponsor and presenter at GOTO Amsterdam 2015!","publish_date":"2015-06-17 00:00:00","link":"https://engineering.zalando.com/posts/2015/06/zalando-goes-to-goto-amsterdam-2015.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"1fb387fc61335d2cb60dc22a3cca15b0","publish_timestamp":1434326400,"title":"Watch \"Fashion Is Hard. PostgreSQL Is Easy\"","blogName":"Zalando","image":"","categories":["Zalando","PostgreSQL"],"description":"Watch the video of Zalando's Valentin Gogichashvili keynoting this year's PGConf US.","publish_date":"2015-06-15 00:00:00","link":"https://engineering.zalando.com/posts/2015/06/watch-fashion-is-hard-postgresql-is-easy.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"fd2989cdf6cfbff52129a109c3e9f639","publish_timestamp":1434326400,"title":"Video: \"Rewiring Zalando's Infrastructure Outside Datacenters\"","blogName":"Zalando","image":"","categories":["Zalando"],"description":"Learn how Zalando has rethought our identity and access management systems.","publish_date":"2015-06-15 00:00:00","link":"https://engineering.zalando.com/posts/2015/06/watch-now-rewiring-zalandos-infrastructure-outside-datacenters.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"a5518ffc99f3783c9046c0b835ad3599","publish_timestamp":1434326400,"title":"Zalando Hosts PostgreSQL Meetup Group Berlin #2","blogName":"Zalando","image":"","categories":["Zalando","PostgreSQL"],"description":"Two great PostgreSQL speakers present at Zalando on June 18!","publish_date":"2015-06-15 00:00:00","link":"https://engineering.zalando.com/posts/2015/06/zalando-hosts-postgresql-meetup-group-berlin-2.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e9b4097baf9ad25954c66296edb3039b","publish_timestamp":1433980800,"title":"Zalando-Dublin Presents \"Cassandra in Focus\"","blogName":"Zalando","image":"","categories":["Zalando","Cassandra","Zalando Dublin"],"description":"Our first meetup in Dublin focuses on Cassandra, one of our favorite big data technologies.","publish_date":"2015-06-11 00:00:00","link":"https://engineering.zalando.com/posts/2015/06/zalando-dublin-hosts-cassandra-in-focus-meetup.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"82b17b56d779a276b28d9422acc71dd4","publish_timestamp":1433462400,"title":"Speeding up Xcode Builds","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/f8a86b0fdb0f5c7967fa90f7473d59874a87817c_bespalovxcodebuildschart.png?auto=compress,format","categories":["Zalando","Swift","Xcode"],"description":"How Zalando Mobile achieved 80% faster Xcode builds and 30% faster compilation speeds.","publish_date":"2015-06-05 00:00:00","link":"https://engineering.zalando.com/posts/2015/06/speeding-up-xcode-builds.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"600512c9e6db5f6c3fa26b909c935418","publish_timestamp":1433376000,"title":"RSVP for New Relic’s Free Workshop with Zalando Mobile","blogName":"Zalando","image":"","categories":["Zalando"],"description":"Sign up ASAP for New Relic's free workshop with Zalando's Jan Gorman in Berlin!","publish_date":"2015-06-04 00:00:00","link":"https://engineering.zalando.com/posts/2015/06/rsvp-for-new-relics-free-workshop-with-zalando-mobile.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"7a88a2365532cae212386f9052a81f4d","publish_timestamp":1433289600,"title":"Radical Agility with Autonomous Teams and Microservices in the Cloud","blogName":"Zalando","image":"","categories":["Zalando","Management"],"description":"Watch the video and view the slides from Zalando's talk at DevOpsCon 2015.","publish_date":"2015-06-03 00:00:00","link":"https://engineering.zalando.com/posts/2015/06/radical-agility-with-autonomous-teams-and-microservices-in-the-cloud.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"5a8e761b55b9f48203f907a7796bb79e","publish_timestamp":1429488000,"title":"How to Fix What You Can't Kill: Undead PostgreSQL queries","blogName":"Zalando","image":"","categories":["Zalando","PostgreSQL"],"description":"The standard way to kill a TCP connection inPostgreSQLis to usepg_terminate_backend($PID). However, in some situations this function does not work. To help you avoid negative outcomes when closing such connections, here is a simple hack.","publish_date":"2015-04-20 00:00:00","link":"https://engineering.zalando.com/posts/2015/04/how-to-fix-what-you-cant-kill-undead-postgresql-queries.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"3b84ec7c94f824875328d8b8495e248b","publish_timestamp":1425340800,"title":"We Launched It! The Zalando Space Shoe (Video)","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/08089bf09a4e42191a92c03c64126c0689562832_balloontripkm.jpg?auto=compress,format","categories":["Zalando","Hack Week"],"description":"Zalando launched a lone Zign shoe to space on the 21st of February, 2015.","publish_date":"2015-03-03 00:00:00","link":"https://engineering.zalando.com/posts/2015/03/we-launched-it-the-zalando-space-shoe-video.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"45435ab2c011d8235876f3f1abc225d7","publish_timestamp":1418947200,"title":"Hack Week: Zalando 3D printing","blogName":"Zalando","image":"","categories":["Zalando","Hack Week"],"description":"Make your 3D printer work.","publish_date":"2014-12-19 00:00:00","link":"https://engineering.zalando.com/posts/2014/12/hack-week-zalando-3d-printing.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"95009b6023a2f0e3ab5953d32973f184","publish_timestamp":1418860800,"title":"Behind the scenes: Zalando Space Launch","blogName":"Zalando","image":"","categories":["Zalando","Hack Week"],"description":"Sending a shoe into space is no easy task. But we did it!","publish_date":"2014-12-18 00:00:00","link":"https://engineering.zalando.com/posts/2014/12/behind-the-scenes-zalando-space-launch.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"58e37bfc7f9eb4247174e4d6db933c47","publish_timestamp":1418774400,"title":"Hack Week: 3D Item View with cardboard like Virtual Reality Kit","blogName":"Zalando","image":"","categories":["Zalando","Hack Week"],"description":"Hack Week: Virtual reality is all the rage right now.","publish_date":"2014-12-17 00:00:00","link":"https://engineering.zalando.com/posts/2014/12/hack-week-3d-item-view-with-cardboard-like-virtual-reality-kit.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"b0f9a77d7c89260b4b10b3d4d7f7d89b","publish_timestamp":1418774400,"title":"Hack Week: Ask Zalanda","blogName":"Zalando","image":"","categories":["Zalando","Hack Week"],"description":"Using Artificial Intelligence to create Zalanda.","publish_date":"2014-12-17 00:00:00","link":"https://engineering.zalando.com/posts/2014/12/hack-week-ask-zalanda.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"7f0c9194d43c4566c202a6406148224d","publish_timestamp":1418688000,"title":"Hack Week: A Short Introduction","blogName":"Zalando","image":"","categories":["Zalando","Hack Week"],"description":"Hack Week 3 starts today! Interested in what will happen the next days?","publish_date":"2014-12-16 00:00:00","link":"https://engineering.zalando.com/posts/2014/12/hack-week-a-short-introduction.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"d38c8c8fd4dca74950bdc83dec5aa0ab","publish_timestamp":1418342400,"title":"Hack Week: Fashion Meets Tech - Smart Wearables","blogName":"Zalando","image":"","categories":["Zalando","Hack Week"],"description":"Fashion designers meet engineers!","publish_date":"2014-12-12 00:00:00","link":"https://engineering.zalando.com/posts/2014/12/hack-week-fashion-meets-tech---smart-wearables.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"13b95cf860b092606110ebe2896cb44a","publish_timestamp":1417824000,"title":"HACK WEEK: Reverse Engineering with Zalando parcels","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/d7192887c2b79909c9a17cc7de9b0ce9b51aeb1f_14204312460_c9c50a0517_b.jpg?auto=compress,format","categories":["Zalando","Hack Week"],"description":"It's Hack Week again and here's all about our cardboard furniture project.","publish_date":"2014-12-06 00:00:00","link":"https://engineering.zalando.com/posts/2014/12/hack-week-reverse-engineering-with-zalando-parcels.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"ef1a0b7494bc04e35c0cac61d5e6c9ef","publish_timestamp":1417824000,"title":"HACK WEEK: The Great Unpacking Experience","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/e9054fa79ba3d6ad053e2e14a8ee86e403328c13_1234.jpg?auto=compress,format","categories":["Zalando","Hack Week"],"description":"How dfo you feel when you receive your favorite shoes from Zalando?","publish_date":"2014-12-06 00:00:00","link":"https://engineering.zalando.com/posts/2014/12/hack-week-the-great-unpacking-experience.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"571beaded6cd10e8a976145676a4ec93","publish_timestamp":1417478400,"title":"Zalando Hack Week - Making Innovation Visible","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/efd16189651297ce04416831ba5e35eb176af49e_screen-shot-2015-05-27-at-17.42.13.png?auto=compress,format","categories":["Zalando","Hack Week"],"description":"One week to brainstorm and execute your own ideas without limits!","publish_date":"2014-12-02 00:00:00","link":"https://engineering.zalando.com/posts/2014/12/zalando-hack-week---making-innovation-visible.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"4652624df6a3d5f34c7e9c35cb8ec6c0","publish_timestamp":1402531200,"title":"HACK WEEK: Design Thinking Applied","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/6772a06d25dc1fe0b6ac87045a4811a03c8d7245_14404953622_6a7c1f74c8_b.jpg?auto=compress,format","categories":["Zalando","Hack Week"],"description":"Learn about Design Thinking principles.","publish_date":"2014-06-12 00:00:00","link":"https://engineering.zalando.com/posts/2014/06/hack-week-design-thinking-applied.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"df29a0b6e7279432b8df5256b03464c5","publish_timestamp":1402444800,"title":"HACK WEEK: Taking the Shopping Experience to the next level","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/d1073b9ca39961afff289b1ba6cc5adcdf9827dc_14211220080_be9a25575e_b.jpg?auto=compress,format","categories":["Zalando","Hack Week"],"description":"Users could try-on Zalando's products with our \"KINECT Virtual Dressing” project.","publish_date":"2014-06-11 00:00:00","link":"https://engineering.zalando.com/posts/2014/06/hack-week-taking-the-shopping-experience-to-the-next-level.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"bc57514d3a91f764f612855a9fe969f5","publish_timestamp":1402358400,"title":"HACK WEEK: Let’s Hack!","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/f7be245b29b121b2f278864b45fc95ee60db5b2c_1.jpg?auto=compress,format","categories":["Zalando","Hack Week"],"description":"Business as usual? Not this week! Let's Hack!","publish_date":"2014-06-10 00:00:00","link":"https://engineering.zalando.com/posts/2014/06/hack-week-lets-hack.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"e45ea49b2c7147b0300f89421eca56c1","publish_timestamp":1396224000,"title":"Writing Python command line scripts","blogName":"Zalando","image":"https://images.prismic.io/zalando-jobsite/a0f6c7dfc468545dd673a9b1c336b8f8b0794e40_screen-shot-2015-05-27-at-18.02.44.png?auto=compress,format","categories":["Zalando","Python"],"description":"Python is great for writing command line scripts.","publish_date":"2014-03-31 00:00:00","link":"https://engineering.zalando.com/posts/2014/03/writing-python-command-line-scripts.html","blog":{"id":"zalando","link":"https://engineering.zalando.com","name":"Zalando","rssFeed":"https://engineering.zalando.com/atom.xml","type":"company"},"blogType":"company"},{"id":"55f2fc6ee5ea43c7cd084ff4dbe068ee","publish_timestamp":1615846044,"title":"Docker Compose: From Local to Amazon ECS","blogName":"Docker","image":"","categories":["Engineering","Products","Amazon ECS","containers","docker","docker compose"],"description":"By using cloud platforms, we can take advantage of different resource configurations and compute capacities. However, deploying containerized applications on cloud platforms is proving to be quite challenging, especially for new users who have no expertise on how to use that platform. As each platform may provide specific APIs, orchestrating the deployment of a containerized [&#8230;]\nThe post Docker Compose: From Local to Amazon ECS appeared first on Docker Blog.\n","publish_date":"2021-03-15 22:07:24","link":"https://www.docker.com/blog/docker-compose-from-local-to-amazon-ecs/","blog":{"id":"docker","link":"https://www.docker.com/blog/category/engineering","name":"Docker","rssFeed":"https://www.docker.com/blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"4060135ebee6f74e4427ba704542be9f","publish_timestamp":1613494800,"title":"How to Deploy GPU-Accelerated Applications on Amazon ECS with Docker Compose","blogName":"Docker","image":"https://www.docker.com/blog/wp-content/uploads/2021/02/shutterstock_1315361570-1110x683.jpg","categories":["Engineering","Compose","ECS","GPU","Machine Learning","python"],"description":"Many applications can take advantage of GPU acceleration, in particular resource-intensive Machine Learning (ML) applications. The development time of such applications may vary based on the hardware of the machine we use for development. Containerization will facilitate development due to reproducibility and will make the setup easily transferable to other machines. Most importantly, a containerized [&#8230;]\nThe post How to Deploy GPU-Accelerated Applications on Amazon ECS with Docker Compose appeared first on Docker Blog.\n","publish_date":"2021-02-16 17:00:00","link":"https://www.docker.com/blog/deploy-gpu-accelerated-applications-on-amazon-ecs-with-docker-compose/","blog":{"id":"docker","link":"https://www.docker.com/blog/category/engineering","name":"Docker","rssFeed":"https://www.docker.com/blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"14736a72a679c87f949a9e1b74ec9184","publish_timestamp":1611842400,"title":"🧪 Open Sourcing the Docker Hub CLI Tool","blogName":"Docker","image":"https://lh5.googleusercontent.com/mGjrYVbt-dw86YVzOcLVaLrdpXH3RXqpyzi614L9v3xaEafsHbKynLtOL5nCccB3N-ws8piB8Qg6FuSaU8-Sk8fa6l5P9fBwTaANhC83EpZPD3C_UiX2hN40cwE8kD3wDjunvx9c","categories":["Engineering","docker hub","open source"],"description":"At Docker, we are committed to making developer’s lives easier, and maintaining and extending our commitment to the Open Source community and open standards for many of our projects. We believe building new capabilities into the Docker Platform in partnership with our developer community and in full transparency leads to much better software. Last December, [&#8230;]\nThe post &#x1f9ea; Open Sourcing the Docker Hub CLI Tool appeared first on Docker Blog.\n","publish_date":"2021-01-28 14:00:00","link":"https://www.docker.com/blog/open-sourcing-the-docker-hub-cli-tool/","blog":{"id":"docker","link":"https://www.docker.com/blog/category/engineering","name":"Docker","rssFeed":"https://www.docker.com/blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"c0a86ba2d25f3d5a4f085c121c4152c4","publish_timestamp":1608141155,"title":"Download and Try the Tech Preview of Docker Desktop for M1","blogName":"Docker","image":"https://lh4.googleusercontent.com/9_z8b72EjrIQlyFpSYM3lIZi7xgmIkMFeWMwtiP5p6XA0hf5JTP49SQGVeRq6YC7XEHeqVFG7UsMxAxGvsa6Cu8LEPxCRHP3wRR_nr0TFtrKqhWgEAhrz8UClk81TuTZ3C2zgihC","categories":["Engineering","Products","developers","docker desktop","Mac"],"description":"Last week, during the Docker Community All Hands, we announced the availability of a developer preview build of Docker Desktop for Macs running on M1 through the Docker Developer Preview Program. We already have more than 1,000 people testing these builds as of today. If you’re interested in joining the program for future releases you [&#8230;]\nThe post Download and Try the Tech Preview of Docker Desktop for M1 appeared first on Docker Blog.\n","publish_date":"2020-12-16 17:52:35","link":"https://www.docker.com/blog/download-and-try-the-tech-preview-of-docker-desktop-for-m1/","blog":{"id":"docker","link":"https://www.docker.com/blog/category/engineering","name":"Docker","rssFeed":"https://www.docker.com/blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"3dbd9f6bf16aa8c29020ffd507950c4c","publish_timestamp":1605816000,"title":"Docker Compose for Amazon ECS Now Available","blogName":"Docker","image":"https://lh4.googleusercontent.com/2y1lm_GabZDJusq2FB6gcFZv8vWJNXLSnMvmyjjpNCM0fH7CoyrRPIplbUQ7r8NxCroFq1Z-l27mrYRlCLjA9nvIpPoJVfWyhLPR7gYqTxShLraOELPsOQETLiTtKLaylM6-JqjY","categories":["Engineering","Products","aws","containers","docker","ECS"],"description":"Docker is pleased to announce that as of today the integration with Docker Compose and Amazon ECS has reached V1 and is now GA! &#x1f389; We started this work way back at the beginning of the year with our first step &#8211; moving the Compose specification into a community run project. Then in July we [&#8230;]\nThe post Docker Compose for Amazon ECS Now Available appeared first on Docker Blog.\n","publish_date":"2020-11-19 20:00:00","link":"https://www.docker.com/blog/docker-compose-for-amazon-ecs-now-available/","blog":{"id":"docker","link":"https://www.docker.com/blog/category/engineering","name":"Docker","rssFeed":"https://www.docker.com/blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"013f2df6f572ed0fa1bde50825f5d87c","publish_timestamp":1604088369,"title":"What you need to know about upcoming Docker Hub rate limiting","blogName":"Docker","image":"","categories":["Community","Company","Engineering","Products","docker hub","docker subscription","image retention","subscription"],"description":"On August 24th, we announced the implementation of rate limiting for Docker container pulls for some users. Beginning November 2, Docker will begin phasing in limits of Docker container pull requests for anonymous and free authenticated users. The limits will be fully enforced Monday, November 2, from 9-10 am PT, and then reduced to 5,000 [&#8230;]\nThe post What you need to know about upcoming Docker Hub rate limiting appeared first on Docker Blog.\n","publish_date":"2020-10-30 20:06:09","link":"https://www.docker.com/blog/what-you-need-to-know-about-upcoming-docker-hub-rate-limiting/","blog":{"id":"docker","link":"https://www.docker.com/blog/category/engineering","name":"Docker","rssFeed":"https://www.docker.com/blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"ce78997fe1292bfb30b82b9292d87824","publish_timestamp":1604082294,"title":"Checking Your Current Docker Pull Rate Limits and Status","blogName":"Docker","image":"","categories":["Community","Company","Engineering","Products","docker hub","docker subscription","image retention","subscription"],"description":"Continuing with our move towards consumption-based limits, customers will see the new rate limits for Docker pulls of container images at each tier of Docker subscriptions starting from November 2, 2020.&#160; Anonymous free users will be limited to 100 pulls per six hours, and authenticated free users will be limited to 200 pulls per six [&#8230;]\nThe post Checking Your Current Docker Pull Rate Limits and Status appeared first on Docker Blog.\n","publish_date":"2020-10-30 18:24:54","link":"https://www.docker.com/blog/checking-your-current-docker-pull-rate-limits-and-status/","blog":{"id":"docker","link":"https://www.docker.com/blog/category/engineering","name":"Docker","rssFeed":"https://www.docker.com/blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"9cfd2db32c9c114dbd1b05156ca85d58","publish_timestamp":1603814068,"title":"Docker V2 Github Action is Now GA","blogName":"Docker","image":"https://lh3.googleusercontent.com/05RlRLhTEUO0rAJ6D8xQptYdoV7hANYuSbsBlX1nkpCVnJR_vntvFpWqT0lsbFLeSurgi3JJa3qScr_nape9iZG0xlzyMA6rLe_EaoSapUtc9fIdAB8OLSgNZ2W965cLd7VJ6419","categories":["Engineering","Products","docker","github","github action"],"description":"Docker is happy to announce the GA of our V2 Github Action. We’ve been working with @crazy-max over the last few months along with getting feedback from the wider community on how we can improve our existing Github Action. We have now moved from our single action to a clearer division and advanced set of [&#8230;]\nThe post Docker V2 Github Action is Now GA appeared first on Docker Blog.\n","publish_date":"2020-10-27 15:54:28","link":"https://www.docker.com/blog/docker-v2-github-action-is-now-ga/","blog":{"id":"docker","link":"https://www.docker.com/blog/category/engineering","name":"Docker","rssFeed":"https://www.docker.com/blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"a1575c318f50f3d33d8e45eb9865397b","publish_timestamp":1603322172,"title":"Understanding Inner Loop Development and Pull Rates","blogName":"Docker","image":"https://lh4.googleusercontent.com/Um5A98mnKYNrEAqgv3lwn8ttfNTZVwQkduYB97XFSjKHL7Cf3rwaKZNVEOngEVawj7_4m2-Dw-odJOXpr1yNAnzU2KeIi0gAv7p19JM-0_dmn-qUnpbAwbdCyxJx7qO7qUFBtLJu","categories":["Engineering","Products","docker","pulls","rate limit"],"description":"We have heard feedback that given the changes Docker introduced relating to network egress and the number of pulls for free users, that there are questions around the best way to use Docker as part of your development workflow without hitting these limits. This blog post covers best practices that improve your experience and uses [&#8230;]\nThe post Understanding Inner Loop Development and Pull Rates appeared first on Docker Blog.\n","publish_date":"2020-10-21 23:16:12","link":"https://www.docker.com/blog/understanding-inner-loop-development-and-pull-rates/","blog":{"id":"docker","link":"https://www.docker.com/blog/category/engineering","name":"Docker","rssFeed":"https://www.docker.com/blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"f22597e6da6140b989a8c2d8b2906273","publish_timestamp":1602188988,"title":"Improve the Security of Hub Container Images with Automatic Vulnerability Scans","blogName":"Docker","image":"https://lh5.googleusercontent.com/PcLfKnkr8CLxkW6rJ1F0lVcnSZGXuBPqSPeJ8j_fGKJZNJ01Tn3zr6IlKdEkYqEJ49neaPxE83WAO-LmQ8OwMvag0RAKJOir9cFzkJixS5zfHY7u3NCJ-pl-m9vHHKkWlHm9u5cD","categories":["Engineering","Products","Container Security","docker","Docker security","Vulnerability Scanning"],"description":"In yesterday’s blog about improvements to the end-to-end Docker developer experience, I was thrilled to share how we are integrating security into image development, and to announce the launch of vulnerability scanning for images pushed to the Hub. This release is one step in our collaboration with our partner Snyk where we are integrating their [&#8230;]\nThe post Improve the Security of Hub Container Images with Automatic Vulnerability Scans appeared first on Docker Blog.\n","publish_date":"2020-10-08 20:29:48","link":"https://www.docker.com/blog/improve-the-security-of-hub-container-images-with-automatic-vulnerability-scans/","blog":{"id":"docker","link":"https://www.docker.com/blog/category/engineering","name":"Docker","rssFeed":"https://www.docker.com/blog/category/engineering/feed/","type":"company"},"blogType":"company"},{"id":"14dcbc72f66ff3b6c9886c58d534388b","publish_timestamp":1616084302,"title":"Introducing Amazon S3 Object Lambda – Use Your Code to Process Data as It Is Being Retrieved from S3","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/03/18/Site-Merch_S3-Object-Lambda_Final_SocialMedia_1.png","categories":["Amazon Simple Storage Services S3","Announcements","AWS Lambda","Launch","News","Serverless","Storage"],"description":"When you store data in Amazon Simple Storage Service (S3), you can easily share it for use by multiple applications. However, each application has its own requirements and may need a different view of the data. For example, a dataset created by an e-commerce application may include personally identifiable information (PII) that is not needed […]","publish_date":"2021-03-18 16:18:22","link":"https://aws.amazon.com/blogs/aws/introducing-amazon-s3-object-lambda-use-your-code-to-process-data-as-it-is-being-retrieved-from-s3/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"e5a9a3cb03b0638fed4bae37ce76e22e","publish_timestamp":1615931531,"title":"IAM Access Analyzer Update – Policy Validation","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/827bfc458708f0b442009c9c9836f7e4b65557fb/2020/06/03/Blog-Post_thumbnail.png","categories":["AWS IAM Access Analyzer","AWS Identity and Access Management IAM","Launch","News"],"description":"AWS Identity and Access Management (IAM) is an important and fundamental part of AWS. You can create IAM policies and service control policies (SCPs) that define the desired level of access to specific AWS services and resources, and then attach the policies to IAM principals (users and roles), groups of users, or to AWS resources. […]","publish_date":"2021-03-16 21:52:11","link":"https://aws.amazon.com/blogs/aws/iam-access-analyzer-update-policy-validation/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"05d9d9cbb328b3f1c6c844212dc8945e","publish_timestamp":1615931279,"title":"New Amazon EC2 X2gd Instances – Graviton2 Power for Memory-Intensive Workloads","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/03/16/Site-Merch_Amazon-EC2-X2gd_SocialMedia_1.png","categories":["Amazon EC2","Graviton","Launch","News"],"description":"We launched the first Graviton-powered EC2 instances in late 2018 and announced the follow-on Graviton2 processor just a year later. The dual SIMD units, support for int8 and fp16 instructions, and other architectural improvements between generations combine to make the Graviton2 a highly cost-effective workhorse processor. Today, you can choose between General Purpose (M6g and […]","publish_date":"2021-03-16 21:47:59","link":"https://aws.amazon.com/blogs/aws/new-amazon-ec2-x2gd-instances-graviton2-power-for-memory-intensive-workloads/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"9610c01c2c77eaac8df3b337882ac903","publish_timestamp":1615849097,"title":"AWS Fault Injection Simulator – Use Controlled Experiments to Boost Resilience","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/03/15/Site-Merch_AWS-Fault-Injection-Simulator_SocialMedia_1.png","categories":["AWS reInvent","Launch","News"],"description":"AWS gives you the components that you need to build systems that are highly reliable: multiple Regions (each with multiple Availability Zones), Amazon CloudWatch (metrics, monitoring, and alarms), Auto Scaling, Load Balancing, several forms of cross-region replication, and lots more. When you put them together in line with the guidance provided in the Well-Architected Framework, […]","publish_date":"2021-03-15 22:58:17","link":"https://aws.amazon.com/blogs/aws/aws-fault-injection-simulator-use-controlled-experiments-to-boost-resilience/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"2e2d9e2a4144941c0adfdfcba5731293","publish_timestamp":1615822090,"title":"Amazon S3 Glacier Price Reduction","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/03/12/s3_gl_modes_2.png","categories":["Amazon S3 Glacier","Amazon Simple Storage Services S3","Launch","News","Price Reduction","Storage"],"description":"The Amazon S3 Glacier storage class is ideal for data archiving and long-term backup of information that will be accessed at least once per quarter (Amazon S3 Glacier Deep Archive is a better fit for data that is seldom accessed). Amazon S3 Glacier stores your data across three Availability Zones (AZs), each physically separated from […]","publish_date":"2021-03-15 15:28:10","link":"https://aws.amazon.com/blogs/aws/amazon-s3-glacier-price-reduction/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"b457177067b3ac85116499355da9d64d","publish_timestamp":1615743355,"title":"Celebrate 15 Years of Amazon S3 with ‘Pi Week’ Livestream Events","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/03/12/site_merch_s3_birthday_blog_1.png","categories":["Amazon S3 Glacier","Amazon Simple Storage Services S3","Launch","News","Storage"],"description":"I wrote the blog post that announced Amazon Simple Storage Service (S3) fifteen years ago today. In that post, I made it clear that the service was accessed via APIs and that it was targeted at developers, outlined a few key features, and shared pricing information. Developers found that post, started to write code to […]","publish_date":"2021-03-14 17:35:55","link":"https://aws.amazon.com/blogs/aws/amazon-s3s-15th-birthday-it-is-still-day-1-after-5475-days-100-trillion-objects/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"9d94219cd50244de8f998a198797bbdf","publish_timestamp":1615502023,"title":"How to Use Channel Assembly with AWS Elemental MediaTailor to Launch Virtual Channels from Existing Sources","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/03/04/Channel_Assembly_HERO_IMAGE.png","categories":["AWS Elemental MediaTailor"],"description":"If you’re a broadcaster or Over-the-Top (OTT) channel operator, you can now create virtual channels by pulling together programming from multiple existing on-demand sources and blending them into a linear playlist. Creating a customized channel for a smaller audience group — such as a category of viewers based on interest, or regional preferences — was […]","publish_date":"2021-03-11 22:33:43","link":"https://aws.amazon.com/blogs/aws/how-to-use-channel-assembly-with-aws-elemental-mediatailor-to-launch-virtual-channels-from-existing-sources/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"8b8c6b8f0bbf316de9a1b477e96defa3","publish_timestamp":1615496491,"title":"Get to know the first new AWS Heroes of 2021!","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2018/07/03/AWS-Heroes.png","categories":["Announcements","AWS Heroes","Launch","News"],"description":"The global AWS Heroes program recognizes individuals who are prominent leaders in local tech communities. Heroes help others learn about AWS by sharing knowledge via blog posts, presentations, social media, and open source projects; or by organizing events, Meetups, and workshops. As global communities grow we continue to see new leaders emerge, and today we’re […]","publish_date":"2021-03-11 21:01:31","link":"https://aws.amazon.com/blogs/aws/get-to-know-the-first-new-aws-heroes-of-2021/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"c60613af4754736d40861b27fac748ab","publish_timestamp":1615324303,"title":"New – Lower Cost Storage Classes for Amazon Elastic File System","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/03/05/Site-Merch_Amazon-EFS_SocialMedia_1.png","categories":["Amazon Elastic File System EFS","Launch","News"],"description":"Amazon Elastic File System (Amazon EFS) provides a simple, serverless, set-and-forget elastic file system for shared data across Amazon Elastic Compute Cloud (EC2) instances or with container and serverless services such as Amazon Elastic Container Service (ECS), Amazon Elastic Kubernetes Service (EKS), AWS Fargate, and AWS Lambda. Until now, customers could choose Amazon EFS Standard […]","publish_date":"2021-03-09 21:11:43","link":"https://aws.amazon.com/blogs/aws/new-lower-cost-one-zone-storage-classes-for-amazon-elastic-file-system/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"241df16ff433589480794e5c0b1453ac","publish_timestamp":1614656927,"title":"AWS Asia Pacific (Osaka) Region Now Open to All, with Three AZs and More Services","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/827bfc458708f0b442009c9c9836f7e4b65557fb/2020/06/03/Blog-Post_thumbnail.png","categories":["Announcements","Regions"],"description":"AWS has had a presence in Japan for a long time! We opened the Asia Pacific (Tokyo) Region in March 2011, added a third Availability Zone (AZ) in 2012, and a fourth in 2018. Since that launch, customers in Japan and around the world have used the region to host an incredibly wide variety of […]","publish_date":"2021-03-02 03:48:47","link":"https://aws.amazon.com/blogs/aws/aws-asia-pacific-osaka-region-now-open-to-all-with-three-azs-more-services/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"d48fa598df68ab574499c53fc77634b0","publish_timestamp":1614630200,"title":"AWS DeepRacer League’s 2021 Season Launches With New Open and Pro Divisions","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/02/25/leagues-2.jpg","categories":["Announcements","Artificial Intelligence","AWS DeepRacer"],"description":"As a developer, I have been hearing a lot of stories lately about how companies have solved their business problems using machine learning (ML), so one of my goals for 2021 is to learn more about it. For the last few years I have been using artificial intelligence (AI) services such as, Amazon Rekognition, Amazon […]","publish_date":"2021-03-01 20:23:20","link":"https://aws.amazon.com/blogs/aws/aws-deepracer-leagues-2021-season-launches-with-new-open-and-pro-divisions/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"1b28e69d80702278ba3c991bb95204d9","publish_timestamp":1613426647,"title":"Amplify Flutter is Now Generally Available: Build Beautiful Cross-Platform Apps","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/02/12/Site-Merch_AWS-Amplify-Flutter_SocialMedia_1.png","categories":["Announcements","AWS Amplify","FrontEnd Web  Mobile","Uncategorized"],"description":"AWS Amplify is a set of tools and services for building secure, scalable mobile and web applications. Currently, Amplify supports iOS, Android, and JavaScript (web and React Native) and is the quickest and easiest way to build applications powered by Amazon Web Services (AWS). Flutter is Google’s UI toolkit for building natively compiled mobile, web, […]","publish_date":"2021-02-15 22:04:07","link":"https://aws.amazon.com/blogs/aws/amplify-flutter-is-now-generally-available-build-beautiful-cross-platform-apps/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"7d62e3ca59bd70f8436809d577a3ec24","publish_timestamp":1612481033,"title":"New – Amazon Elastic Block Store Local Snapshots on AWS Outposts","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/01/19/Site-Merch_EBS_Snapshots_Outposts_SocialMedia_1.png","categories":["Amazon Elastic Block Storage EBS","AWS Outposts","CloudEndure Disaster Recovery","CloudEndure Migration","Launch","News"],"description":"Today I am happy to announce that AWS Outposts customers can now make local snapshots of their Amazon Elastic Block Store (EBS) volumes, making it easy to meet data residency and local backup requirements. AWS Outposts is a fully managed service that extends AWS infrastructure, services, APIs, and tools to virtually any datacenter, co-location space, […]","publish_date":"2021-02-04 23:23:53","link":"https://aws.amazon.com/blogs/aws/new-amazon-elastic-block-store-local-snapshots-on-aws-outposts/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"0e6f3ba0a05a59cbffa7b1226274036a","publish_timestamp":1612298969,"title":"AWS PrivateLink for Amazon S3 is Now Generally Available","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/01/26/Site-Merch_AWS-PrivateLink-for-Amazon-S3_SocialMedia_1.png","categories":["Amazon Simple Storage Services S3","Announcements","AWS PrivateLink","Storage"],"description":"At AWS re:Invent, we pre-announced that AWS PrivateLink for Amazon S3 was coming soon, and soon has arrived — this new feature is now generally available. AWS PrivateLink provides private connectivity between Amazon Simple Storage Service (S3) and on-premises resources using private IPs from your virtual network. Way back in 2015, S3 was the first […]","publish_date":"2021-02-02 20:49:29","link":"https://aws.amazon.com/blogs/aws/aws-privatelink-for-amazon-s3-now-available/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"0fb05975af89832d373cb4046ab627d8","publish_timestamp":1611791452,"title":"New – Multiple Private Marketplace Catalogs","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/01/26/Private-Marketplace-icon.jpg","categories":["Announcements","AWS Marketplace"],"description":"We launched AWS Marketplace in 2014, and it allows customers to find, buy, and immediately start using cloud-based applications developed by independent software vendors (ISVs). In 2018, we added the ability to add a Private Marketplace where you can curate a list of approved products your users can purchase from AWS Marketplace. Today we are […]","publish_date":"2021-01-27 23:50:52","link":"https://aws.amazon.com/blogs/aws/new-multiple-private-marketplace-catalogs/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"1a14555d0003f542f6660c8b6dd35e13","publish_timestamp":1611275953,"title":"Amazon Lex Introduces an Enhanced Console Experience and New V2 APIs","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2021/01/16/Lex-Social-Image-10.20.png","categories":["Amazon Lex","Announcements","Artificial Intelligence"],"description":"Today, the Amazon Lex team has released a new console experience that makes it easier to build, deploy, and manage conversational experiences. Along with the new console, we have also introduced new V2 APIs, including continuous streaming capability. These improvements allow you to reach new audiences, have more natural conversations, and develop and iterate faster. […]","publish_date":"2021-01-22 00:39:13","link":"https://aws.amazon.com/blogs/aws/amazon-lex-enhanced-console-experience/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"75633d69512a7c79db95d6b9c808c3f6","publish_timestamp":1610038216,"title":"New – AWS Transfer Family support for Amazon Elastic File System","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2020/04/14/Site-Merch_AWS_Transfer_Family_SocialMedia_1.png","categories":["Amazon Elastic File System EFS","AWS Transfer Family","Launch","Migration  Transfer Services","News"],"description":"AWS Transfer Family provides fully managed Secure File Transfer Protocol (SFTP), File Transfer Protocol (FTP) over TLS, and FTP support for Amazon Simple Storage Service (S3), enabling you to seamlessly migrate your file transfer workflows to AWS. Today I am happy to announce AWS Transfer Family now also supports file transfers to Amazon Elastic File […]","publish_date":"2021-01-07 16:50:16","link":"https://aws.amazon.com/blogs/aws/new-aws-transfer-family-support-for-amazon-elastic-file-system/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"e26d77691a80213aa01561d2c616b792","publish_timestamp":1608135061,"title":"Amazon Location – Add Maps and Location Awareness to Your Applications","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2020/12/16/Amazon-Location-Services-1.jpg","categories":["AWS reInvent","FrontEnd Web  Mobile","Launch","News"],"description":"We want to make it easier and more cost-effective for you to add maps, location awareness, and other location-based features to your web and mobile applications. Until now, doing this has been somewhat complex and expensive, and also tied you to the business and programming models of a single provider. Introducing Amazon Location Service Today […]","publish_date":"2020-12-16 16:11:01","link":"https://aws.amazon.com/blogs/aws/amazon-location-add-maps-and-location-awareness-to-your-applications/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"a221bfd33231796b0f4c48d7206f38dc","publish_timestamp":1608055649,"title":"New –  FreeRTOS Long Term Support to Provide Years of Feature Stability","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2020/12/06/Site-Merch_LTS1.0-FreeRTOS-final_SocialMedia_1.png","categories":["AWS reInvent","FreeRTOS","Launch","News"],"description":"Today, I’m particularly happy to announce FreeRTOS Long Term Support (LTS). FreeRTOS is an open source, real-time operating system for microcontrollers that makes small, low-power edge devices easy to program, deploy, secure, connect, and manage. LTS releases offer a more stable foundation than standard releases as manufacturers deploy and later update devices in the field. […]","publish_date":"2020-12-15 18:07:29","link":"https://aws.amazon.com/blogs/aws/new-freertos-long-term-support-to-provide-years-of-feature-stability/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"d3ce2a42e7eb7546fe8ddbfd732b095e","publish_timestamp":1608055642,"title":"Announcing AWS IoT Greengrass 2.0 – With an Open Source Edge Runtime and New Developer Capabilities","blogName":"AWS","image":"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2020/12/09/Site-Merch_Amazon-Greengrass_2.0_SocialMedia_1.png","categories":["AWS IoT Greengrass","AWS reInvent","Internet of Things","Launch","News"],"description":"I am happy to announce AWS IoT Greengrass 2.0, a new version of AWS IoT Greengrass that makes it easy for device builders to build, deploy, and manage intelligent device software. AWS IoT Greengrass 2.0 provides an open source edge runtime, a rich set of pre-built software components, tools for local software development, and new […]","publish_date":"2020-12-15 18:07:22","link":"https://aws.amazon.com/blogs/aws/announcing-aws-iot-greengrass-2-0-with-an-open-source-edge-runtime-and-new-developer-capabilities/","blog":{"id":"aws","link":"https://aws.amazon.com/blogs/aws/","name":"AWS","rssFeed":"https://aws.amazon.com/blogs/aws/feed/","type":"company"},"blogType":"company"},{"id":"df0ea5141a590688bdca22284d672750","publish_timestamp":1614739694,"title":"Scaling Scala: How we chose our backend language and tooling","blogName":"Asana","image":"https://blog.asana.com/wp-content/post-images/05-Distributed-Teams-01.png","categories":["Engineering","Technical","engineering"],"description":"We’ve been using Scala at Asana since 2013. Since we started using Scala, the number of engineers writing it has increased from a single team to almost every engineer at Asana (and Asana’s grown, too!) As we’ve scaled, we’ve had to carefully consider our tooling and best practices, and we thought it would be useful [&#8230;]\nThe post Scaling Scala: How we chose our backend language and&nbsp;tooling appeared first on The Asana Blog.\n","publish_date":"2021-03-03 02:48:14","link":"https://blog.asana.com/2021/03/scaling-scala/","blog":{"id":"asana","link":"https://blog.asana.com/category/eng/","name":"Asana","rssFeed":"https://blog.asana.com/category/eng/feed/","type":"company"},"blogType":"company"},{"id":"90aaa6628075d5c191aa1e0ccd471f0b","publish_timestamp":1613070776,"title":"How we use Kubernetes at Asana","blogName":"Asana","image":"http://blog.asana.com/wp-content/post-images/906e13.png","categories":["Engineering","Technical","engineering"],"description":"At Asana, we use Kubernetes to deploy and manage services independently from our monolith infrastructure. We encountered a few pain points when initially using Kubernetes, and built a framework to standardize the creation and maintenance of Kubernetes applications, aptly named KubeApps. Over the last two years, the Infrastructure Platform team has been making improvements to [&#8230;]\nThe post How we use Kubernetes at&nbsp;Asana appeared first on The Asana Blog.\n","publish_date":"2021-02-11 19:12:56","link":"https://blog.asana.com/2021/02/kubernetes-at-asana/","blog":{"id":"asana","link":"https://blog.asana.com/category/eng/","name":"Asana","rssFeed":"https://blog.asana.com/category/eng/feed/","type":"company"},"blogType":"company"},{"id":"c377808f9b9679d93939038bddb05a48","publish_timestamp":1610138520,"title":"How Asana ships stable web application releases","blogName":"Asana","image":"https://blog.asana.com/wp-content/post-images/How-Asana-Ships-Application-Releases.png","categories":["Engineering","Technical","engineering"],"description":"The majority of our product code—our large client bundle, our back-end mutation server, our distributed job system—are shipped together as a single web release. We’ve built systems that allow us to continue to ship web releases safely, three times a day, even as our engineering team and product codebase grow geometrically. In this blog post, [&#8230;]\nThe post How Asana ships stable web application&nbsp;releases appeared first on The Asana Blog.\n","publish_date":"2021-01-08 20:42:00","link":"https://blog.asana.com/2021/01/asana-engineering-ships-web-application-releases/","blog":{"id":"asana","link":"https://blog.asana.com/category/eng/","name":"Asana","rssFeed":"https://blog.asana.com/category/eng/feed/","type":"company"},"blogType":"company"},{"id":"fd03f3f2b9509bf8bb7cc578a123aafd","publish_timestamp":1604589422,"title":"Meet our 2020 summer intern class","blogName":"Asana","image":"http://blog.asana.com/wp-content/post-images/906e13.png","categories":["Engineering","Intern Program","engineering","Interning at Asana"],"description":"Every summer, we have a class of interns join our Design and Engineering teams. They work on important and impactful projects, contribute to feature launches, site stability and making the codebase a more pleasant place to develop in. This year we had our first fully remote class, and it was our largest class yet: 30 [&#8230;]\nThe post Meet our 2020 summer intern&nbsp;class appeared first on The Asana Blog.\n","publish_date":"2020-11-05 15:17:02","link":"https://blog.asana.com/2020/11/2020-summer-interns-engineering/","blog":{"id":"asana","link":"https://blog.asana.com/category/eng/","name":"Asana","rssFeed":"https://blog.asana.com/category/eng/feed/","type":"company"},"blogType":"company"},{"id":"43cbf753de922b7ffe2bfa11864ca083","publish_timestamp":1599255655,"title":"Part 2 – WorldStore: Distributed caching with Reactivity","blogName":"Asana","image":"http://blog.asana.com/wp-content/post-images/906e13.png","categories":["Engineering","Technical"],"description":"This post is the second in a two-part series on how we implemented distributed caching in a fully reactive framework. Read Part 1 before reading this article. Last time we talked about why we needed to add distributed caching, and where it fit into our infrastructure. Picking up from there, in this post we’ll jump [&#8230;]\nThe post Part 2 &#8211; WorldStore: Distributed caching with&nbsp;Reactivity appeared first on The Asana Blog.\n","publish_date":"2020-09-04 21:40:55","link":"https://blog.asana.com/2020/09/worldstore-distributed-caching-reactivity-part-2/","blog":{"id":"asana","link":"https://blog.asana.com/category/eng/","name":"Asana","rssFeed":"https://blog.asana.com/category/eng/feed/","type":"company"},"blogType":"company"},{"id":"f7383b272ac9ccdcaf3ebf3e1adbb417","publish_timestamp":1599254974,"title":"Part 1 – WorldStore: Distributed caching with Reactivity","blogName":"Asana","image":"http://blog.asana.com/wp-content/post-images/906e13.png","categories":["Engineering","Technical"],"description":"This post is the first in a two-part series on how we implemented distributed caching in a fully reactive framework. As part of our application framework overhaul [1] a few years ago, we built LunaDb [2], a reactive data system to underpin the Luna web framework. While LunaDb has worked well, Asana’s growing scale and [&#8230;]\nThe post Part 1 &#8211; WorldStore: Distributed caching with&nbsp;Reactivity appeared first on The Asana Blog.\n","publish_date":"2020-09-04 21:29:34","link":"https://blog.asana.com/2020/09/worldstore-distributed-caching-reactivity-part-1/","blog":{"id":"asana","link":"https://blog.asana.com/category/eng/","name":"Asana","rssFeed":"https://blog.asana.com/category/eng/feed/","type":"company"},"blogType":"company"},{"id":"96b4c9a1eef225e1a52f6627633e805c","publish_timestamp":1598483077,"title":"Meet 3 of our Engineering Program Leads","blogName":"Asana","image":"https://blog.asana.com/wp-content/post-images/engblogheader.jpg","categories":["Engineering","Engineering Culture"],"description":"There are many different ways to grow and develop as a software engineer.&#160;One misconception is that the only way to grow is to become a people manager. However, there are many growth paths in software engineering for people who want to remain individual contributors. One of the paths for growth at Asana is Program Leadership.&#160; [&#8230;]\nThe post Meet 3 of our Engineering Program&nbsp;Leads appeared first on The Asana Blog.\n","publish_date":"2020-08-26 23:04:37","link":"https://blog.asana.com/2020/08/engineering-program-leads/","blog":{"id":"asana","link":"https://blog.asana.com/category/eng/","name":"Asana","rssFeed":"https://blog.asana.com/category/eng/feed/","type":"company"},"blogType":"company"},{"id":"cc99d7348a8a59c30dd3ebb70fae674b","publish_timestamp":1594139544,"title":"10 tips for setting engineering goals from Asana Head of Engineering, Prashant Pandey","blogName":"Asana","image":"https://blog.asana.com/wp-content/post-images/Our-top-integrations-for-remote-work_3136x2080.png","categories":["Best Practices","Engineering"],"description":"Read this article in French, German, Portuguese, Spanish, or Japanese. As an engineering leader, one of the greatest gifts you can give your team is clarity of purpose, plan, and responsibility. Recently, Asana Head of Engineering, Prashant Pandey sat down with Plato to discuss why clarity is so important and how it impacts goals. Here [&#8230;]\nThe post 10 tips for setting engineering goals from Asana Head of Engineering, Prashant&nbsp;Pandey appeared first on The Asana Blog.\n","publish_date":"2020-07-07 16:32:24","link":"https://blog.asana.com/2020/07/engineering-goals/","blog":{"id":"asana","link":"https://blog.asana.com/category/eng/","name":"Asana","rssFeed":"https://blog.asana.com/category/eng/feed/","type":"company"},"blogType":"company"},{"id":"ffb3dcc88311a8f496c674594d5d167e","publish_timestamp":1593199899,"title":"Why I joined Asana: Kshitij Grover, Software Engineer","blogName":"Asana","image":"https://blog.asana.com/wp-content/post-images/Kshitij_Blog.png","categories":["Culture","Engineering","Engineering Culture"],"description":"Welcome to our monthly “Why I joined Asana” series! Every month, we talk with Asanas across teams and offices to learn who they are and why they chose to pursue careers at Asana. Kshitij Grover is a Software Engineer on the Infrastructure team based out of San Francisco. He is a huge fan of reading [&#8230;]\nThe post Why I joined Asana: Kshitij Grover, Software&nbsp;Engineer appeared first on The Asana Blog.\n","publish_date":"2020-06-26 19:31:39","link":"https://blog.asana.com/2020/06/why-i-joined-asana-kshitij-grover/","blog":{"id":"asana","link":"https://blog.asana.com/category/eng/","name":"Asana","rssFeed":"https://blog.asana.com/category/eng/feed/","type":"company"},"blogType":"company"},{"id":"b139cfa3ac806a32e73f83bed26d87ab","publish_timestamp":1590595539,"title":"How to thrive as an intern while working remotely","blogName":"Asana","image":"https://blog.asana.com/wp-content/post-images/status-announcement-LI_FB.png","categories":["Engineering","Intern Program","Remote Work","engineering","Interning at Asana","Internship"],"description":"At Asana, our internship program is built to provide support, mentorship, and challenging opportunities for aspiring software engineers. Given our recent shift to working 100% remotely due to COVID-19, we faced a unique challenge we hadn’t yet experienced: How do we turn our internship program — with our biggest class of interns to date — [&#8230;]\nThe post How to thrive as an intern while working&nbsp;remotely appeared first on The Asana Blog.\n","publish_date":"2020-05-27 16:05:39","link":"https://blog.asana.com/2020/05/remote-engineering-intern-working-from-home/","blog":{"id":"asana","link":"https://blog.asana.com/category/eng/","name":"Asana","rssFeed":"https://blog.asana.com/category/eng/feed/","type":"company"},"blogType":"company"},{"id":"6a0f969a59c55e02100644e39464eff4","publish_timestamp":1614880800,"title":"Atlas: Our journey from a Python monolith to a managed platform","blogName":"Dropbox","image":"https://aem.dropbox.com/cms/content/dam/dropbox/tech-blog/en-us/2021/03/atlas/diagrams/Techblog-Atlas-Social.png","categories":["Python","gRPC","Service Oriented Architecture","Envoy"],"description":"\n\n\n\n    \n    \nDropbox, to our customers, needs to be a reliable and responsive service. As a company, we’ve had to scale constantly since our start, today serving more than 700M registered users in every time zone on the planet who generate at least 300,000 requests per second. Systems that worked great for a startup hadn’t scaled well, so we needed to devise a new model for our internal systems, and a way to get there without disrupting the use of our product.\nIn this post, we’ll explain why and how we developed and deployed Atlas, a platform which provides the majority of benefits of a Service Oriented Architecture, while minimizing the operational cost that typically comes with owning a service. \nMonolith should be by choice\nThe majority of software developers at Dropbox contribute to server-side backend code, and all server side development takes place in our server monorepo. We mostly use Python for our server-side product development, with more than 3 million lines of code belonging to our monolithic Python server. \nIt works, but we realized the monolith was also holding us back as we grew. Developers wrangled daily with unintended consequences of the monolith. Every line of code they wrote was, whether they wanted or not, shared code—they didn’t get to choose what was smart to share, and what was best to keep isolated to a single endpoint. Likewise, in production, the fate of their endpoints was tied to every other endpoint, regardless of the stability, criticality, or level of ownership of these endpoints.\nIn 2020, we ran a project to break apart the monolith and evolve it into a serverless managed platform, which would reduce code tangles and liberate services and their underlying engineering teams from being entwined with one another. To do so, we had to innovate both the architecture (e.g. standardizing on gRPC and using Envoy’s gRPC-HTTP transcoding) and the operations (e.g. introducing autoscaling and canary analysis). This blog post captures key ideas and learnings from our journey.\n\n\n\n\n     Metaserver: The Dropbox monolith\n\n\n\nDropbox’s internal service topology as of today can be thought of as a “solar system” model, in which a lot of product functionality is served by the monolith, but platform-level components like authentication, metadata storage, filesystem, and sync have been separated into different services.\nAbout half of all commits to our server repository modify our large monolithic Python web application, Metaserver.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            Extremely simplified view of existing serving stack\n\n        \n    \n\n\nMetaserver is one of our oldest services, created in 2007 by one of our co-founders. It has served Dropbox well, but as our engineering team marched to deliver new features over the years, the organic growth of the codebase led to serious challenges.\nTangled codebase\nMetaserver’s code was originally organized in a simple pattern one might expect to see in a small open source project—library, model, controllers—with no centralized curation or guardrails to ensure the sustainability of the codebase. Over the years, the Metaserver codebase grew to become one of the most disorganized and tangled codebases in the company. \n\n\n\n\n\n\n\n\n\n\n    \n        Copy\n    \n    //metaserver/controllers/ …\n//metaserver/model/ …\n//metaserver/lib/ …\n\n\n\nMetaserver Code Structure\n\n\n\nBecause the codebase had multiple teams working on it, no single team felt strong ownership over codebase quality. For example, to unblock a product feature, a team would introduce import cycles into the codebase rather than refactor code. Even though this let us ship code faster in the short term, it left the codebase much less maintainable, and problems compounded.\n\nInconsistent push cadence\nWe push Metaserver to production for all our users daily. Unfortunately, with hundreds of developers effectively contributing to the same codebase, the likelihood of at least one critical bug being added every day had become fairly high. This would necessitate rollbacks and cherry picks of the entire monolith, and caused an inconsistent and unreliable push cadence for developers. Common best practices (for example, from Accelerate) point to fast, consistent deploys as the key to developer productivity. We were nowhere close to ideal on this dimension.\nInconsistent push cadence leads to unnecessary uncertainty in the development experience. For example, if a developer is working towards a product launch on day X, they aren’t sure whether their code should be submitted to our repository by day X-1, X-2 or even earlier, as another developer’s code might cause a critical bug in an unrelated component on day X and necessitate a rollback of the entire cluster completely unrelated to their own code.\nInfrastructure debt\nWith a monolith of millions of lines of code, infrastructure improvements take much longer or never happen. For example, it had become impossible to stage a rollout of a new version of an HTTP framework or Python on only non-critical routes.\nAdditionally, Metaserver uses a legacy Python framework unused in most other Dropbox services or anywhere else externally. While our internal infrastructure stack evolved to use industry standard open source systems like gRPC, Metaserver was stuck on a deprecated legacy framework that unsurprisingly had poor performance and caused maintenance headaches due to esoteric bugs. For example, the legacy framework only supports HTTP/1.0 while modern libraries have moved to HTTP/1.1 as the minimum version. \nMoreover, all the benefits we developed or integrated in our internal infrastructure, like integrated metrics and tracing, had to be hackily redone for Metaserver which was built atop different internal frameworks.\nOver the past few years, we had spun up several workstreams to combat the issues we faced. Not all of them were all successful, but even those we gave up on paved the way to our current solution.\n\n\n\n\n     SOA: the cost of operating independent services\n\n\n\nWe tried to break up Metaserver as part of a larger push around a Service Oriented Architecture (SOA) initiative. The goal of SOA was to establish better abstractions and separation of concerns for functionalities at Dropbox—all problems that we wanted to solve in Metaserver. \nThe execution plan was simple: make it easy for teams to operate independent services in production, then carve out pieces of Metaserver into independent services. \nOur SOA effort had two major milestones:\n\nMake it possible and easy to build services outside of Metaserver\nExtract core functionalities like identity management from the monolith and expose them via RPC, to allow new functionalities to be built outside of Metaserver\nEstablish best practices and a production readiness process for smoothly and scalably onboarding new multiple services that serve customer-facing traffic, i.e. our live site services\n\n\nBreak up Metaserver into smaller services owned and operated by various teams\n\nThe SOA effort proved to be long and arduous. After over a year and a half, we were well into the first milestone. However, the experience from executing that first milestone exposed the flaws of the second milestone. As more teams and services were introduced into the critical path for customer traffic, we found it increasingly difficult to maintain a high reliability standard. This problem would only compound as we moved up the stack away from core functionalities and asked product teams to run services.\nNo one solution for everything\nWith this insight, we reassessed the problem. We found that product functionality at Dropbox could be divided into two broad categories:\n\nlarge, complex systems like all the logic around sharing a file\nsmall, self-contained functionality, like the homepage\n\nFor example, the “Sharing” service involves stateful logic around access control, rate limits, and quotas. On the other hand, the homepage is a fairly simple wrapper around our metadata store/filesystem service. It doesn’t change too often and it has very limited day to day operational burden and failure modes. In fact, operational issues for most routes served by Dropbox had common themes, like unexpected spikes of external traffic, or outages in underlying services. \n\nThis led us to an important conclusion:\n\nSmall, self contained functionality doesn’t need independently operated services. This is why we built Atlas.\nIt’s unnecessary overhead for a product team to plan capacity, set up good alerts and multihoming (automatically running in multiple data centers) for small, simple functionality. Teams mostly want a place where they can write some logic, have it automatically run when a user hits a certain route, and get some automatic basic alerts if there are too many errors in their route. The code they submit to the repository should be deployed consistently, quickly and continuously.\n\nMost of our product functionality falls into this category. Therefore, Atlas should optimize for this category.\n\nLarge components should continue being their own services, with which Atlas happily coexists.\nLarge systems can be operated by larger teams that sustainably manage the health of their systems. Teams should manage their own push schedules and set up dedicated alerts and verifiers.\n\n\n\n\n\n     Atlas: a hybrid approach\n\n\n\nWith the fundamental sustainability problems we had with Metaserver, and the learning that migrating Metaserver into many smaller services was not the right solution for everything, we came up with Atlas, a managed platform for the self-contained functionality use case.\nAtlas is a hybrid approach. It provides the user interface and experience of a “serverless” system like AWS Fargate to Dropbox product developers, while being backed by automatically provisioned services behind the scenes. \nAs we said, the goal of Atlas is to provide the majority of benefits of SOA, while minimizing the operational costs associated with running a service. \nAtlas is “managed,” which means that developers writing code in Atlas only need to write the interface and implementation of their endpoints. Atlas then takes care of creating a production cluster to serve these endpoints. The Atlas team owns pushing to and monitoring these clusters. \nThis is the experience developers might expect when contributing to a monolith versus Atlas:\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            Before and after Atlas\n\n        \n    \n\n\nGoals\nWe designed Atlas with five ideal outcomes in mind:\n\nCode structure improvements\nMetaserver had no real abstractions on code sharing, which led to coupled code. Highly coupled code can be the hardest to understand and refactor, and the most likely to sprout bugs when modified. We wanted to introduce a structure and reduce coupling so that new code would be easier to read and modify.\nIndependent, consistent pushes\nThe Metaserver push experience is great when it works. Product developers only have to worry about checking in code which will automatically get pushed to production. However, the aforementioned lack of push isolation led to an inconsistent experience. We wanted to create a platform where teams were not blocked on push due to a bug in unrelated code, and create the foundation for teams to push their own code in the future.\nMinimized operational busywork\nWe aimed to keep the operational benefits of Metaserver while providing some of the flexibility of a service. We set up automatic capacity management, automatic alerts, automatic canary analysis, and an automatic push process so that the migration from a monolith to a managed platform was smooth for product developers.\nInfrastructure unification\nWe wanted to unify all serving to standard open source components like gRPC. We don’t need to reinvent the wheel.\nIsolation\nSome features like the homepage are more important than others. We wanted to serve these independently, so that an overload or bug in one feature could not spill over to the rest of Metaserver.\n\nWe evaluated using off-the-shelf solutions to run the platform. But in order to de-risk our migration and ensure low engineering costs, it made sense for us to continue hosting services on the same deployment orchestration platform used by the rest of Dropbox. \nHowever, we decided to remove custom components, such as our custom request proxy Bandaid, and replace them with open source systems like Envoy that met our needs.\n\n\n\n\n     Technical design\n\n\n\nThe project involved a few key efforts:\nComponentization\n\nDe-tangle the codebase by feature into components, to prevent future tangles\nEnforce a single owner per component, so new functionality cannot be tacked onto a component by a non-owner\nIncentivize fewer shared libraries and more code sharing via RPC\n\nOrchestration\n\nAutomatically configure each component into a service in our deployment orchestration platform with &lt;50 lines of boilerplate code\nConfigure a proxy (Envoy) to send a request for a particular route to the right service, instead of simply sending each request to a Metaserver node\nConfigure services to speak to one another in gRPC instead of HTTP\n\nOperationalization\n\nAutomatically configure a deployment pipeline that runs daily and pushes to production for each component \nSet up automatic alerts and automatic analysis for regressions to each push pipeline to automatically pause and rollback in case of any problems\nAutomatically allocate additional hosts to scale up capacity via an autoscaler for each component based on traffic\n\nLet’s look at each of these in detail.\nComponentization\nLogical grouping of routes via servlets\n Atlas introduces Atlasservlets (pronounced “atlas servlets”) as a logical, atomic grouping of routes. For example, the home Atlasservlet contains all routes used to construct the homepage. The nav Atlasservlet contains all the routes used in the navigation bar on the Dropbox website.\nIn preparation for Atlas, we worked with product teams to assign Atlasservlets to every route in Metaserver, resulting in more than 200 Atlasservlets across more than 5000 routes. Atlasservlets are an essential tool for breaking up Metaserver.\n\n\n\n\n\n\n\n\n\n\n    \n        Copy\n    \n    //atlas/home/ …\n//atlas/nav/ …\n//atlas/&lt;some other atlasservlet&gt;/ …\n\n\n\nAtlas code structure, organized by servlets\n\n\n\nEach Atlasservlet is given a private directory in the codebase. The owner of the Atlasservlet has full ownership of this directory; they may organize it however they wish, and no one else can import from it. The Atlasservlet code structure inherently breaks up the Metaserver code monolith, requiring every endpoint to be in a private directory and make code sharing an explicit choice rather than an unexpected outcome of contributing to the monolith. \nHaving the Atlasservlet codified into our directory path also allows us to automatically generate production configs that would normally accompany a production service. Dropbox uses the Bazel build system for server side code, and we enforced prevention of imports through a Bazel feature called visibility rules, which allows library owners to control which code can use their libraries.\nBreakup of import cycles\nIn order to break up our codebase, we had to break most of our Python import cycles. This took several years to achieve with a bunch of scripts and a lot of grunt work and refactoring. We prevented regressions and new import cycles through the same mechanism of Bazel visibility rules.\n\nOrchestration\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            Atlas cluster strategy\n\n        \n    \n\n\nIn Atlas, every Atlasservlet is its own cluster.  This gives us three important benefits:\n\nIsolation by default\n A misbehaving route will only impact other routes in the same Atlasservlet, which is owned by the same team anyway.\nIndependent pushes\n Each Atlasservlet can be pushed separately, putting product developers in control of their own destiny with respect to the consistency of their pushes.\nConsistency\n Each Atlasservlet looks and behaves like any other internal service at Dropbox. So any tools provided by our infrastructure teams—e.g. periodic performance profiling—will work for all other teams’ Atlasservlets.\n\ngRPC Serving Stack\n One of our goals with Atlas was to unify our serving infrastructure. We chose to standardize on gRPC, a widely adopted tool at Dropbox. In order to continue to serve HTTP traffic, we used the gRPC-HTTP transcoding feature provided out of the box in Envoy, our proxy and load balancer. You can read more about Dropbox’s adoption of gRPC and Envoy in their respective blog posts.\n \n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            http transcoding\n\n        \n    \n\n\nIn order to facilitate our migration to gRPC, we wrote an adapter which takes an existing endpoint and converts it into the interface that gRPC expects, setting up any legacy in-memory state the endpoint expects. This allowed us to automate most of the migration code change. It also had the benefit of keeping the endpoint compatible with both Metaserver and Atlas during mid-migration, so we could safely move traffic between implementations.\nOperationalization\nAtlas’s secret sauce is the managed experience. Developers can focus on writing features without worrying about many operational aspects of running the service in production, while still retaining the majority of benefits that come with standalone services, like isolation. \nThe obvious drawback is that one team now bears the operational load of all 200+ clusters. Therefore, as part of the Atlas project we built several tools to help us effectively manage these clusters.\nAutomated Canary Analysis\n Metaserver (and Atlas by extension) is stateless. As a result one of the most common ways a failure gets introduced into the system is through code changes. If we can ensure that our push guardrails are as airtight as possible, this eliminates the majority of failure scenarios.\n \n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            Canary analysis\n\n        \n    \n\n\nWe automate our failure checking through a simple canary analysis service very similar to Netflix’s Kayenta. Each Atlas service consists of three deployments: canary, control, and production, with canary and control receiving only a small random percentage of traffic. During the push, canary is restarted with the newest version of the code. Control is restarted with the old version of the code but at the same time as canary to ensure the operate from the same starting point. \nWe automatically compare metrics like CPU utilization and route availability from the canary and control deployments, looking for metrics where canary may have regressed relative to control. In a good push, canary will perform either equal to or better than control, and the push will be allowed to proceed. A bad push will be stopped automatically and the owners notified.\nIn addition to canary analysis, we also have alerts set up which are checked throughout the process, including in between the canary, control, and production pushes of a single cluster. This lets us automatically pause and rollback the push pipeline if something goes wrong.\nMistakes still happen. Bad changes may slip through. This is where Atlas’s default isolation comes in handy. Broken code will only impact its one cluster and can be rolled back individually, without blocking code pushes for the rest of the organization.\nAutoscaling and capacity planning\n Atlas's clustering strategy results in a large number of small clusters. While this is great for isolation, it significantly reduces the headroom each cluster has to handle increases in traffic. Monoliths are large shared clusters, so a small RPS increase on a route is easily absorbed by the shared cluster. But when each Atlasservlet is its own service, a 10x increase in route traffic is harder to handle.\nCapacity planning for 200+ clusters would cripple our team. Instead, we built an autoscaling system. The autoscaler monitors the utilization of each cluster in real time and automatically allocates machines to ensure that we stay above 40% free capacity headroom per cluster. This allows us to handle traffic increases as well as remove the need to do capacity planning. \nThe autoscaling system reads metrics from Envoy’s Load Reporting Service and uses request queue length to decide cluster size, and probably deserves its own blog post.\n\n\n\n\n    Execution\n\n\n\nStepping stones, not milestones\nMany previous efforts to improve Metaserver had not succeeded due to the size and complexity of the codebase. This time around, we wanted to deliver value to product developers even if we didn’t succeed in fully replacing Metaserver with Atlas. \nThe execution plan for Atlas was designed with stepping stones, not milestones (as elegantly described by former Dropbox engineer James Cowling), so that each incremental step would provide sufficient value in case the next part of the project failed for any reason.\nA few examples:\n\nWe started off by speeding up testing frameworks in Metaserver, because we knew that an Atlas serving stack in tests might cause a regression in test times.\nWe had a constraint to significantly improve memory efficiency and reduce OOM kills when we migrated from Metaserver to Atlas, since we would be able to pack more processes per host and consume less capacity during the migration. We focused on delivering memory efficiency purely to Metaserver instead of tying the improvements to the Atlas rollout.\nWe designed a load test to prove that an Atlas MVP would be able to handle Metaserver traffic. We reused the load test to validate Metaserver’s performance on new hardware as part of a different project.\nWe backported workflow simplifications as much as feasible to Metaserver. For example, we backported some of the workflow improvements in Atlas to our web workflows in Metaserver.\nMetaserver development workflows are divided into three categories based on the protocol: web, API, and internal gRPC. We focused Atlas on internal gRPC first to de-risk the new serving stack without needing the more risky parts like gRPC-HTTP transcoding. This in turn gave us an opportunity to improve workflows for internal gRPC independent of the remaining risky parts of Atlas.\n\nHurdles\nWith a large migration like this, it’s no surprise that we ran into a lot of challenges. The issues faced could be their own blog post. We’ll summarize a few of the most interesting ones:\n\nThe legacy HTTP serving stack contained quirky, surprising, and hard to replicate behavior that had to be ported over to prevent regressions. We powered through with a combination of reading the original source code, reusing legacy library functions where required, relying on various existing integration tests, and designing a key set of tests that compare byte-by-byte outputs of the legacy and new systems to safely migrate.\nWhile splitting up Metaserver had wins in production, it was infeasible to spin up 200+ Python processes in our integration testing framework. We decided to merge the processes back into a monolith for local development and testing purposes. We also built heavy integration with our Bazel rules, so that the merging happens behind the scene and developers can reference Atlasservlets as regular services.\nSplitting up Metaserver in production broke many non-obvious assumptions that could not be caught easily in tests. For example, some infrastructure services had hardcoded the identity of Metaserver for access control. To minimize failures, we designed a meticulous and incremental migration plan with a clear understanding of the risks involved at each stage, and slowly monitored metrics as we rolled out the new system.\nEngineering workflows in Metaserver had grown organically with the monolith, arriving at a state where engineers had to page in an enormous amount of context to get the simplest work done. In order to ensure that Atlas prioritizes and solves major engineering pain points, we brought on key product developers as partners in the design, then went through several rounds of iteration to set up a roadmap that would definitively solve both product and infrastructural needs.\n\n\n\n\n\n    Status\n\n\n\nAtlas is currently serving more than 25% of the previous Metaserver traffic. We have validated the remaining migration in tests. We’re on a clear path to deprecate Metaserver in the near future.\n\n\n\n\n    Conclusion\n\n\n\nThe single most important takeaway from this multi-year effort is that well-thought-out code composition, early in a project’s lifetime, is essential. Otherwise, technical debt and code complexity compounds very quickly. The dismantling of import cycles and decomposition of Metaserver into feature based directories was probably the most strategically effective part of the project, because it prevented new code from contributing to the problem and also made our code simpler to understand.\nBy shipping a managed platform, we took a thoughtful approach on how to break up our Metaserver monolith. We learned that monoliths have many benefits (as discussed by Shopify) and blindly splitting up our monolith into services would have increased operational load to our engineering organization.\nIn our view, developers don’t care about the distinction between monoliths and services, and simply want the lowest-overhead way to deliver end value to customers. So we have very little doubt that a managed platform which removes operational busywork like capacity planning, while providing maximum flexibility like fast releases, is the way forward. We’re excited to see the industry move toward such platforms.\nWe’re hiring!\nIf you’re interested in solving large problems with innovative, unique solutions—at a company where your push schedule is more predictable : ) —please check out our open positions.\nAcknowledgements\nAtlas was a result of the work of a large number of Dropboxers and Dropbox alumni, including but certainly not limited to: Agata Cieplik, Aleksey Kurkin, Andrew Deck, Andrew Lawson, David Zbarsky, Dmitry Kopytkov, Jared Hance, Jeremy Johnson, Jialin Xu, Jukka Lehtosalo, Karandeep Johar, Konstantin Belyalov, Ivan Levkivskyi, Lennart Jansson, Phillip Huang, Pranay Sowdaboina, Pranesh Pandurangan, Ruslan Nigmatullin, Taylor McIntyre, and Yi-Shu Tai.\n\n\n\n    \n\n","publish_date":"2021-03-04 18:00:00","link":"https://dropbox.tech/infrastructure/atlas--our-journey-from-a-python-monolith-to-a-managed-platform","blog":{"id":"dropbox","link":"https://dropbox.tech/","name":"Dropbox","rssFeed":"https://dropbox.tech/feed/","type":"company"},"blogType":"company"},{"id":"036f941abc0f4703cd259d7c8e041901","publish_timestamp":1614802500,"title":"New Paper Endpoints Released in Preview","blogName":"Dropbox","image":"","categories":["Announcements","Preview","Paper"],"description":"New endpoints are available to interact with Paper in the File System using the Dropbox API.","publish_date":"2021-03-03 20:15:00","link":"https://dropbox.tech/developers/new-paper-endpoints-released-in-preview","blog":{"id":"dropbox","link":"https://dropbox.tech/","name":"Dropbox","rssFeed":"https://dropbox.tech/feed/","type":"company"},"blogType":"company"},{"id":"50ba0d8b48afa9f5561400d1741bc0e4","publish_timestamp":1612890000,"title":"How we sped up Dropbox Android app startup by 30%","blogName":"Dropbox","image":"https://aem.dropbox.com/cms/content/dam/dropbox/tech-blog/en-us/2021/02/android-app-startup/diagrams/Techblog-AndroidAppStartup-Social.png","categories":["Android","Performance"],"description":"\n\n\n\n    \n    \nApplication startup is the first thing our users experience after they install an app and then again every single time they launch it. A simple and snappy application brings users a lot more joy than an application that has a ton of features but takes an eternity to load. Realizing this, Dropbox Android team has invested in measuring, identifying, and fixing the issues that were affecting our app startup time. We ended up improving our app start time by 30%, and this is the story of how we did it.\n\n\n\n\n     Scary climb\n\n\n\nHistorically at Dropbox we have been tracking app start by measuring how long it took from the moment a user taps our app icon up until the moment the app was fully loaded and ready for user interactions. \nWe abstracted away the measurement for app initialization in the following way:\n\n\n\n\n\n\n\n\n\n\n    \n        Copy\n    \n    perfMonitor.startScenario(AppColdLaunchScenario.INSTANCE)\n\n// perform the work needed for launching the application\n\nperfMonitor.stopScenario(AppColdLaunchScenario.INSTANCE)\n\n\n\n\n\n\nBeing a data-driven team, we have been tracking and monitoring the app initialization with the help of graphs accessible to all engineers. The graph below shows app startup p90 measurements from late March to early April 2020.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            \n        \n    \n\n\nAs you can see in the graph, the app startup time does not seem to be changing that much. Small fluctuations in app startup that range a couple milliseconds are expected in an application used by millions of users with variety of devices and OS versions. \nHowever, when we looked at the app startup time from December 2019 to April 2020, we saw a different picture. The app startup time has been creeping up for months as the app evolved and more features got added. However, because our charts were only showing us changes over a two week period, we missed the slow creep of app startup time that was happening over several months.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            \n        \n    \n\n\nWhile the discovery of the slowly increasing startup time was accidental, it caused us to dig deeper into the reasons why, as well as make sure that our monitoring, alerting tools, and charts were taking a broader, more cohesive look at our metrics. \n\n\n\n\n     Give me more numbers\n\n\n\nSome features in the application can be relatively simple to measure, such as how long an API call to the server takes. While other features, such as app startup, are a lot more complicated to measure. App startup requires many different steps on the device before the main app screen is shown to the user and the application is ready for user interaction. Examining our initialization code, we identified the main events that happen during our app initialization. Here are a few of them:\n\nRun migrations\nLoad application services\nLoad initial users\n\nWe started our investigation by leveraging the profiling tools in Android Studio to measure performance on our test phones. The problem with profiling the performance with this approach was that our test phones would not give us a statistically significant sampling of how well the app startup is actually doing. Dropbox Android application has over 1 billion installs on Google Play Store, spanning multiple types of devices, some of them old Nexus 5s, and others the newest and greatest Google devices. It would be a fool’s errand to try and profile that many configurations. So we decided to measure the different steps of app startup using scenarios and scenario steps in production.\nHere is the updated initialization code where we added logging for the three aforementioned steps: \n\n\n\n\n\n\n\n\n\n\n    \n        Copy\n    \n    perfMonitor.startScenario(AppColdLaunchScenario.INSTANCE)\nperfMonitor.startRecordStep(RunMigrationsStep.INSTANCE)\n\n// perform migrations step wrok\n\nperfMonitor.startRecordStep(LoadAppServicesStep.INSTANCE)\n\n// load application services step\n\nperfMonitor.startRecordStep(LoadInitialUsers.INSTANCE)\n\n// perform initial user loading step\n\nperfMonitor.stopScenario(AppColdLaunchScenario.INSTANCE)\n\n\n\n\n\n\nWith the measurements instrumented in code, we were able to understand what steps in the app’s initialization were contributing the most to the app startup increase.\n\n\n\n\n     Performance offenders we found\n\n\n\nThe below graph shows the overall app startup time from January to October 2020.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            \n        \n    \n\n\nIn May, we introduced measurements for each of the major steps that happen during application initialization. These measurements allowed us to identify and address the largest offenders to the performance of the app startup. \n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            \n        \n    \n\n\nThe major app startup offenders included Firebase Performance library initialization, feature flag migration, and initial user loading.\nFirebase Performance Library\nFirebase Performance library is included in Google’s Firebase suite of products to measure and send metrics about the performance of the apps. It provides helpful functionality such as metrics for individual method performance as well as infrastructure to monitor and visualize the performance of different parts of the app. Unfortunately, Firebase Performance library also comes with some hidden costs. Among them are an expensive initialization process and as a result a significant increase in the build times. In our debugging, we discovered that Firebase suite initialization was seven times longer when Firebase Performance tool was enabled. To fix the performance issue, we chose to remove the Firebase Performance tool from the Android Dropbox application. Firebase Performance library is a wonderful tool that allows you to profile very low level performance details such as individual function calls, but since we have not been using these features, we decided that fast app startup outweighed the individual method performance data, and so we removed the reference to that library.\nMigrations\nThere are several internal migrations that run each time the Dropbox application is launched. These can include updating feature flags, database migrations, etc. Unfortunately, we found that some of these migrations were running at every app launch. We didn’t notice the bad performance of these migrations previously, because the application launched quickly on development and test devices. Unfortunately, this migration code performed especially poorly on older versions of the OS and older devices, contributing to increased startup time. To resolve the issue, we began by investigating which migrations were essential on every launch and which migrations could now be removed entirely from the app. We found and removed at least one migration that was very old and therefore no longer needed, helping get our app start time back on track. \nUser loading\n\nIn the legacy part of our application, we store Dropbox user contacts metadata on the device as JSON blobs. In an ideal world, those JSON blobs should be read and converted into Java objects only once. Unfortunately, the code to extract users was getting called multiple times from different legacy features of the app, and each time, the code would perform expensive JSON parsing to convert user JSON blobs into Java objects. Storing user contacts as JSON was a very outdated design and a part of our monolith legacy code. To get an immediate fix for this issue, we added functionality to cache the parsed user objects during initialization. As we continue working on breaking down the monolith legacy code, a more efficient and modern solution for user contact storage would be to use Room database objects and convert those objects to business entities.\n\n\n\n\n     What now?\n\n\n\nAs a result of removing reference to Firebase Performance, removing expensive migration steps, and caching user loading, we improved the app launch performance of the Dropbox Android application by 30%. Through this work, we have also put together dashboards that will help prevent degradation of the app startup time in the future. \nWe also adopted several practices that will hopefully prevent us from making the same performance mistakes. Here are a couple of them:\nMeasure app startup performance impact when adding third party libraries\n With the discovery of how much Firebase Performance library degraded our app startup, we introduced a process for adding third party libraries. Today, we require measuring the application startup time, build time and the APK size of the application before the library can be added and used in our codebase. \nCache all the things\n Since two of the major offenders to the app startup performance had to do with caching, we prefer to cache expensive computations even if it does make the code a bit more complicated. After all, a better user experience is worth a little extra maintenance. \n \n\n\n\n\n    Conclusion\n\n\n\nWith more investments into credible analytics and performance measuring, we have become a lot more data-driven as a team which allows us to make larger investments into our code base, such as deprecating legacy C++ code, with a lot more confidence. Today, we monitor several dashboards that that provide us insights on performance of the most critical parts of our applications, and have processes in place to ensure that Dropbox applications continue to be snappy and a delight to our users.\nThanks to Amanda Adams, Angella Derington, Anthony Kosner, Chris Mitchell, David Chang, Eyal Guthmann, Israel Ferrer Camacho, and Mike Nakhimovich for comments and review. \n\n\n\n    \n\n","publish_date":"2021-02-09 17:00:00","link":"https://dropbox.tech/mobile/how-we-sped-up-dropbox-android-app-startup-by-30-","blog":{"id":"dropbox","link":"https://dropbox.tech/","name":"Dropbox","rssFeed":"https://dropbox.tech/feed/","type":"company"},"blogType":"company"},{"id":"ca20ebe436fcbf5c2fe8092c31d89428","publish_timestamp":1611770400,"title":"Cannes: How ML saves us $1.7M a year on document previews","blogName":"Dropbox","image":"https://aem.dropbox.com/cms/content/dam/dropbox/tech-blog/en-us/2021/01/cannes/Techblog-Cannes-Social.png","categories":["Machine Intelligence","Infrastructure","Previews","cost optimization"],"description":"\n\n\n\n    \n    \nRecently, we translated the predictive power of machine learning (ML) into $1.7 million a year in infrastructure cost savings by optimizing how Dropbox generates and caches document previews. Machine learning at Dropbox already powers commonly-used features such as search, file and folder suggestions, and OCR in document scanning. While not all our ML applications are directly visible to the user, they still drive business impact in other ways. \n\n\n\n\n     What are Previews?\n\n\n\nThe Dropbox Previews feature allows users to view a file without downloading the content. In addition to thumbnail previews, Dropbox also offers an interactive Previews surface with sharing and collaborating capabilities, including comments and tagging other users.\nOur internal system for securely generating file previews, Riviera, handles preview generation for the hundreds of supported file types. It does this by chaining together various content transformation operations to create the preview assets appropriate for that file type. For example, Riviera might rasterize a page from a multi-page PDF document to show a high-resolution preview on the Dropbox web surface. The full-content preview feature supports interactions such as commenting and sharing. The large image assets might later be converted into image thumbnails that will be shown to the user in a variety of contexts, including search results or the file browser.\nAt Dropbox scale, Riviera processes tens of petabytes of data each day. To speed up the Previews experience for certain classes of large files, Riviera pre-generates and caches preview assets (a process we call pre-warming). The CPU and storage costs of pre-warming are considerable for the volume of files we support.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            Thumbnail previews when browsing files. Previews can be enlarged and interacted with as a proxy for the application file.\n\n        \n    \n\n\nWe saw an opportunity to reduce this expense with machine learning because some of that pre-generated content was never viewed. If we could effectively predict whether a preview will be used or not, we would save on compute and storage by only pre-warming files that we are confident will be viewed. We dubbed this project Cannes, after the famous city on the French Riviera where international films are previewed.\n\n\n\n\n     Tradeoffs in machine learning\n\n\n\nThere were two tradeoffs that shaped our guiding principles for preview optimization.\nThe first challenge was to negotiate the cost-benefit tradeoff of infrastructure savings with ML. Pre-warming fewer files saves money—and who doesn’t like that!—but reject a file incorrectly and the user experience suffers. When a cache miss happens, Riviera needs to generate the preview on the fly while the user is waiting for the result to appear. We worked with the Previews team to develop a guardrail against degrading user experience, and used the guardrail to tune a model that would provide a reasonable amount of savings.\nThe other tradeoff was complexity and model performance vs. interpretability and cost of deployment. In general, there is a complexity vs. interpretability tradeoff in ML: more complex models usually have more accurate predictions at the cost of less interpretability of why certain predictions are made, as well as possibly increased complexity in deployment. For the first iteration, we aimed to deliver an interpretable ML solution as quickly as possible.\nSince Cannes was a new application of ML built into an existing system, favoring a simpler, more interpretable model let us focus on getting the model serving, metrics, and reporting pieces right before we added more complexity. Should something go wrong or we surface unexpected behavior in Riviera, the ML team could also more easily debug and understand whether the cause was Cannes or something else. The solution needed to be relatively easy and low cost to deploy for nearly half a billion requests per day. The current system was simply pre-warming all previewable files, so any improvement on this would result in savings—and the sooner the better!\n\n\n\n\n     Cannes v1\n\n\n\nWith these tradeoffs in mind, we targeted a simple, fast-to-train, and explainable model for Cannes. The v1 model was a gradient-boosted classifier trained on input features including file extension, the type of Dropbox account the file was stored in, and most recent 30 days of activity in that account. On an offline holdout set, we found this model could predict previews up to 60 days after time of pre-warm with &gt;70% accuracy. The model rejected about 40% of pre-warm requests in the holdout, and performance was within the guardrail metrics we set for ourselves at the onset. There were a small number of false negatives (files that we predicted would not be viewed, but did end up being viewed in the subsequent 60 days), which would cause us to pay the cost of generating preview assets on the fly. We used the “percentage-rejected” metric minus the false negatives to ballpark the $1.7 million total annual savings.\nEven before we explored the Previews optimization space, we wanted to ensure the potential savings outweighed the cost of building an ML solution. We had a ballpark estimate of the projected savings we wanted to target with Cannes. Designing and deploying ML systems in large, distributed systems means accepting some changes to the system will impact your estimates over time. By keeping the initial model simpler, we hoped the order of magnitude of the cost impact would remain worthwhile even if there are small changes to adjacent systems over time. Analyzing the trained model gave us a better idea of what we actually would save in v1 and confirmed the investment was still worthwhile.\nWe conducted an A/B test of the model on a random 1% sample of Dropbox traffic using our internal feature gating service, Stormcrow. We validated that model accuracy and pre-warms “saved” were in line with our results from offline analysis—which was good news! Because Cannes v1 no longer pre-warms all eligible files, we expected the cache hit rate to drop; during the experiment, we observed a cache hit rate a couple percentage points lower than the holdout population from the A/B test. Despite this drop, overall preview latency remained largely unchanged.\nWe were especially interested in tail latency (latency for requests above the 90th percentile), because cache misses that contributed to higher tail latency would more severely impact users of the Previews feature. It was encouraging that we did not observe a degradation to either preview tail latency or overall latency. The live test gave us some confidence to begin deploying the v1 model to more Dropbox traffic.\n\n\n\n\n     Live predictions at scale\n\n\n\nWe needed a way to serve real-time predictions to Riviera on whether to pre-warm a given file as files come through the pre-warming path. To solve this problem, we built Cannes as a prediction pipeline that fetches signals relevant to a file and feeds them into a model that predicts the probability of future previews being used.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            Cannes architecture\n\n        \n    \n\n\n\nReceive file id from Riviera pre-warm path. Riviera collects all file ids eligible for pre-warm. (Riviera can preview ~98% of files stored on Dropbox. There are a small number of files that are not a supported file type or otherwise cannot be previewed.) Riviera sends a prediction request with the file id we need a prediction for and the  file type.\nRetrieve live signals. To collect the most recent activity signals for a file at prediction time, we use an internal service named the Suggest Backend. This service validates the prediction request, then queries for the appropriate signals relevant to that file. Signals are stored in either Edgestore (Dropbox’s primary metadata storage system) or the User Profile Service (a RocksDB data store which aggregates Dropbox activity signals).\nEncode signals into feature vector. The collected signals are sent to the Predict Service, which encodes the raw signals into a feature vector representing all relevant information for the file, then sends this vector to a model for evaluation.\nGenerate a prediction. The model uses the feature vector to return a predicted probability that the file preview will be used. This prediction is then sent back to Riviera, which pre-warms files likely to be previewed up to 60 days in the future. \nLog information about request. Suggest Backend logs the feature vector, prediction results, and request stats—critical information for troubleshooting performance degradation and latency issues.\n\nAdditional consideration\n Reducing prediction latency is important because the pipeline above is on the critical path for Riviera’s pre-warming functionality. For example, when rolling out to 25% of traffic, we observed edge cases that decreased Suggest Backend availability to below our internal SLAs. Further profiling showed that these cases were timing out on step 3. We improved the feature encoding step and added several other optimizations to the prediction path, bringing tail latency down for those edge cases.\n \n\n\n\n\n     Operationalizing ML\n\n\n\nDuring the rollout process and beyond, we emphasized stability and making sure not to negatively impact the customer experience on the Previews surface. Close monitoring and alerting on multiple levels are critical components of the ML deployment process. \nCannes v1 metrics\nPrediction serving infra metrics: Shared systems have their own internal SLAs around uptime and availability. We rely on existing tools like Grafana for real-time monitoring and alerts. Metrics include:\n\nAvailability of Suggest Backend and Predict Service\nData freshness of User Profile Service (our activity data store)\n\nPreview metrics: We have key metrics for preview performance—namely, preview latency distribution. We left a 3% holdout for comparing previews metrics with and without Cannes, guarding against model drift or unanticipated system changes that could degrade model performance. Grafana is also a common solution for application-level metrics. Metrics include:\n\nPreview latency distribution (Cannes vs non-Cannes holdout), with extra attention to latency above p90\nCache hit rate (Cannes vs non-Cannes holdout): total cache hits/total requests to preview content\n\nModel performance metrics: We have model metrics for Cannes v1 that the ML team consumes. We built our own pipeline for calculating these metrics. Metrics of interest include:\n\nConfusion matrix, with extra attention to changes in rate of false negatives\nArea under ROC curve: While we directly monitor the confusion matrix stats, we also calculate an AUROC with an eye towards using it to compare to performance of future models.\n\nThe model performance metrics above are calculated hourly and stored in Hive. We use Superset for visualizing important metrics and creating a live dashboard of Cannes performance over time. Superset alerts built off the metrics tables proactively let us know when underlying model behavior has changed, hopefully well in advance of any client-facing impact. \nHowever, monitoring and alerting alone are insufficient for ensuring system health; establishing clear ownership and escalation processes is also necessary. For instance, we documented specific upstream dependencies of ML systems that could impact the results of the model. We also created a runbook for the on-call engineer which details steps for troubleshooting whether the issue is within Cannes or another part of the system, and a path of escalation if the root cause is the ML model. Close collaboration between ML and non-ML teams thus helps ensure Cannes continues to run smoothly.\n\n\n\n\n     Current state and future exploration\n\n\n\nCannes is now deployed to almost all Dropbox traffic. As a result, we replaced an estimated $1.7 million in annual pre-warm costs with $9,000 in ML infrastructure per year (primarily from increased traffic to Suggest Backend and Predict Service).\nThere are many exciting avenues to explore for the next iteration of this project. There are more complex model types we can experiment with now that the rest of the Cannes system is in production. We can also develop a more fine-tuned cost function for the model based on more detailed internal expense and usage data. Another new Previews application we’ve discussed is using ML to make predictive decisions more granular than a binary prewarm/don’t-prewarm per file. We may be able to realize further savings by being more creative with predictive prewarming, reducing costs with no deterioration to the file preview experience from the user’s perspective. \nWe hope to generalize the lessons and tools built for Cannes to other infrastructure efforts at Dropbox. ML for infrastructure optimization is an exciting area of investment.\nThanks to the Previews and ML Platform teams for their partnership on Cannes. In particular, kudos to Zena Hira, Jongmin Baek, Jason Briceno, Neeraj Kumar, and Kris Concepcion on the ML team; Anagha Mudigonda, Daniel Wagner and Robert Halas on the Previews team; and Ian Baker, Sean Chang, Aditya Jayaraman, and Mike Loh on the ML Platform team.\nAbout Us: The Intelligence team at Dropbox uses machine learning (ML) to drive outsized business and user value by leveraging a high fidelity understanding of users, content, and context. We work closely with other product and engineering teams to deliver innovative solutions and features. See open positions at Dropbox here! \n\n\n\n    \n\n","publish_date":"2021-01-27 18:00:00","link":"https://dropbox.tech/machine-learning/cannes--how-ml-saves-us--1-7m-a-year-on-document-previews","blog":{"id":"dropbox","link":"https://dropbox.tech/","name":"Dropbox","rssFeed":"https://dropbox.tech/feed/","type":"company"},"blogType":"company"},{"id":"f5a54b2b9a0ab0a83fd904aea7d17f39","publish_timestamp":1611064800,"title":"Why we chose Apache Superset as our data exploration platform","blogName":"Dropbox","image":"https://aem.dropbox.com/cms/content/dam/dropbox/tech-blog/en-us/2021/01/superset/Techblog-ApacheSuperset-Social.png","categories":["analytics","superset","data science","business intelligence","Standard Tags","data visualization"],"description":"\n\n\n\n    \n    \nToday the Apache Software Foundation announced Apache Superset as one of its official top-level projects. Apache Superset is a modern, open source data exploration and visualization platform already in use at Airbnb, American Express, Lyft, Nielsen, Rakuten Viki, Twitter, and Udemy among others.\nI worked on Apache Superset at Airbnb in its early days. When I came to Dropbox in late 2018, I got to continue working on it as a side project. Eventually, the company decided it was important enough to our internal needs to put our full attention to incorporating Apache Superset as our data exploration platform. We’ve been using it in production since early 2020, expanding its use until it became our main data exploration tool.\nOur choice of Apache Superset wasn’t cut and dry, though. I’ll explain what alternatives we looked at, and why we chose Superset. Of course I think it’s great, but for your own purposes it’s important to choose the tool that solves the most important real problems you have.\n\n\n\n\n    Problems we started with\n\n\n\nAs many companies of our size have done, we had built a number of internal tools to help engineers, analysts, and business stakeholders with their analytics needs. At one point we had more than 10 different data visualization solutions in order to:\n\nMonitoring and ensuring uptime\nPerforming migrations as needed\nPiping data between systems\nEnsuring correct access controls\nData governance\nOnboarding users into these systems\nProviding on-demand user support\n\nIn late 2019, we decided to consolidate our data exploration tooling and introduce a single solution. For our needs, it would need to be able to:\n\nTransform SQL queries into charts with minimal friction\nEnable quick data exploration for ad-hoc analysis, without needing to design a dataset first\nAllow users to create charts and dashboards that could be shared with others\nEncourage re-use of queries through customizable macros\nConsume data from our centralized data store (Hive and Presto clusters on top of S3 data)\n\n\n\n\n\n     Evaluating data exploration tools\n\n\n\nMost of our dashboarding and data visualization needs could be classified into three buckets:\n\nBusiness analytics: exploration and investigation of past business performance to gain insights, identify trends, evaluate ideas and experiments, and size up opportunities.\nOperational analytics: investigation of our operational systems, especially software ones (software reliability, trends, debugging, etc).\nAutomation of repetitive tasks: as a data-driven company, there was a lot of demand for report automation—monitoring system health, reviewing team metrics, measuring OKRs, etc.\n\nTo arrive at the right tool, we prioritized the following capabilities in descending order of importance:\n\nSecurity: Data visualization tools have access to sensitive data. We need first and foremost to ensure that data access can be governed and audited, and that we are following the best practices of the Dropbox security team.\nUser friendliness: A shallow learning curve, good documentation, and good support were our top priority after security, so that insights from data can be generated quickly. A steep learning curve has historically been one of the major adoption blockers of any data visualization or dashboarding tool we’ve deployed.\nMaintainability: Minimize the maintenance cost of the system, which breaks down to two key factors:\nEnd users should have minimal overhead to maintain their charts and dashboards.\nThe Data Platform team should be able to ensure continuous support of the tool and its development.\n\n\nFlexibility and extensibility: Seamless integration into the ecosystem of data infrastructure tools and adaptability to future needs is a plus, but not as important as user friendliness and maintainability for us.\n\nWhile data processing and data visualization are critical to our business, they are neither our core competency, nor are they a competitive advantage themselves. Building best-in-class tools requires a lot of long-term investment and has proven to be a challenging task.\nOur general philosophy is to buy a solution that satisfies our need whenever possible, or leverage an open source initiative. We will build a tool in-house only if neither of the first two options are possible. \nWith all this in mind, we created a list of properties we were looking for in a data exploration tool:\n\nPreference to buy a solution or leverage open source rather than build\nMinimize the number of solutions needed to cover our internal use cases\nOptimize for ease of iteration and dashboard creation\nEncourage the right behavior and ETL best practices in data quality\nMaintain a clear delineation between visualization and data processing\n\n\nStay as close as possible to the source of truth\nDelegate computation to the database engine\nMinimize the number of intermediate layers or configurations needed to build a chart \n\n\n\n\n\n\n\n     Why we chose Superset\n\n\n\nAs our table below shows, no one option is the best at everything. They all have unique strengths. Periscope (recently renamed to Sisense) allows drag-and-drop interfaces and works with any git server. Mode’s implementation of topics and cross-topic searches is impressive. Redash has alerts today which Superset has only begun to develop, plus a user activity log not found in some other solutions. Metabase is also worth considering, but we ruled it out for Dropbox because it is written in Clojure, which is neither adopted internally within Dropbox engineering, nor supported by our current infrastructure.\nFor us, Superset offered abilities that mattered to our internal users, who want to get answers without writing SQL and for whom time and effort creating visualizations needs to be minimal. The ability to create reusable virtual table columns and metrics that are shared across teams is a big plus for them. Access control list granularity helped us meet the high security expectations we have at Dropbox, streamline data access and cross-team sharing, and minimize support and administration work. Superset’s API for creating charts powers a couple of internal tools and has saved engineering time when building custom visualizations. And in daily use for busy people, a progress bar is more helpful than you might realize.\nIn short, we chose Apache Superset because it was the best match to our specific internal needs. We were ready to invest engineering effort to close the gaps wherever needed. \nWe also enjoyed contributing the following features back to the Superset project:\n\nAlert improvements\nSchema permissions model\nSlack integration\nPresto and Hive support in CI\nAnd more!\n\nAs part of our decision-making process, we created this table which lists the key properties of the leading contenders. We’re happy to share it to help you make your own decisions.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            View or download the full-size comparison matrix here.\n\n        \n    \n\n\n\n    Results\n\n\n\nSix months after we bet on Superset, it’s clearly been a big success at Dropbox. Superset had quick and strong adoption across the organization, and is now the main data exploration tool for our data warehouse. It has helped a number of teams to improve their workflows—so much so that they agreed to be quoted:\nIt's been a game changer for us. It is much less friction than the previous flow, and it makes it easy to add new metrics. We'll be able to get much more visibility into how the various aspects of the sync engine are working, which will help us detect issues earlier and minimize their impact on our customers. —Core Sync Team\nSuperset is great, the speed of queries is astounding! It has been a big upgrade for the team over our legacy tools and the dashboards are much easier to build.  —Product Analytics\nTheir praise is quantified by a chart of Superset’s Weekly Average Users at Dropbox following rollout:\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            Superset Weekly Average Users trend following initial rollout (top), overall users and content created (bottom)\n\n        \n    \n\n\n\n     Choose the best solution for your own biggest problems\n\n\n\nI’m proud of what we’ve built with Superset, but you should research and decide what your own priorities are before committing to a platform. In our case, user-friendliness was more important than flexibility and extensibility. Apache Superset stood out to us in ease of user adoption, yet was flexible enough to meet our needs. \nWe’re lucky to be living in a time where there are a number of great solutions on the market that can provide powerful data exploration capabilities to your organization. We hope this post helps you make your own best decision.\n\n\n\n\n    \n\n","publish_date":"2021-01-19 14:00:00","link":"https://dropbox.tech/application/why-we-chose-apache-superset-as-our-data-exploration-platform","blog":{"id":"dropbox","link":"https://dropbox.tech/","name":"Dropbox","rssFeed":"https://dropbox.tech/feed/","type":"company"},"blogType":"company"},{"id":"02c0bbe863cd31b8f8d74c382308a396","publish_timestamp":1609878600,"title":"Lessons learned in incident management","blogName":"Dropbox","image":"https://aem.dropbox.com/cms/content/dam/dropbox/tech-blog/en-us/2021/01/incident-management/diagrams/Techblog-IncidentManagement-Social.png","categories":["sre","observability","incident management","reliability"],"description":"\n\n\n\n    \n    \nAt Dropbox, we view incident management as a central element of our reliability efforts. Though we also employ proactive techniques such as Chaos engineering, how we respond to incidents has a significant bearing on our users’ experience. Every minute counts for our users during a potential site outage or product issue.\nThe key components of our incident management process have been in place for several years, but we’ve also found constant opportunities to evolve in this area. The tweaks we’ve made over time include technological, organizational, and procedural improvements. \nThis post goes deeper into some of the lessons Dropbox has learned in incident management. You probably won’t find all of these in a textbook description of an incident command structure, and you shouldn’t view these improvements as a one-size-fits-all approach for every company. (Their usefulness will depend on your tech stack, org size, and other factors.) Instead, we hope this serves as a case study for how you can take a systematic view of your organization’s own incident response and evolve it to meet your users’ needs.\n\n\n\n\n    Background\n\n\n\nThe basic framework for managing incidents at Dropbox, which we call SEVs (as in SEVerity), is similar to the ones employed by many other SaaS companies. (For those who are less familiar with the topic, we recommend this series of tutorials by Dropbox alum Tammy Butow to get an overview.)\nAvailability SEVs, though by no means the only type of critical incident, are a useful slice to explore in more depth. No online service is immune to these incidents, and that includes Dropbox. Critical availability incidents are often the ones most disruptive to the largest number of users—just think about the last time your favorite website or SaaS app was down—so we find that these SEVs put the highest strain on the timeliness of our incident response. Success means shaving every possible minute from that response.\nNot only do we closely measure the impact time for our availability SEVs, but there are real business consequences for this metric. Every minute of impact means more unhappy users, increased churn, decreased signups, and reputation damage from social media and press coverage of an outage. Beyond this, Dropbox commits to an uptime SLA in contracts with some of our customers, particularly in mission-critical industries. We define this based on the overall availability of the systems that serve our users, and officially cross from “up” to “down” when our availability degrades past a certain threshold. To stay within our SLA of 99.9% uptime, we must limit any down periods to roughly 43 minutes total per month. We set the bar even higher for ourselves internally, targeting 99.95% (21 minutes per month).\nTo safeguard these targets, we have invested in a variety of incident prevention techniques. These include Chaos engineering, risk assessments, and systems to validate production requirements, to name a few. However, no matter how much we probe and work to understand our systems, SEVs will still happen. That is where incident management comes in.\n\n\n\n\n     The SEV process\n\n\n\nThe SEV process at Dropbox dictates how our various incident-response roles work together to mitigate a SEV, and what steps we should take to learn from the incident.\nEvery SEV at Dropbox has several basic features:\n\nA SEV type, which categorizes the incident’s impact; well-known examples include Availability, Durability, Security, and Feature Degradation\nA SEV level from 0-3, to indicate criticality; 0 is the most critical\nAn IMOC (Incident Manager On Call), who is responsible for spearheading a speedy mitigation, coordinating SEV respondents, and communicating the status of the incident\nA TLOC (Tech Lead On Call), who drives the investigation and makes technical decisions\n\nSEVs with customer impact also include a BMOC (Business Manager On Call), who manages non-engineering functions to provide external updates where needed. Depending on the scenario, this might include status page updates, direct customer communication, and in rare cases, regulatory notifications.\nDropbox has built its own incident management tool, which we call DropSEV. Any Dropboxer can declare a SEV, which triggers the assignment of the roles above and creates a set of communication channels to handle the incident. These include a Slack channel for real-time collaboration, an email thread for broader updates, a Jira ticket to collect artifacts and data points, and a pre-populated postmortem document to be used for retrospection later. Employees can also subscribe to a specific Slack channel or email list to see the stream of new SEVs that are created. Here is a sample DropSEV entry to declare a minor availability incident for our mobile app.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            \n        \n    \n\n\nAfter a user creates an incident in DropSEV, the SEV process dictates the basic stages it will follow.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            \n        \n    \n\n\nThough there is a lot of nuance around that latter stage of generating a valuable postmortem with effective action items—see “Postmortem Culture” in the Google SRE Workbook—this post will focus more on the period before a SEV is mitigated, while the user experience is still impacted. To simplify, we break this up into three overall phases:\n\nDetection: The time it takes to identify an issue and alert a responder\nDiagnosis: The time it takes for responders to root-cause an issue, and/or identify a resolution approach\nRecovery: The time it takes to mitigate an issue for users once we have a resolution approach\n\nRemember our 21-minute max target for downtime in a month? That’s not a lot of time to detect, diagnose, and recover from a complex technical issue. We found that we had to optimize all three phases to make it happen.\n\n\n\n\n     Detection\n\n\n\nThe time it takes to identify an issue and alert a responder\nA reliable and efficient monitoring system\n When it comes to detecting issues at Dropbox, our monitoring systems are a key component. Over the years we’ve built and refined several systems that engineers rely on during an incident. First and foremost is Vortex, our sever-side metrics and alerting system. Vortex provides an ingestion latency on the order of seconds, a 10 second sampling rate, and a simple interface for services to define their own alerts.\n\nThese features are key to driving down the time-to-detection for issues in production. In a previous blog post we unboxed the technical design, which is also shown below.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            An overview of the Vortex architecture. Check out our detailed blog post for more on Vortex.\n\n        \n    \n\n\nThe 2018 redesign of this system was foundational for our reliability efforts. To understand why, think about that 21-minute internal target for monthly downtime. We needed to know that within tens of seconds of an incident beginning, Vortex would pick up these signals and alert the right responders via PagerDuty. If Vortex were unreliable or slow, our response would be hindered before it even began.\nYour organization may not use a homegrown monitoring system like ours, but ask yourself: just how quickly will its signals trigger your incident response?\nOptimizing metrics and alerts\n Vortex is key in quickly alerting on issues, but it is useless without well-defined metrics to alert on. In a lot of ways this is a hard problem to solve generally, since there will always be use case-specific metrics that teams will need to add themselves.\nWe’ve tried to lessen the burden on service owners by providing a rich set of service, runtime, and host metrics that come for free. These metrics are baked into our RPC framework Courier, and our host-level infrastructure. In addition to a slew of standard metrics, Courier also provides distributed tracing and profiling to further aid in incident triage. Courier provides the same set of metrics in all languages we use at Dropbox (Go, Python, Rust, C++, Java).\nThough these out-of-the-box metrics are invaluable, noisy alerts are also a common challenge, and often make it hard to know if a page is for a real problem. We provide several tools to help alleviate this. The most powerful is an alert dependency system with which service owners can tie their alerts to other alerts, and silence a page if the problem is in some common dependency. This allows teams to avoid getting paged for issues that are not actionable by them, and act on true issues more quickly.\nTaking the human element out of filing incidents\n Our teams receive a wide variety of alerts from PagerDuty about the health of their systems, but not all of these are “SEV-worthy.” Historically this meant that a Dropbox first responder receiving a page had to worry not only about fixing the issue, but whether it was worthy of filing a SEV and kicking off the formal incident management process.\nThis distinction could be confusing, especially for those who had less experience as an on-call for their team. To make the decision simpler, we revamped DropSEV (the incident management tool described earlier) to surface SEV definitions directly to users. For example, if you selected “Availability” as a SEV type in DropSEV, it would pop up a table like this one to map the degree of global availability impact to a SEV level.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            \n        \n    \n\n\nThis was a step forward, but we realized the “do I file a SEV?” decision was still slowing down our responders. Let’s say you were on-call for the frontend component of Magic Pocket, our in-house multi-exabyte storage system, which is responsible for handling Get and Put requests for data blocks. When you received a page, you’d have to ask yourself a series of questions:\n\nIs availability dipping?\nBy how much?\nIs this having an upstream impact on global availability? (Where’s the dashboard for that again?)\nBy how much? How does that line up with the SEV table?\n\nThat is not a smooth procedure, unless you are a highly trained on-call and you’ve seen your fair share of SEVs before. Even in the best case, in an outage it would cost us a couple minutes out of that critical 21-minute budget. A couple times SEVs weren’t even filed, meaning we missed out on involving the IMOC and BMOC while the technical team worked to restore availability.\nSo this summer, we began automatically filing all availability SEVs. A service owner will still receive their own system alerts, but DropSEV will detect SEV-worthy availability impact and automatically kick off the formal incident response process. Service owners no longer have to distract themselves with filing the SEV, and we have higher confidence that all incident response roles will be involved. And critically, we shave minutes off of the response for every availability SEV.\nWhere can you short-circuit human decision-making in your own incident response flow?\n\n\n\n\n     Diagnosis\n\n\n\nThe time it takes for responders to root-cause an issue, and/or identify a resolution approach\nCommon on-call standards\n During the Diagnosis phase we often need to pull in additional responders to help beyond the initial recipients of an alert. We’ve built a “Page the on-call” button into our internal service directory, so a human can efficiently reach another team if needed. This is another way we’ve used PagerDuty for many years.\n \n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            Our technical service directory includes a reference to each team’s on-call, and a button to quickly page them if needed in an emergency.\n\n        \n    \n\n\nHowever, we found a critical variable which made it impossible to consistently keep our outages below 21 minutes: How is a team’s PagerDuty setup configured? Over time, well-intentioned teams at Dropbox made very different decisions on questions like these:\n\nHow many layers does our escalation policy need?\nHow much time should there be between each escalation? Across the entire escalation chain?\nHow should our on-calls receive PagerDuty notifications? Do they need push, SMS, or phone calls set up, or a combination?\n\nAfter grappling with these inconsistencies in a few SEVs, we instituted a set of on-call checks which evaluated each team’s PagerDuty setup against common guidelines. We built an internal service that queries the PagerDuty API, runs our desired checking logic, and contacts teams about any violations.\nWe made the hard decision to enforce these checks strictly, with zero exceptions. This was a challenging shift to make, since teams had gotten used to some flexibility, but having consistent answers to the questions above unlocked much more predictability in our incident response. After the initial buildup it was easy to iteratively add additional checks, and we found ways to make our initial set more nuanced (e.g. different standards depending on the criticality of your team’s services). In turn, your own on-call guidelines should be a function of the business requirements for incident response in your own organization.\nAs of this post’s publication, PagerDuty has released an On-Call Readiness Report (for some of their plans) which allows you to run some similar checks within their platform. Though the coverage is not identical to what Dropbox built internally, it may be a good place to start if you want to quickly create some consistency.\nTriage Dashboards\n For our most critical services, such as the application that drives dropbox.com, we’ve built a series of triage dashboards that collect all the high-level metrics and provide a series of paths to narrow the focus of an investigation. For these critical systems, reducing the time it takes to triage is a top priority. These dashboards have reduced the effort needed to go from a general availability page, to finding the system at fault.\nOut-of-the-box dashboards for common root causes\n Though no two incidents or backend services are identical, we know that certain data points are valuable to our incident responders time and time again. To name a few:\n\n\nClient- and server-side error rates\nRPC latency\nException trends\nQueries per second (QPS)\nOutlier hosts (e.g. those with higher error rates)\nTop clients\n\nTo shrink diagnosis time, we want these metrics to be available to every team when they need them most, so they don’t waste valuable minutes looking for the data that will point them to a root cause. To that end, we built an out-of-the-box dashboard that covers all of the above, and more. The build-up effort for a new service owner at Dropbox is zero, other than bookmarking the page. (We also encourage service owners to build more nuanced dashboards for their team-specific metrics.)\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            A segment of the Grafana-based Courier dashboard that service owners receive out-of-the-box. \n\n        \n    \n\n\nThe power of having a common platform like this is that you can easily iterate over time. Are we seeing a new pattern of root causes in our incidents? Great—we can add a panel to the common dashboard which surfaces that data. We’ve also invested in adding annotations in Grafana, which overlay key events (code pushes, DRTs, etc.) over the metrics to help engineers with correlation. Each of these iterations shrinks diagnosis time a little bit across the company.\nException Tracking\n One of the highest-signal tools Dropbox has for diagnosing issues is our exception tracking infrastructure. It allows any service at Dropbox to emit stack traces to a central store and tag them with useful metadata. The frontend allows developers to easily see and explore exception trends within their services. This ability to dive into exception data and analyze trends is super useful when diagnosing problems in our larger python applications.\n \n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            \n        \n    \n\n\nThe Incident Manager’s role: clearing distractions\n During an outage or other critical incident, a lot of stakeholders are invested in what is going on while the SEV team is diagnosing the issue.\n\n\nCustomer-facing teams need to provide updates and an ETA for resolution.\nService owners in the blast radius of the affected system are curious about the technical details.\nSenior leaders, who are accountable for reliability and business objectives, want to convey urgency.\nAnd senior engineering leaders in particular may want to roll up their sleeves and join the diagnosis efforts.\n\nTo make matters worse, the crosstalk in your incident Slack channels may have grown in 2020 as responders began working from home and could not collaborate in-person.\nAt Dropbox, we’ve seen all of the above unfold during our incidents. However, we’ve sometimes struggled with a key element of the IMOC’s role: to shield the SEV team from these distractions. Thanks to the training they received, IMOCs generally knew the SEV process, associated terminology and tooling, and the expectations for postmortems and incident reviews. But they did not always know how to drive a war room and optimize the efficiency of our SEV response. We heard feedback from our engineers that IMOCs were not providing them the front-line support they needed, and distractions were slowing them down from diagnosing issues. \nWe realized that we had not made those aspects of the IMOC role explicit, and the tribal knowledge of “what makes a good IMOC” had faded over time. A first step was updating our training to emphasize setting urgency, clearing distractions, and consolidating communication in one place. We are now working on game-like SEV scenarios where new IMOCs can actually practice these concepts before their first shift. Finally, we plan to increase the cadence of tabletop exercises involving a broader set of IMOCs, so the group can regularly evaluate its overall readiness.\nIn addition, we established a Backup Response Team of senior IMOCs and TLOCs that could be pulled in for our most severe incidents. We gave them a clear playbook to assess the state of the incident and determine with the existing IMOC/TLOC if they should be transferred ownership. Giving these senior players an explicit, well-known role when necessary made them a valuable support structure, not a set of extra voices in Slack.\nThe key lesson: pay attention to the delta between how your incident response process looks on paper, and how it works in practice.\n\n\n\n\n    Recovery\n\n\n\nThe time it takes to mitigate an issue for users once we have a resolution approach\nEnvision mitigation scenarios\n At first, it may seem challenging to find a silver bullet for the Recovery stage. How do you optimize this part when every incident calls for a different mitigation strategy? This requires a lot of insight into how your systems behave, and the steps your incident responders would have to take in worst-case (but feasible) scenarios.\nAs Dropbox pursued a 99.9% uptime SLA, we began running quarterly reliability risk assessments. This was a bottoms-up brainstorming process across our Infrastructure teams and others whose systems were likely to be involved in SEVs. Knowing we needed to optimize recovery times, we prompted the teams to focus on a simple question: “Which incident scenarios for your systems would take more than 20 minutes to mitigate?” \nThis surfaced a variety of theoretical incidents, with varying degrees of likelihood. If any of these occurred, we were unlikely to stay within our downtime target. As we rolled up the risk assessment outputs to Infrastructure leadership, we aligned on which ones were worth investing in, and several teams set out to eliminate these scenarios. For a few examples:\n\nPush time for our monolithic backend service was &gt; 20 minutes. The owners of the service optimized the deployment pipeline below 20 minutes, and began running regular DRTs to ensure that push time never degraded.\nPromoting standby database replicas could take &gt; 20 minutes, if we lost enough primary replicas during a single-rack failure. Our metadata team improved the rack diversity of our database systems, and shored up the tooling that handles database promotions.\nChanges to experiments and feature gates were hard to identify, and core teams could not roll back these changes in an emergency, making it likely it would take &gt; 20 minutes to resolve an experimentation-related issue. So our experimentation team improved visibility for changes, ensured all experiments and feature gates had a clear owner, and provided rollback capabilities and a playbook to our central on-calls.\n\nBy addressing these and other scenarios, we saw the number of lengthy availability incidents sharply decrease. We continue to use the “20-minute rule” as a measuring stick for new scenarios we uncover, and arguably should tighten this threshold over time.\nYou can adopt a similar methodology in your own organization or team. Write down the potential incidents that will take the longest to handle, stack-rank them by likelihood, and improve the ones at the top of the list. Then, test how the scenario would actually play out with DRTs or “wheel of misfortune” exercises for your incident responders. Did the improvements reduce recovery time to a level that’s acceptable for your business?\nHolding the line at 99.9\n With a tight SLA we don’t have a lot of time to react when things go wrong, but our tech stack is constantly changing underneath us. Where will the next risk pop up, and where do we focus our investments?\nIn addition to the process changes mentioned above, to help hold the line we’ve built up authoritative data sources in two areas:\n\nA single source of truth for our SLA, and what incidents have affected it\nA dashboard tracking the relative impact of services in the critical path\n\nHaving a single source of truth for our SLA makes it straightforward to plan around. This ensures there is no confusion across the organization for how each team’s contributions, such as internal SLAs, bubble up to our customer guarantees.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            Tracing is used to determine what is in the critical path\n\n        \n    \n\n\nWe also track the impact of services in the critical path using distributed tracing. The tracing infrastructure is used to calculate a weight for each service. This weight is an approximation of how important a given service is to Dropbox as a whole. When a service’s weight exceeds a threshold, additional requirements are enforced to ensure rigor in its operations.\nThese weights (and the automation around them) serve two purposes. The first is to act as another data point in our risk assessment process; knowing the relative importance of services lets us better understand how risks in different systems compare. The second is to ensure we aren’t caught by surprise when a service is added to the critical path. With a system the size of Dropbox it’s hard to keep track of every new service, so tracking the critical path automatically ensures we catch everything.\nEstimating user impact\n “How painful is this SEV for our customers?” Addressing this question won’t speed up recovery, but it will allow us to communicate proactively to users through the BMOC and Be Worthy of Trust, which is our first company value. Again, every minute counts for our users during an incident.\nThis question has proven particularly tricky for availability SEVs at Dropbox. You may have noticed above, our availability SEV level definitions start with a somewhat naive assumption that lower availability is more severe. This definition provided some simplicity as the company scaled up, but it has failed us in the long run. We’ve encountered SEVs with a steep availability hit but almost no customer impact; we’ve encountered minor availability hits that barely qualified as SEVs but rendered dropbox.com completely useless. The latter case was what kept us up at night, because we ran the risk of under-serving and under-communicating to our users.\nAs a tactical fix we zeroed in on our website, which has lower traffic than our desktop and mobile apps (meaning web-specific availability issues may stand out less in the numbers). We worked with teams across engineering to identify ~20 web routes which corresponded to key webpages and components. We started monitoring availability for these individual routes, adding these metrics to our dashboards, alerts, and SEV criteria. We trained IMOCs and BMOCs on how to interpret this data, map an availability dip to specific impact, and communicate to customers—and we practiced with a tabletop exercise. As a result, in subsequent incidents impacting our website, we quickly got a sense whether key user workflows were affected, and used this information to engage with customers.\nWe believe we still have work to do in this area. We’re exploring a variety of other ways we can pivot from measuring 9s to measuring customer experience, and we are excited by the technical challenges these will entail:\n\nCan we classify all of our routes, across platforms, into criticality-based buckets which we flag to the response team?\nHow do we use direct signals of customer impact (such as traffic to help and status pages, customer ticket inflow, and social media reaction) to rapidly trigger an engineering response?\nAt our scale, how do we efficiently estimate the number of unique users affected by an availability incident in real-time? How do we get more precise data on the populations affected—regions, SKUs, etc.—after the incident?\nWhere will we benefit most from client-side instrumentation, to measure customer experience end-to-end?\n\n\n\n\n\n     Continuous improvement\n\n\n\nDropbox is not perfect at incident management—no one is. We started with a SEV process that worked great for where Dropbox was, but as our organization, systems, and user base grew, we had to constantly evolve. The lessons we outlined in this post didn’t come for free; it often took a bad SEV or two for us to realize where our incident response was lacking.\nThat’s why it’s so critical to learn from your gaps in each incident review. You can’t fully prevent a critical incident from happening, but you can prevent the same contributing factor from coming back to bite you again.\nAt Dropbox, this starts with a blameless culture in our incident reviews. If a responder makes a mistake during an incident, we don’t blame them. We ask what guardrails our tools were missing against human error, what training we failed to give the responder to prepare them for this situation, and whether automation could take the responder out of the loop in the first place. This culture allows all parties to feel comfortable confronting the hard lessons a SEV teaches us.\nJust as we don’t lay blame on individuals for our SEVs, no single person can take credit for the improvements we’ve made to Dropbox’s incident management. It truly has taken an organization-wide effort over the last several years to incorporate these lessons. But to name a few, we’d like to thank current and past members of the Reliability Frameworks, Telemetry, Metadata, Application Services, and Experimentation teams for the topics covered in this blog post.\nIf you’re interested in helping us take incident management at Dropbox to the next level, we are hiring for Site Reliability Engineers. With Dropbox’s announcement that we are becoming a Virtual First company, location is flexible for these positions long-term. \n\n\n\n    \n\n","publish_date":"2021-01-05 20:30:00","link":"https://dropbox.tech/infrastructure/lessons-learned-in-incident-management","blog":{"id":"dropbox","link":"https://dropbox.tech/","name":"Dropbox","rssFeed":"https://dropbox.tech/feed/","type":"company"},"blogType":"company"},{"id":"c0346a806624daa47eee7fea6d647d82","publish_timestamp":1609870800,"title":"Dropbox Postman Collection for Team Admin Workflows","blogName":"Dropbox","image":"https://aem.dropbox.com/cms/content/dam/dropbox/tech-blog/en-us/2021/01/admin-workflows-collection/workflows-header-fixed.png","categories":["collection","Workflow","Tips and Tricks","Teams"],"description":"Use this Dropbox Postman Collection for managing Team Admin Workflows","publish_date":"2021-01-05 18:20:00","link":"https://dropbox.tech/developers/dropbox-postman-collection-for-team-admin-workflows","blog":{"id":"dropbox","link":"https://dropbox.tech/","name":"Dropbox","rssFeed":"https://dropbox.tech/feed/","type":"company"},"blogType":"company"},{"id":"99f9cf5690adcd31455630602df605b4","publish_timestamp":1608228000,"title":"Alki, or how we learned to stop worrying and love cold metadata","blogName":"Dropbox","image":"https://aem.dropbox.com/cms/content/dam/dropbox/tech-blog/en-us/2020/12/alki/diagrams/Techblog-Alki-Social.png","categories":["Metadata","Databases","Cold Storage","Edgestore"],"description":"\n\n\n\n    \n    \nIn this post, we introduce Alki, a new cost efficient petabyte-scale metadata store designed for storing cold, or infrequently accessed, metadata. We’ll discuss the motivations behind building it, its architecture, and various aspects of how we were able to rapidly prototype and then productionize the system with a small team.\n\n\n\n\n     Metadata Storage at Dropbox\n\n\n\nAt Dropbox, we store most of our product metadata in an in-house one-size-fits-all database called Edgestore. Over time, however, small misalignments in values between Edgestore and the various product use cases it powers became increasingly problematic. Its architectural limitations around data sharding and capacity expansion made storing certain types of metadata at scale infeasible.\nEdgestore is built on top of MySQL with data stored on SSDs. In order to scale, data is distributed among a fixed set of MySQL databases, referred to as shards. An Edgestore cluster serves a subset of those shards and is comprised of a primary host and a number of secondary replica hosts.\nCapacity expansion in Edgestore works by increasing the number of database clusters and redistributing the shards.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            Capacity expansion in Edgestore via splits\n\n        \n    \n\n\nAssuming data is relatively evenly distributed among the hosts, when one cluster hits 80% disk space and needs to be split, most other clusters will also need to be split. This means that capacity expansion most often occurs by splitting the entire fleet, doubling the physical cost of Edgestore each time.\nCapacity Crunch\nAround fall of 2017, we split Edgestore from 256 clusters with 8 shards per cluster to 512 clusters with 4 shards per cluster. This increased Edgestore’s physical footprint from around 1500 database hosts to more than 3000 database hosts after accounting for local and geo-diverse replicas.\nFrom our capacity projections at the time, we predicted the need to split yet again in under just 2 years. Increasing usage of Edgestore from various products, coupled with the growth of Dropbox itself, was causing the rate of splits to increase.\nThis was logistically problematic from a couple of perspectives. First, the financial cost for the exponentially larger and larger splits is a non-trivial amount of money to earmark. Second, buying and setting up so many machines at once is an all-hands-on-deck project for our hardware, datacenter, networking, and database teams and is hard to scale.\nArchitecturally, Edgestore would also only be able to perform 2 more splits, after which we would reach 1 shard per cluster with no more splits available. Our levers for capacity expansion were quickly closing and we were heading into a capacity crunch.\nOptions\nWe began to build a replacement metadata storage engine, called Panda, underneath Edgestore that would support incremental capacity expansion, allowing us to add just a handful of clusters at a time and dynamically move subsets of data to the new clusters, but we knew this would be a multi-engineer-year project to build, validate, and migrate.\nIn the meantime, we wanted to avoid the next round of splits, giving us roughly 2 years to solve Edgestore’s capacity crunch. We investigated and explored a number of options.\nAmong them was an option to optimize Edgestore’s existing data model, shaving off some bytes here and there on schema overhead. Perhaps we could variable-length encode some pieces of metadata, or remove others primarily used for debugging and etc. Many ideas like this were explored by the team, but we determined for most of the ideas that the risks would be too high and the maintainability of the system would be harmed in the long-run in return for not much capacity reduction.\nOur database team also looked into switching the storage engine we use with MySQL 5.6 from InnoDB to Facebook’s MyRocks storage engine built on RocksDB, which has better compression. We also looked into trying to upgrade MySQL to 5.7 to enable usage of hole punching. One other option that was explored was running MySQL atop zfs for better compression as well. But testing and safely rolling out MySQL, kernel, and filesystem changes across a large fleet would be hard to validate safely. With plenty of unknowns, we didn’t want this to become a blocker.\nUltimately, the path we took became apparent when we started to closely examine the data stored in Edgestore.\nAudit logs\nWe noticed that the largest dataset stored in Edgestore, was from audit logs for our customers. These logs power several features that give Dropbox Business account administrators visibility into how their Dropbox data is being used and shared. As an example, admins can search for events via the insights dashboards, subscribe to events via streams using the get_events API, or generate activity reports.\nThese log events are written and retained for many years, but outside of recently ingested events that are more frequently read randomly via the dashboard and get_events API, old events are rarely read and if read are usually done so in large batches of sequential objects. We refer to this as being hot, or frequently accessed, and cold, or infrequently accessed.\nEdgestore, optimized for low latency and high throughput reads of single objects, was a poor fit for this use case due to those largely cold traffic patterns. This was reflected in the high cost of storage from the underlying SSDs whose performance benefits were mostly un-reaped. Moreover, audit logs had been hitting write throughput limitations with Edgestore due to large teams generating proportionally large volumes of events that created too much write contention against MySQL.\nAs such, we decided to work together with the team managing audit logs to look for a better solution to the audit log storage problem.\nChallenge\nThe challenge when we set out to build Alki was to design, implement, and productionize a new optimized system with limited engineering resources in a short timeline. Beyond that, we wanted the system to be generic, benefiting many use cases besides audit logs, as well as to help reduce overall metadata storage costs at Dropbox.\nAmong the few off-the-shelf options we explored, we looked at running HBase or Cassandra on in-house machines equipped with spinning disks. Given a longer timeline or a larger team, it may have been possible to fit our use case onto these systems as is, but we ultimately ruled these out as we didn’t think we could gain the expertise and build the tooling to operate these at scale with high durability in the required amount of time. \nOne problem we ran into with many systems optimized for efficient storage of cold data was that they were unsuitable for high volumes of random reads and writes. While audit logs are eventually cold, they are ingested at high volume and frequently read randomly in the period right after ingestion which made those systems a poor fit for our use case.\nGiven this dichotomy, we decided to build a two-tier system with separate layers for hot storage and cold storage. Data when written would be stored in hot storage, allowing for efficient random writes and reads. Eventually the data would be transitioned to cold storage, allowing for low-cost storage and efficient sequential reads at the expense of random read performance.\n\n\n\n\n    Alki\n\n\n\nThis approach gave us the flexibility to combine systems optimized for opposing goals rather than trying to find a single system good at both. We based the structure of this new system on log-structured merge-tree (LSM tree) storage systems.\nLSM tree\nThis type of storage system is fairly popular and its concepts are used in many modern day storage systems like Cassandra, HBase, and BigTable.\nWrites\n In a non-distributed LSM tree database implementation, when a write request comes in, the new record is inserted into an in-memory sorted table. Implementation of this memory table varies, for example, it could be represented as a red-black binary search tree to allow for inserts and searches in logarithmic time.\n \n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            A key-value record [potato: vegetable] is inserted to our memory table in key order.\n\n        \n    \n\n\nHowever, memory is volatile and has smaller capacity relative to other storage mediums. It would not be scalable to store all data in in-memory tables.\nOffloads\n To limit the amount of data we keep in memory, we occasionally flush, or offload, the set of records out to disk, in a file, as a sorted list, or run, of records. For simplicity, we will refer to the data in the in-memory table as part of the in-memory run.\n \n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            A run of in-memory records are flushed out to a file on disk, containing the same records in the same sort order.\n\n        \n    \n\n\nReads\n This complicates the reads because records could be located in either the in-memory table or in any of the on-disk runs. Therefore we need to search in the in-memory table and in all on-disk runs and merge the results for each read request.\n \n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            An example set of on-disk runs, representing data from multiple in-memory offloads.\n\n        \n    \n\n\nAs an example, in order to find the key Blackberry with the above set of on-disk runs, we would need to search for that key in the runs for &lt;Apple,...Tomato&gt; and &lt;Banana,...Zucchini&gt; and merge the results. We do not need to check the &lt;Chard,...Watermelon&gt; run because we know the key falls outside of the run’s range of keys.\nIf the key appears in no on-disk runs nor in the in-memory table, we know the record does not exist. If it appears in multiple places, we may choose to discard all of the old references in favor of the newest entry when merging results, depending on implementation.\nEfficient lookups of a key in any single run, in-memory or on-disk, can be easily implemented because data in each run is sorted by key. On the other hand, as the number of runs increase, the number of runs to read from for each read request also increase. This is problematic over time due to the periodic flushing of the in-memory run. This negatively impacts read performance and throughput and wastes disk resources on the read amplification.\nCompaction\n In order to limit this read amplification, we periodically compact/merge sets of small sorted runs on disk to larger sorted runs. Since runs are all sorted, this compaction process can be done fairly efficiently, similar to the merge phase of a merge sort.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            The [Apple, . . . Tomato] run and [Chard, . . . Watermelon] run are merged into the [Apple, . . . Watermelon] run.\n\n        \n    \n\n\nOne way to perform this periodic compaction process would be to constantly compact every new on-disk run into a single global on-disk run. This approach would correlate the total number of on-disk runs at any point in time to the speed at which we could merge new runs into the global run relative to how often we flush runs from memory to disk. In practice, this approach would be quite inefficient as this would effectively cause compaction to have to re-write the entire global run each time, wasting disk resources on the write amplification.\nIf we instead pick a compaction scheme that logarithmically compacts sets of runs, e.g. every 2 small runs into 1 medium run, every 2 medium runs into 1 large run, and etc., we can logarithmically bound the number of runs we would need to ever read from for any read request. \n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            Logarithmic compaction results in O(logN) runs with O(logN) compactions. \n\n        \n    \n\n\nThis approach attempts to balance the write amplification of compacting too often with the read amplification of needing to read from too many runs.\nAlki’s Architecture\nThe key trick of LSM trees is using multiple storage tiers to play to the strengths of different storage technologies. Specifically, you have a “hot” tier that's really good at high-throughput, fast, random access (which tends to make it correspondingly more expensive) and a “cold” tier where storage is really cheap, but where I/O throughput is lower and where reading/writing 10MB of data is about as expensive as 1KB (favoring infrequent access and big sequential access when you do any I/O at all). In non-distributed LSM trees, RAM makes a good “hot” tier, and disk a good “cold” tier.\nFor Alki, we found a couple of existing cloud storage technologies with a similar cost/performance tradeoff relationship to RAM/disk, and we used the same tricks used by LSM trees to play to each layer’s strengths while presenting a unified read/write API. We use a distributed key-value store, Amazon DynamoDB, as the hot store, and a distributed blob store, Amazon S3, as the cold store. We also use AWS Step Functions and an in-house batch processing system called Blackbird to orchestrate the control plane and run the offline ETL jobs.\n\nBy building Alki on top of powerful building blocks that exposed well-defined APIs and feature sets, we did not have to spend valuable time and resources to tackle and re-solve many of the hard distributed database problems.\nDaily Lifecycle\n At the beginning of each day, we dynamically create a table in the key-value store, representing a run of data in which to ingest writes. At the end of the day, this hot run is sealed and the data is offloaded to the cold store as a sorted cold run. Once offloaded, the table for the hot run is deleted. Periodically, we compact the cold runs in the cold store, merging them in order to reduce the total number of runs. Once compacted, the source cold runs are deleted.\n \n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            Alki's architecture\n\n        \n    \n\n\nCold Store Format\n In some LSM-tree based stores, offloaded data is stored using a variant of a special indexed file format that provides a mapping from keys to data pointers, pointing to the actual data stored elsewhere in the file. A similar implementation over a blob store might require multiple blob reads per lookup and markedly worse latencies, so we instead store this index as an internal metadata table in the key-value hot store.\nThe indexing metadata itself could be very large depending on the ratio of the size of the keys versus the size of the values. In order to reduce the storage costs of storing this in the key-value store, we batch many records into a single S3 blob and only store a sparse mapping of 1 key for every blob, corresponding to the key of the first value in the blob. The blob that potentially contains any given key is found by searching for the last entry in the ordered index table with a first key smaller than or equal to the given key. Using a sparse index further optimizes for storage cost at the expense of potentially needing to read blobs that do not contain the key at all.\nAWS Services\nWe chose to build Alki on top of AWS services for a few reasons.\nThe main one is the relatively low maintenance cost of using fully managed systems which allowed us to rapidly prototype and build Alki. AWS gave us a great deal of access to their support engineers, who taught us many tips and tricks (and idiomatic usage patterns) to get the most out of the AWS services we were building on top of.\nDynamoDB and S3 are also horizontally scalable both in storage and throughput which allowed us to handle the scale of our audit logs and more from day one. This came in handy during the migration as we were able to easily scale up the systems to handle more than 100x steady state traffic and it continues to provide us benefits as we onboard other large use cases.\nHowever, the specific hot and cold stores are implementation details of Alki, and any of the backend systems could be easily swapped out, allowing us to take advantage of unique properties of other storage systems.\nData Plane vs Control Plane\nAnother benefit of using these managed systems as building blocks is that it allows the entire data plane on our end to be stateless, merely translating incoming requests into reads and writes against the underlying hot and cold store. This greatly reduces our maintenance cost to operate Alki.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            Periodic offload and compaction performed by our offline batch processing system.\n\n        \n    \n\n\nIn the offline path, we use Blackbird (our batch execution system) together with StepFunctions to periodically perform the offload and compaction processes. Using a hot store that is horizontally scalable also removes the need to offload data with strict deadlines, since any downtime of the batch processing layer only results in extra storage cost, but not in unavailability or data loss. Likewise, an extended downtime of the compaction process only results in slightly degraded read latencies.\nVerification and Immutability\nFor both offload and compaction verification, before committing and marking the new runs visible, we verify the correctness of the transitions by running a series of checks comparing the pre- and post- data. We validate that keys of the new run are sorted, counts of records match up, and that the hash of the (key, value) pairs match.\nFor the hashing function, we use an order-independent hashing function built on arithmetic over several large prime modulos. This allows us to compute the hash for a run efficiently by modulo summing the hashes from concurrently computed hashes of different chunks of the run. For compaction, it also allows us to compute the expected hash of the compacted run by simply modulo summing the hashes of the individual runs to be compacted.\nVerification of large scale batch operations to transform data is often a hard problem but is made easy in Alki by the immutability of data in a run once it is sealed.\nDevelopment and Migration\nWhen designing and building large systems like this in a tight timeline, we found it helped to try to deliver incremental value as it helps to discover unknown unknowns as early as possible. This means that we split building Alki and the migration of audit log data from Edgestore into several phases with concrete deliverables that could help us re-prioritize and re-focus our efforts.\nPhase 0: Benchmarking\n Before settling on Amazon DynamoDB and S3, we developed and performed many artificial benchmarks against several hot and cold stores. This phase helped rule out several backends early on that would’ve caused significant pain down the line from performance and throughput limitations at scale. It also helped us develop good relationships early on with the DynamoDB and S3 solutions architects to learn how to operate those systems at scale.\nPhase 1: Double Writes\n We began by implementing a basic write path into the hot store that allowed us to start ingesting data in Alki. We immediately integrated this into the existing live write path of audit logs by writing to Edgestore first and attempting to write to Alki after. At this phase, we did not have the offload from hot to cold implemented, so we simply dropped the ingested data after some period of time to avoid incurring high cost due to the pile-up of data in expensive hot storage. Since we didn’t have confidence in Alki’s reliability yet, we didn’t count Alki write failures against our external APIs’ success rates, but the early integration helped us begin to gauge the performance and availability of Alki and to start building operational expertise. In particular, it revealed early on several issues with handling bursty requests in our gRPC layer and with provisioning of AWS DynamoDB capacity that we were able to implement best practices to mitigate.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            \n        \n    \n\n\nPhase 2: Double Reads w/ Edgestore as Source of Truth\nOnce we had a read path implemented, we began to perform shadow reads on the live path. This meant every request to read audit log data would, in the background, issue the same request to Alki and compare the results.\n\nAt first, because we had not yet made Alki durable and were regularly dropping data, this would show a high rate of missing records. Once we implemented the offline processes and reads of cold data, we were able to immediately verify that offload and reads worked by seeing the rate of missing records plummet to a low rate of reads of old records never ingested into Alki.\nBeyond allowing us to see immediate validation that our offload implementation worked, this early verification also allowed us to start building tooling around investigating the causes of mismatched reads, which became indispensable for tracking down bugs in the ingestion and read path.\nOnce all of the functionality to read, write, and migrate data into Alki had been battle tested, we wiped Alki’s data one last time and begin to durably write audit logs into Alki. At this point, reads were still using Edgestore as a source of truth. In the background, we began to migrate data from Edgestore directly into Alki’s cold store using a batch upload API that we built.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            \n        \n    \n\n\nPhase 3: Double Reads w/ Alki as Source of Truth\n After we verified the migration was complete, we let the shadow read verification run for a few more weeks to build more confidence. We then switched the source of truth for reads and writes over to Alki. We remained in this double read/double write phase for another few weeks to ensure we could rollback to using Edgestore if needed. Though the rollback was never needed, the continuation of validating reads revealed that Alki actually ingested data more reliably than Edgestore. In particular, Alki was able to handle ingestion spikes more gracefully and we often saw Alki reads return more complete data than Edgestore.\nPhase 4: Alki Only\n Once we were confident Alki was working as expected, we turned off audit log writes to Edgestore and began deleting the now-migrated data from Edgestore.\n \n\n\n\n\n    Conclusion\n\n\n\nToday, Alki serves roughly 350 TB worth of user data (pre-replication and not counting indexes), at about 1/6 the cost of Edgestore per GB per year. This was made possible by leveraging the inexpensive storage costs of cold blob storage.\nWe’re in the process of onboarding a couple of other cold metadata use cases at Dropbox. In particular, there are many similar logging use-cases where the retention is long but the data is rarely read after some initial ingestion period that are prime for migration to our optimized cold metadata system.\nLooking forward, we’re also planning to build a system to automatically detect and migrate cold metadata from Edgestore and the in-development Panda key-value store into Alki.\nMany thanks to past and present contributors, design reviewers, and mentors: Anuradha Agarwal, James Cowling, William Ehlhardt, Alex Grach, Stas Ilinskiy, Alexey Ivanov, Anirudh Jayakumar, Gevorg Karapetyan, Olga Kechina, Zack Kirsch, Preslav Le, Jonathan Lee, Cheng Li, Monica Pardeshi, Olek Senyuk, Lakshmi Kumar T, Lawrence Xing, Sunny Zhang\n\n\n\n\n     We’re Hiring!\n\n\n\nDo you enjoy building databases and storage infrastructure at scale? Dropbox has petabytes of metadata, exabytes of data, and serves millions of requests per second. The Persistent Systems team at Dropbox is hiring both SWEs and SREs to work on building the next generation of reliable, highly scalable, and durable storage systems.\n\n\n\n    \n\n","publish_date":"2020-12-17 18:00:00","link":"https://dropbox.tech/infrastructure/alki--or-how-we-learned-to-stop-worrying-and-love-cold-metadata","blog":{"id":"dropbox","link":"https://dropbox.tech/","name":"Dropbox","rssFeed":"https://dropbox.tech/feed/","type":"company"},"blogType":"company"},{"id":"e4ca4dc6d5e7efd81fb67cfba6aab061","publish_timestamp":1607103000,"title":"PKCE: What and Why?","blogName":"Dropbox","image":"https://aem.dropbox.com/cms/content/dam/dropbox/tech-blog/en-us/2020/12/pkce-what-why/pkce-header.png","categories":["OAuth flow","Authorization","Security","Tips and Tricks","Oauth"],"description":"Come learn about the PKCE OAuth flow! How it works, why it’s valuable, and how to use it to authorize your Dropbox app. ","publish_date":"2020-12-04 17:30:00","link":"https://dropbox.tech/developers/pkce--what-and-why-","blog":{"id":"dropbox","link":"https://dropbox.tech/","name":"Dropbox","rssFeed":"https://dropbox.tech/feed/","type":"company"},"blogType":"company"},{"id":"cdf544f3595f45a95cfc32eb4daefc84","publish_timestamp":1606838400,"title":"Revamping the Android testing pipeline at Dropbox","blogName":"Dropbox","image":"https://aem.dropbox.com/cms/content/dam/dropbox/tech-blog/en-us/2020/12/android-testing/diagrams/Techblog-Testing...Is this thing on-Social.png","categories":["Distributed Testing","Firebase Test Lab","Testing","Kotlin","Flank","Android","CI"],"description":"\n\n\n\n    \n    \nAt Dropbox, we are proud to have recently hit one billion downloads on what is still our original Android app. Historically, we have been a relatively small team of about a dozen engineers. Recently, though, our org has grown to more than 60 mobile engineers. This growth led to an explosion of features (great!) as well as an explosion of execution time for our testing pipeline (not so great). \nAs the org grew, the time it took to run our automated suite of unit tests went from 30 to 75 minutes. What started as the effort of a handful of engineers had increased to 400 modules and 900,000 lines of code. Maintaining a high bar of quality with this much code requires a continuous investment in testing. \nThis year, we spun up a Mobile Foundation team and let them loose on our mobile testing infrastructure. We want to share how we scaled our CI pipeline, which is built on top of an in-house task runner called Changes to better serve the 3x growth of new engineers, new features and of course, new tests. \nUsing industry standard tools, some code borrowed from AndroidX, and lots of Kotlin, we reduced our CI runtime from an average of 75 minutes to 25 minutes. We did this by learning a lot about Gradle, offloading work to Firebase Test Lab, and rearchitecting our CI jobs to better allow parallelism.\n\n\n\n\n     Step 1: Recognize the problem\n\n\n\nThe growing team and codebase exposed some pain points in our development infrastructure. One of our biggest came from our testing pipeline which was run on every update to our diffs (i.e. pull requests). Our Android app has a suite of over 6,000 unit tests to ensure we can continually make improvements and update the app without any regressions.  \nWe run some of these unit tests off-device (i.e. in a JVM) while others run on emulators. The long CI cycle was compounded by how we calculated code coverage. We want to make sure that coverage does not drop when writing new code. We were using a custom Python toolchain to validate the coverage level of each file in the codebase.  \nUnfortunately, the custom tooling suffered from occasional flakiness and a persistent lack of flexibility. The combination of this flakiness with long-running checks was extremely frustrating, and a significant productivity killer.\nSave developers from developing bad habits\n One of the wonderful things about Dropbox engineers is that they will go to great lengths to avoid frustration and wasted time. Unfortunately, when it came to our time-consuming CI, this trait led our engineers into development anti-patterns. Rather than many small diffs, folks were making a few large ones. \n\nEngineers sometimes ended up writing less automated tests for new code, due to a fear of flakes causing delays for hours. This meant that we had a higher reliance on manual QA than we wanted. An example was engineers adding coverage exceptions of 0% early to files which then grew, untested, over time. These exceptions told our tooling to not validate test coverage on those files. Individually, these decisions were great for engineers who wanted to ship a small piece of code as quickly as possible. But they were less than great for the overall org, and for the long-term health of the codebase. \nIt’s important to recognize that the root cause of these issues was the bad developer experience of waiting for CI to finish running. It was halting their work unnecessarily too often. Investing in a better developer experience was well worth the time and focus they would get back. (At Dropbox, we believe focus is the new productivity.)\nOur plan to reduce both CI flakiness and execution time revolved around a few goals:\n\nTest what is needed—no more no less\nMigrate to industry standard tools \nParallelize Android unit tests that we run on device\n\n\n\n\n\n     Step 2: Examine the existing tech setup\n\n\n\nThe Dropbox Android app is built using Gradle and has about 400 modules stored in a Git monorepo.  We have one module which is referred to as the monolith and has about 200,000 lines of code along with 2,200 unit tests. The remainder of the modules are very small by comparison, and mostly contain SDKs or new features. \nPrior to our overhaul, we ran every test in the codebase on every pull request (PR) from an engineer. The process worked like this:\n\nWhen a developer sends a PR, a job in our CI infrastructure is created—a virtual machine begins executing various tasks.\nThis virtual machine will set up the environment, use Gradle to build the app APK, assemble and run the unit tests, then launch an emulator and run the Android Tests. \nAll our JVM tests , along with on-device unit tests produced coverage data. Once all tests are finished, custom scripts read and combine that execution data to produce an overall report for the project. When the report is generated, the job is complete and the PR is updated with the results. \n\nOur existing setup had its fair share of custom tooling. For example, on CI we had previously made a decision to merge the source sets of our various modules into a single module/APK. This isn’t standard, but was done to make device tests execute more quickly.  By creating a mega test module we only needed to compile one rather than dozens of test APKs. \nThis was a savings of roughly 15 minutes on every CI run. But as is often the case with custom tooling, it had tradeoffs. It led to a suboptimal experience for developers, where tests could succeed locally but then fail in CI. A common example: An engineer would forget to add a dependency to the app module when adding it to a library module.\nOur handling of code coverage was another example of using custom infrastructure that involved tradeoffs. We used the industry standard tool Jacoco to measure code coverage, but had custom Python code for verifying coverage above a threshold. We wanted coverage reported on a per-file level, and allow for tests anywhere in the codebase to contribute to coverage for a given file, which was in a custom toolchain \nCalculating coverage this way had the advantage of increasing overall coverage in our codebase. However, it led to a number of edge cases in our code which occasionally resulted in flakes in our CI process. It also allowed engineers to write tests in one module to improve coverage in another, an anti-pattern for module encapsulation. As a result, reusing modules in a new app would result in a drop in coverage for the new app. \n\n\n\n\n     Step 3: Selectively scale the pipeline\n\n\n\nRunning all of the tests on every PR was a simple way to guarantee the safety of a given change to the codebase. The downside was it took lots of time. Logically, you don’t actually need to run every test for every change to get the full safety of automated tests. You only need to run tests for code that changed, or that could have been affected by the change.\nTo visualize this, consider the following example module diagram for an application.\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            \n        \n    \n\n\nIf code changes in the :networking module, what tests should be run to ensure everything still works? Of course, you would need to execute the tests inside the :networking module. What is a little less intuitive, but absolutely vital, is that you would need to also run the tests in :featureA. This is because :featureA might expect the :networking module to behave a certain way. The changes could violate those expectations and break :featureA. \nSo, when one module directly depends on another (e.g. :featureA directly depends on :networking), you must run the dependent module’s tests whenever the base module changes. \nNext, it’s important to realize modules that implicitly depend on :networking might also break from this change. Think a Markov Blanket.\nThe reasoning is the same as for explicit dependencies, but is worth elaborating. Consider the impact of changes to the :networking module on the :app module. As we established above, it is possible that the change in the :networking module will change the behavior of the :featureA module. If :app was relying on :featureA to behave a certain way, and the change in the :networking module changes the way :featureA behaves, :app could break. This is how a change in the :networking module could end up breaking the :app module. \nIdeally the code is not so tightly coupled that this situation would happen frequently, but it’s important to recognize that this is always a possibility. When a base module changes, anything that depends on the module, explicitly or implicitly, could break. Thus, we need to run the tests for :networking, :featureA, and :app whenever the :networking module is changed.\nNow that we understand what tests must be run when a base module changes, we can see clearly what tests do not need to be run. In the example, tests for the :utils and :featureB modules do not need to be run when the :networking module changes. Those modules are not dependent, either explicitly or implicitly, on the behavior of the :networking module. They don’t even know the :networking module exists.\nThe key to saving time when running tests in CI is to not run the :featureB and :utils tests when an engineer updates the :networking module. When you have 5 modules, as in the example, the savings are likely small. But when you have 400 modules, you can really save time by figuring out what tests are needed and only running those tests, especially for product engineers who rarely make changes in base or util modules. Smaller test cycles increases developer confidence by allowing iterative changes in the same time that 1 large change used to be tested for.\nTo realize those time savings, we needed to:\n\nFind a way to determine what modules needed to be tested from the file changes in an engineer’s diff  \nConstruct a graph of the dependencies between modules\nIdentify, for a given change, which modules were affected. This is a non-trivial programming task, given the format of the input data and available APIs. \n\nInitially we thought we would need to make drastic changes, such as changing CI systems or abandoning Gradle for Bazel, which would have been a multi-year effort. After spending months evaluating Bazel we came to the conclusion that we do not have the resources to abandon the plethora of tools and community support that Gradle has.\nAs always in these situations, it would have been nice if someone else had already solved this problem and could help us out. Fortunately, in the world of Android development, there is a robust open-source community. AndroidX is one of the most prominent examples of open source repositories to improve Android development. Late last year, we were lucky enough to grab a few minutes with Yigit Boyar, an engineer on the AndroidX team, who shared with us how his team solved this problem while still using the Gradle build system.   \nWe were doubly lucky. Not only did Yigit share his theoretical knowledge of this problem, he actually pointed us to the implementation used by the AndroidX team, which is open source! He got us to explore a part of AndroidX that we had never before thought to use before: the code that actually builds and tests AndroidX, particularly its novel approach to testing through an Affected Module Detector. If AndroidX can succeed using Gradle we were confident we can scale with it as well.\nWhile this code was coupled to how AndroidX does revisions through gerrit, it nonetheless gave us a fantastic starting point for our own solution (Spoiler alert: We’ve open-sourced it.)  We were able to migrate some helper classes to instead be dependent only on Git, and begin testing Git’s ability to determine what modules are changed when a given file is updated in our codebase. In our initial pass on JVM unit tests, we saw fantastic results from the Affected Module Detector. We used it to disable any JVM test task which didn’t need to run for a given change. \n\n\n\n\n\n\n\n\n\n\n    \n        Copy\n    \n    project.tasks.withType(Test::class.java) { task -&gt;\n                task.onlyIf {\n                    affectedModuleDetector.shouldInclude(task.project)\n                }\n            }\n\n\n\n\n\n\n\nThis was promising progress, and an aha moment of clarity that helped us pick up a general pattern of writing Gradle configuration code in Kotlin, which increases their testability. \nHowever, this alone was not enough for a production-ready solution. A module with only disabled tasks still consumes a small amount of time to process—in our experience, about 500ms to 750ms. When multiplied by 400 modules, build profilers, and loggers, even a no-op run could take several minutes to bypass our unit tests.\nDon’t exclude tasks, include dependencies instead\n To prevent this unnecessary churning, we turned around our approach. Instead of excluding unnecessary tasks, we create a task which only includes necessary dependencies. Many Gradle tutorials later, we settled on the following, again all in Kotlin:\n \n\n\n\n\n\n\n\n\n\n\n    \n        Copy\n    \n    \nprivate fun registerRunAffectedUnitTests(rootProject: Project, affectedModuleDetector: AffectedModuleDetector) {\n        val paths = LinkedHashSet&lt;String&gt;()\n        rootProject.subprojects { subproject -&gt;\n            val pathName = &quot;${subproject.path}:testUnitTest&quot;\n            if (affectedModuleDetector.shouldInclude(subproject) &amp;&amp; \n                subproject.tasks.findByPath(pathName) != null) {\n                paths.add(pathName)\n            }\n        }\n        rootProject.tasks.register(&quot;runAffectedUnitTests&quot;) { task -&gt;\n            paths.forEach { path -&gt;\n                task.dependsOn(path)\n            }\n            task.onlyIf { paths.isNotEmpty() }\n        }\n    }\n\n\n\n\n\n\nThis gave us hope that we were on the right track and could soon try to apply this same algorithm to our on-device Android tests. We have released these tasks as part of our version of the affected module plugin.\nRun Android Tests in the Cloud\n Our previous implementation ran our Android tests on a series of emulators hosted in-house which were spun up using custom tooling written in Python. This had served us well, but it presented limitations in sharding the tests across multiple emulators, prevented us from testing on physical devices, and required upkeep on our part.\nWe evaluated a few options for hosting physical devices:\n\nManaged devices in house \nGoogle’s Firebase Test Lab\nAmazon’s Device Farm\n\nWe ultimately chose Google’s Firebase Test Lab because we wanted to be able to share knowledge with other companies our size, and because of the incredible availability of Firebase Test Lab support engineers on the Firebase community on Slack.\nUsing the same strategy as we had with unit tests, we rewrote our testing scripts to instead leverage Gradle and Android Plugin APIs by registering a task which only depends on modules which include Android tests.  This task will call assembleAndroidTests and generate all of the appropriate Android Test APKs.  \nWhile this approach increases the number of APKs we generate, in turn increasing our overall build time, it achieves our goal of allowing us to test each module in isolation. Developers can now write more targeted tests and apply coverage in a more focused way. It also allows us to more safely use modules across multiple apps.\nFrom there, we’ve incorporated Fladle into our Gradle scripts.  This allows us to write a simple Domain Specific Language (DSL) and scale our individual Android Tests across multiple Firebase Test Lab Matrixes, sharding suites of tests where appropriate. \nReviewing our source code, most modules have fewer than 50 Android tests and run in under 30 seconds.  However, our monolith still has hundreds of tests which take a few minutes to complete. Fortunately, Flank allows us to easily shard the tests and split them across multiple devices to run in parallel, reducing time drastically.\nIn a scenario where we run all of our tests from all of our modules, we will start 26 matrices, with up to 3 shards in the monolith’s matrix. Each matrix runs tests for a maximum of 2 minutes. Including the time to upload APKs, allocate devices, run the tests, and process the results, this step takes 7 minutes (Firebase only charges for the 2 minutes of runtime).  \nDue to Flank and Firebase Test Lab’s abilities to scale with our modules and to shard tests within an individual module, we expect this time to remain fairly constant as we continue to scale our code base. \n\n\n\n\n     Always: Evaluate opportunity costs\n\n\n\nIn the process of overhauling our testing strategy to go from “every test on every diff” to “only the necessary tests,” we discovered a few issues with our code coverage infrastructure. Our custom coverage implementation was separate from our standard Gradle toolchain, and thus required increased maintenance. This maintenance wasn’t always prioritized. And we discovered that the infrastructure had silently broken. \nWe faced a decision: update and fix our code to maintain the custom solution, or go a different direction. \nWe ultimately decided to move away from our custom solution. While the technical aspects of migrating to a more industry standard coverage solution are interesting, we think it’s more valuable to cover the process we used to make this decision. So, let’s briefly detour to describe our general approach to these kinds of questions and then cover how the general concepts applied in this particular case.\nAt Dropbox, engineering choices are all about evaluating trade-offs in the context of our values and goals. We balance perceived benefits against perceived costs. But simply looking at whether the benefits outweigh the costs is not enough. For any project we work on, whether an internal infrastructure tool or new feature for our users, we are not simply looking to build things with positive net value. We want to build things with the greatest value at the lowest cost. We are looking for work that generates maximum leverage for our investment of resources. \nThus, the right decision involves understanding opportunity costs. We must compare a given option with the best alternative. We must answer, “if we didn’t work on this, what is the highest-leverage thing we would work on instead?”\nIn an ideal world, the answers to these questions would be obvious, and picking the high-leverage things to work on would be trivial. In reality, answering these questions is often challenging and can easily become very time consuming. But spending lots of time making decisions is itself a problem!\nSo, just as we use heuristics to come up with acceptable solutions to NP-complete problems in computer science, we often rely on heuristics to make engineering decisions. There are several such heuristics we could cover that apply in various contexts, but in the interest of brevity we will focus on a few here.\nPareto solutions\n The first heuristic that we apply at Dropbox is to look for Pareto solutions, where we can get most of the benefits of a given solution with a small fraction of the effort. Pareto solutions are inherently high-leverage, and are almost always an element of our final decisions. For example, in a small codebase with a small number of engineers, running all tests against every pull request is a Pareto solution compared with developing a selective testing algorithm. \nBut Pareto is not the be-all-end-all. There are times where it makes sense to go beyond an 80/20 solution. In particular, it is often worthwhile to go well beyond an 80/20 solution when a decision relates to a core competency of your team or organization. \nThe origin of Dropbox is a good example: emailing files to yourself was an 80/20 solution for sharing data across devices. Of course, it proved worthwhile to push beyond that particular Pareto solution.\nLeverage your core competencies\n Just as Dropbox has core competencies as a company, teams inside the company have their own core competencies for the customers they support, internally or externally. Thus, in addition to Pareto, we always consider whether a given decision relates back to one of our core competencies.\nSometimes it’s not worth it\n That’s a very high-level look at the process of engineering decision making at Dropbox: Does a given direction provide the highest leverage for our time and resources when evaluated against some common heuristics? How does it connect back to our code coverage decision?\nIn this case, we asked ourselves: does fixing and maintaining our custom coverage infrastructure represent an 80/20 solution for coverage, and is code coverage infrastructure within our team's core competencies?\nWhile we decided that code coverage infrastructure was indeed a core area of ownership for our team, the custom solution was more trouble than it was worth. Our team decided that we are able to provide more value to Dropbox through other infrastructure investments. \nIn the end we migrated away from the old solution and towards the industry standard of using Jacoco’s verification task. The only custom configuration we have is locating coverage data from firebase and local tests. We moved the logic surrounding this to Kotlin, and created a data model for our coverage files:\n\n\n\n\n\n\n\n\n\n\n    \n        Copy\n    \n    \nfun forVerification(): JacocoProjectStructure {\n   val extras = addExtraExecutionFiles(module, getCoverageOutputProp(SourceTask.VERIFY))\n   module.logger.lifecycle(&quot;Extra execution File Tree $extras&quot;)\n   val executionFiles = getExecutionFiles(module) + extras\n   return JacocoProjectStructure(\n       getClassDirs(module),\n       getSourceDirs(module),\n       executionFiles\n   )\n}\n\n\n\n\n\n\n\n     Midpoint evaluation\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            \n        \n    \n\n\nOnce all the pieces were in place, we began to evaluate how we were doing in comparison to our existing method. After a few days of running as a shadow job, we found our average job run time was about 35 minutes. This included selectively building the source code, running unit tests, executing Android Test on Firebase Test Lab and calculating the code coverage. \nThis was a marked improvement over our existing job’s average time of 75 minutes. However, when we entered a scenario where someone changed one of the core modules, it would trigger most or all modules to be rebuilt, spiking our selective job’s run time to over 90 minutes.\nProfiling our pipeline revealed that the main culprits were generating the additional module’s APKs and interacting with Firebase Test Lab. Generating the additional APKs added 10 minutes. And while Firebase Test Lab was able to shard the tests, the overall provisioning of devices and collecting results added four to five minutes overhead to our existing approach. \nWe wouldn’t be able to circumvent generating the APKs if we wanted to measure coverage on a per module basis. And Firebase Test Lab provided benefits around physical devices and sharding that we didn’t want to implement ourselves. This led us to evaluate how we were running the job, and if we could shard the number of modules which needed to be run. \nShard modules whenever possible\n Just as Firebase Test Lab sharded each of the module’s tests across multiple devices, we wondered if we could shard the assembly of the unit tests across multiple nodes in our CI environment. \nInitially, we wanted to start our job, split the unit tests to one node, split the Android tests to another node, then finally collect all the results and run coverage over the results. We began to explore this option, but ran into limitations with Changes sharing artifacts across nodes—in particular, collecting artifacts. The nodes were on different VMs without a clear way to share results.\nPausing that investigation, we considered what was supported in Changes. We had the ability to provide a list of IDs to our task runner Changes, which would start an appropriate number of nodes each with a subset of ids passed in to run. This functionality is usually used to pass a list of test names to run per shard. \nWe, however, did not shard the individual tests as that would not be possible when running tests on an external device cloud. Instead we shared at the granularity of each module which contained unit tests and or Android tests. This would also alleviate the need to collect results from each of the nodes as they could all run coverage on their specific modules and report success or failure. \nThis strategy allowed us to do shard testing of our 400 modules on up to 10 VMs. On average, 20 modules are touched per diff which means each VM only needs to test 2 modules.\nProviding information to Changes to shard proved fairly straightforward. Once we left an artifact with the modules to run, we could retrieve the subset of modules in the shard, provide that to the Affected Module Detector, and indicate that a given module should be included if it had source files changed or if it was passed in. \nThe sharding approach put us into a position where the average run time is 25 minutes, with a maximum run time of 35 minutes. Most runs require about 30 minutes of compute time, whereas the worst case requires 3 hours. While the worst case is higher, we feel this is acceptable. It’s rare for a developer to trigger this case. Overall, our revisions help get feedback to the developer as quickly as possible.  \nWe also expect these times to decrease as we decompose monolithic modules into smaller feature modules. This is significantly better than our starting point of 75 minutes:\n\n\n\n\n    \n        \n            \n    \n\n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n        \n        \n        \n\n    \n\n            \n        \n    \n\n\n\n     Takeaways—and source code\n\n\n\nThree months after we set out to improve our CI pipeline, we’re fairly satisfied with the results. Most importantly, we have a scalable solution for testing. It will allow us to allocate more VMs for JVM testing or more Firebase devices for on-device testing as our module counts double and beyond.  \nThe biggest takeaways for us were:\n\nInvest in Build/CI architecture as much as in production code.\nDon’t reinvent the wheel. Delegate the hard parts to Firebase, Flank, and Jacoco.\nWhen you’re feeling blue, sprinkle some Kotlin on it.\n\nGet our AffectedModuleDetector on GitHub\n It’s not enough to just talk about it: We’ve open sourced our affected module detector as a standalone Gradle plugin. We’ll add additional hooks to it in coming months. \nWe love community contributions. And if this sounds like work you want to get paid to do, we’re hiring!\n \n\n\n\n    \n\n","publish_date":"2020-12-01 16:00:00","link":"https://dropbox.tech/mobile/revamping-the-android-testing-pipeline-at-dropbox","blog":{"id":"dropbox","link":"https://dropbox.tech/","name":"Dropbox","rssFeed":"https://dropbox.tech/feed/","type":"company"},"blogType":"company"},{"id":"16823a9bff3bb6d112aeb010f7adfaf4","publish_timestamp":1614316530,"title":"To The Future Black History Makers in Tech","blogName":"Medium","image":"https://miro.medium.com/max/1200/0*ncwQaYGaOn2FNJrf","categories":[],"description":"Black History Month is coming to a close this week. Like many others, I’ve used this month to celebrate Black culture and the achievements of Black pioneers.While we often celebrate Black history-makers from the past, I want to take some time to look to the future. After all, history is being made every day. The young people of today will be the leaders of tomorrow, and maybe that Black history-maker will be you.Image Credit: Medium Brand Design TeamI’m Michael, and I’ve been a software engineer for about 2 years. My family is from Jamaica and I grew up in New Jersey. At Medium, I get to build new features for our iOS app and support a platform for writers and publications around the world, including our in-house publications like Zora, Level, and Momentum.In a world that is increasingly digital, where social apps define how we connect and interact with one another, software engineering can be a high-impact way to help shape the world we live in and leave a mark in history.I know firsthand how this industry can feel out-of-reach and exclusive, and how as a beginner it can feel like you will never know enough to be a real software engineer. So, apart from building great things here at Medium, I aspire to make it a little easier for the next young person who looks like me to feel represented and to feel empowered to pursue their wildest dreams in tech.I’ve asked some of the other Black engineers here at Medium to share some of their inspirations, experiences, and advice with you. If you’re on the fence about diving into the world of software engineering, or if you’ve just started your journey and wonder if you have what it takes, hopefully some of these words from my colleagues will inspire you to keep going. By pursuing your dreams and building a better future, you too can make your mark in history.Mopewa Ogundipe, Senior Software Engineer, 4+ years: I got into engineering and computer science because my older brothers are both engineers and I grew up watching them tinkering with our household appliances. I love this field and have stayed in it because I haven’t found anything that beats the feeling of having an idea, building or coding it, and then putting it in someone’s hand and watching them use it.Nichelle Hall, Software Engineer, 1.5 years: My parents always wanted me and my siblings to be engineers. In high school, I took Chemistry and had a hard time in the class. I remember Googling engineering majors without chemistry, and Computer Science popped up in the search results. This planted a seed. I wasn’t quite sure what Computer Science was, and I assumed it had to do something with fixing printers. A couple of summers later, I signed up for an engineering apprenticeship program for high school students at the Philadelphia Naval Yard. My mentor didn’t have anything planned for me and my cohort. For security reasons, the Naval yard has very limited WiFi and I didn’t have the clearance to use it. Instead, I used the time to look through the O’Reilly Python books that were lying around and learn about Computer Science. I spent the summer going through the book’s coding exercises with the other teenagers. I made some friends and had some fun, so I decided to stick with it.Ifedayo Famojuro, Data Engineer, 1 year: I became an engineer because it made me feel empowered. Coding is self-actualizing! My engineering skills allow me to take an idea from inside my head, and give it form so that others can interact with it. I always found this really cool, and would definitely recommend that anyone else searching for that feeling of empowerment, look into becoming an engineer.David Osemwengie, Senior Production Engineer, 9 years: Never stop learning! There is so much to learn and there are plenty of free resources on the internet. You would be amazed how far in the industry you can get just by having the initiative to learn. Another piece of advice is to take rejection as a learning opportunity. As many rejections as I have encountered during my career, I took the time to figure out where my weaknesses were and focused on improving those areas. This will build character and make you a resilient engineer!Misiel Rodriguez, Android Engineer, 2 years: You probably relate to the feeling of entering your CS major classes and not seeing anyone that looks like you (or a select few). Listen I hear you; that feeling is real and sometimes continues in to the industry, but I’m here to tell you even in those spaces, you still belong. Don’t let that deter you away from your love for tech or passion for building things. We’re in these spaces because we worked hard and we deserve it! Also, peep these organizations/fellowships whose missions revolve around helping us navigate these spaces: Code2040 (shoutout #F8OfTech), /dev/color, and ColorStack to name a few!To The Future Black History Makers in Tech was originally published in Medium Engineering on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-02-26 05:15:30","link":"https://medium.engineering/to-the-future-black-history-makers-in-tech-6e87513605b4?source=rss----2817475205d3---4","blog":{"id":"medium","link":"https://medium.com/medium-eng","name":"Medium","rssFeed":"https://medium.engineering/feed","type":"company"},"blogType":"company"},{"id":"61359406fc2acbb0d12dd55e602d1219","publish_timestamp":1608235754,"title":"Rex: Medium’s Go Recommendation Microservice","blogName":"Medium","image":"https://miro.medium.com/max/1200/0*_14TBRD_6mZzbfGP","categories":["recommendations","go","cool","mediumengineering","node"],"description":"Rex: Medium’s Go Recommendations MicroserviceEditor’s note: This blog post was originally written in early 2019. It gives a great overview of why we built Rex and how it initially worked, but a lot has changed since then. Look for future stories on how Rex has evolved over time.Photo: Fausto García/UnsplashAt Medium, we’re focused on delivering the best stories to the users most interested in them. We want to provide as many high-quality stories for our users as possible and have them ready as soon as they open Medium, whether on the site, in the app, or in emails. You’ll often see them in the form of a list of stories (which we call a ranked feed). However they’re using Medium, users can scroll through a ranked feed of stories, searching for the best story that fits them at that moment.In March 2018, the recommendations team at Medium began to explore how we could improve our ranked feeds. We knew we wanted a feed that could quickly deliver as many personalized, high-quality stories as possible for a given user. However, as we delved into the parts of our code powering our ranked feed, we realized that our feed was neither quick to render nor sourcing as many stories as we felt it should. As an example, our homepage feed could take up to seconds to create, and we could not rank as many candidates as we’d like: only around 150 stories. There are hundreds of thousands of incredible stories on our platform — we want to be able to source recommend as many candidates as possible whenever a user goes to Medium.In order to build a recommender system that could source many stories and do so quickly, we decided to go back to the drawing board and build an entirely new service from scratch.Before delving into how we built it, we should take a look at the problems with the old one that were holding us back.One issue was technical debt: As we first made small tuneups to our recommendations system back in March ’18, we found that testing and verifying each change was far more time-consuming than we’d like. Bugs would pop up that were difficult to catch in tests and reproduce. It was clear that if we wanted to move more quickly and test new recommendation strategies, we would at least need a major refactor of the code that existed.However, the biggest issue was language choice. Much of Medium runs within a Node.js monolith, including the code that used to power story recommendations. Node, despite its many strengths, it wasn’t the best tool for this particular task.Optimal and suboptimal cases for the event loop in Node.js — thanks to Xiao MaNode is single-threaded (at least, it is how we use it at Medium). There are no concurrently running operations: All requests are scheduled via a system called the event loop. When a request makes an I/O call, it goes to the back of the event loop and yields use of the CPU to the next request in line.This is great when the computation done per request is pretty simple. Unlike in a synchronous I/O world—where a request might still hold the CPU while waiting for the I/O operation to complete—the CPU is never idle, and no request is hogging the CPU for too long a time. When we have requests that don’t demand too much uninterrupted time of the CPU, we see Node at its finest. The top example in the diagram above is such an example.This is not the case when fetching all the data to put a ranked feed together. Sourcing the stories we want to rank, getting the data to rank many stories, using different ranking services to order those stories effectively: This is just some of the heavy lifting happening for each request to form a ranked feed. Each of these subtasks makes many I/O calls themselves. Hence we’re often giving up the CPU to other requests, and when we have the CPU, we’re holding it for a long time. Making matters worse, when we give up control of the main thread, other requests could be taking the CPU a while as well, causing our request to build a feed to get slower and slower.In short, we’re putting heavyweight operations in a runtime environment that’s optimized for much more quick and simple tasks. If we wanted to build a more performant recommendations system, Node probably wasn’t the answer.With all this in mind, the recommendations team at Medium decided we had to make a change. Thus was born a new service at Medium: the recommendations microservice Rex.Before delving into how Rex works, it’s useful to clarify two things:What we mean by microservice.Why we chose Go as the language for Rex.With respect to the first, I highly recommend reading this story by a former Median, Xiao Ma, for thoughts on microservices, but as a TL;DR: We want our recommender system to be deployable separate from the rest of the codebase. Developing in a new microservice makes the test/ship/deploy process far quicker.As for the second, we considered a few languages, but we landed on Go for the following reasons:More efficient use of the CPU. While Node is single-threaded, Go is much better suited for the combination of I/O and CPU-intensive operations required to build a ranked feed. Splitting our work onto separate Goroutines means we can avoid the issue of the CPU getting hogged by one single request and other requests getting starved.Opinionated. Go makes it pretty hard to write “bad” code. A typed language that is also highly opinionated in terms of code styling means that even a newbie to Go (which I was when we started writing Rex) can quickly start writing clean and readable code.Prior experience with Go. While much of Medium’s codebase is written in Node, we already had a few smaller-purpose microservices in Go. Adding another microservice in a language that we as a company have familiarity with makes building and maintaining this new service much easier.We’ve talked a bit about the motivations for building Rex and what language we wanted to use. How does Rex actually work?The generation of the Medium feed can be described in seven basic steps: aggregating, preprocessing, annotating, ranking, postprocessing, caching, and validating.AggregatingWe source stories we think a user will enjoy, and we understand users may like stories for different reasons. For example, you may always read stories from authors or publications you follow. Or perhaps you really like technology and always want to read stories in the technology topic. For your feed, we have many different story providers, each of which provides you with stories we think you’ll like for a particular reason.A screenshot of my Medium homepage feedThe three stories here were surfaced for the following reasons:From your network: This story was published in a publication I follow (500ish). Rex sources the top-performing stories from publications I follow (like with topic-based providers, we look at stories in a publication many users have read and clapped on).Based on your reading history: Based on the stories I’ve read and clapped on so far, users with a reading history similar to mine have also liked this story. Finding users with a similar reading history to mine and making recommendations based on those is a technique called collaborative filtering, which Rex relies on to find high-quality stories for each user.Photography: I followed the photography topic, so for my homepage, Rex sources some of the top-performing stories in this topic (that is, the stories in the photography topic that many people have read and clapped on), and adds them into my feed.PreprocessingOnce we’ve aggregated these high-quality stories for the user, we filter out stories we think may not be suitable for a user at a given time. Maybe we’ve sourced a user a story they’ve already read — there’s no need to show them the same story twice. We may add a preprocessor to remove stories the user has read before. During the preprocessing step, we use different preprocessors to filter out stories, with each preprocessor filtering for a particular reason.AnnotatingOnce we’ve amassed a group of stories we think a user will like, we have to rank them by how much we think a user will like each story. Before we can rank them we have to fetch a significant amount of data from our data stores to get all of the necessary information (for example, who is the author of a story, what topic is the story in, how many people have clapped on this story, etc.). We calculate most of the features we need for ranking stories via offline Scala jobs and store them in two tables that we query at the time of feed creation. This allows us to minimize the number of I/O calls we’re making when assembling all the necessary data.There’s information about each particular user ↔ story pair that can’t be calculated offline and has to be checked online (for example, does the user for whom we’re generating a feed follow the author of a particular story?), but precalculating the features lets us do much of the work beforehand.RankingOnce we’ve gathered all the necessary data to rank each story, actually ranking the stories depends on what ranking strategy we use. We first transform the results from the annotation step into an array of numerical values and pass each story and set of values to another Medium microservice that hosts our feed-ranking models. This separate microservice assigns a score to each story, where the score represents how likely we think the user is to read this particular story.A lot of great work has gone into building Medium’s model-hosting microservice as well, but that’s a tale for an upcoming Medium Engineering story. 😉PostprocessingAfter ranking stories, there are often some “business rules” we may want to apply. Postprocessors apply story-ranking rules that ensure a better user experience. For example, we’ll see the top of a user’s feed dominated at times by a single author, single publication, or single topic. Because we want a user to see a more diverse set of authors, publications, and topics represented, we added a postprocessor that prevents a single entity from dominating the top of a user’s feed.CachingOnce we’ve finished generating the feed, we store the feed in Redis, an in-memory data store, for future use. We don’t necessarily want to show a new feed every time the user visits Medium. This could make for a confusing user experience if they’re visiting Medium often in a short amount of time. Hence, after generating a feed, we store the feed in Redis for a short period of time.ValidatingIf we’re reading our feed from the cache, some of the stories in the cached ranked-feed list may no longer be suitable for candidates. For example, if I follow and subsequently unfollow a given author, I should remove stories from that author from my feed if stories by that author are in my cached feed. The validation step filters out potentially unwanted stories from the cached feed, stories that may have been suitable candidates when we first created it.The end-to-end flow of generating a feed in Rex.When we first rolled out Rex, the benefits were immediately clear. Instead of ranking 150 stories, we can rank 10x that amount for a given user, and creating a new feed in Rex takes less than one second for 95% of requests.🎉 🎉 🎉🎉 🎉 🎉🎉 🎉 🎉🎉 🎉 🎉🎉 🎉 🎉🎉 🎉 🎉🎉 🎉 🎉🎉 🎉 🎉🎉🎉Just as great is how easily extensible this service is: Plugging in a new type of provider (for sourcing different story types) or testing out new business rules/preprocessing rules is a straightforward process and lets us easily test out new strategies to make recommendations at Medium the best they can be.Rex is a continuously evolving service — the work described here laid the foundation for making recommendations at Medium better. Within Rex, we’ve experimented with new ranking models, new strategies for “cold-start” users, new tests related to our collaborative filtering algorithms, and much more. In addition, we’ve expanded Rex to power recommendations across more and more surfaces across Medium, and making sure that Rex has the ability to scale gracefully with the extra workload has posed an awesome challenge in and of itself.There’s never a shortage of new and impactful recommendation and machine-learning challenges to tackle at Medium. If you’re interested in working on recommendations systems that affect millions of people each day, we’d love to talk with you.Rex: Medium’s Go Recommendation Microservice was originally published in Medium Engineering on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-12-17 20:09:14","link":"https://medium.engineering/rex-mediums-go-recommendation-microservice-e077bc9582a?source=rss----2817475205d3---4","blog":{"id":"medium","link":"https://medium.com/medium-eng","name":"Medium","rssFeed":"https://medium.engineering/feed","type":"company"},"blogType":"company"},{"id":"0483e995b7594dd8725c80fa411448c3","publish_timestamp":1606253281,"title":"Counting your followers","blogName":"Medium","image":"https://miro.medium.com/max/1200/0*PN5_7QG9F6jW3sae","categories":[],"description":"How Medium counts your followers, and the backfills that disrupted itPhoto: Crissy Jarvis on UnsplashA recent wave of users have questioned the validity of the displayed number of followers on their profiles. At one point, their follower number increased, some even by the thousands, but then a few weeks later went back down to around where the user originally remembered. Some users were concerned: Were their profiles hacked? Did a wave of users suddenly follow and then unfollow them? The answer is no—it was a data problem.When Medium started to shift toward more relational content, we noticed that some users were missing authors in their “Latest From Following” section, which should show the followed collections and authors that had recently published. Since this wasn’t true for all users, we had to look into the data that drives this section of Medium.Upon investigation, we noticed a discrepancy in a key column that the “Latest From Following” section uses that the rest of Medium does not. In turn, a backfill had to be done so the key column would properly reflect a user’s followings. After the backfill finished, users started reporting an uptick in followers and the number of people they have followed, even though they had not followed any new users. That’s when we noticed that the original backfill had done its intended purpose while also breaking it.A backfill is supposed to be the process in correcting data or making the data whole without modifying its existing core components. The first backfill did fix the key column needed for the table that “Latest From Following” depended on, but it also ended up modifying another table that contained the displayed number of followers on a user’s profile. Due to this error, another backfill had to be done to bring the numbers back to the users’ real amount of followers and followings.To get some context, let’s do a deep dive of what currently happens when you follow a user, the error that prompted the need for the backfill, what was done for the first backfill and why it failed, and the second backfill that fixed it.What happens when you press the follow button?Three things ultimately happen when you press that green button:You create or recreate a Medium relationship with that user. In this context, a Medium relationship is an indication of whether or not a user is following another, regardless of which platform the follow originated from. (More on this later.)You increase the number of users you follow.You increase the number of followers for the followed user.Behind the scenesThere are two tables in AWS DynamoDB that formulate the data: the user relationship table and the user stats table.The user relationship table has the following columns:user_id (the user doing the following)target_user_id(the writer who was followed)twitter_followed_at(the timestamp of import from Twitter)facebook_followed_at(the timestamp of import from Facebook)medium_followed_at(the timestamp of follow on Medium platform)medium_unfollowed_at(the timestamp of unfollow on Medium)latest_followed_at (the aggregation of all the timestamp columns)The user stats table has the following columns:user_id (the user in question)followers (the number of users the given user follows)followees (the number of users who follow the given user)Current scenarioWhen a user chooses to follow an author, an entry is created for that relationship, if it doesn’t exist already, in the user relationship table. This relationship would fill the current time into the medium_followed_at column, as well as the latest_followed_at column. Additionally, in the user stats table, the number of followers for the author and the number of follows for the user who clicked the follow button is incremented by one.When a user chooses to unfollow an author, the existing entry for the relationship is updated. In the user relationship table, latest_followed_at is set to zero, while medium_unfollowed_at is set to the time the user unfollowed. In the user stats table, the number of followers for the author and the number of follows for the user who clicked the follow button is decremented by one.The code for the logic looks something like the following:putUserRelationship(    source_user_id: string,        target_user_id: string,        social_type: string    created_at: number): Promise&lt;boolean&gt; {        // setting up the data    const relationship: Object = {               user_id: user_id,               target_user_id: target_user_id,          }    switch (social_type) {             case &quot;TWITTER&quot;:                          relationship.latest_followed_at = created_at           relationship.twitter_followed_at = created_at                                    case &quot;FACEBOOK&quot;:           relationship.latest_followed_at = created_at           relationship.facebook_followed_at = created_at       case &quot;MEDIUM&quot;:           relationship.latest_followed_at = created_at           relationship.medium_followed_at = created_at       case &quot;UNFOLLOW&quot;:           relationship.latest_followed_at = 0           relationship.medium_unfollowed_at = created_at// This function puts the data into the user relationship table          return this._socialData                 .putUserRelationship(                      relationship,                   )           //this part updates the user stats table                       .thenBound(this._updateUserCounts, this)                 .then((): boolean =&gt; {              return true                 })  }The function takes in the user_id, target_user_id, the social_type (Twitter, Facebook, Medium, or Unfollow), and the current time as an epoch timestamp, which is essentially the number of seconds elapsed since January 1, 1970, at midnight (00:00:00) in the UTC time zone. The function then ascertains which user relationship table columns to fill based on the social type and places the data into that table. Afterward, it takes the same user_id and target_user_id to updates the users’ followees and followers, respectively.Old Medium dataPrior to 2016, users had the ability to import their Facebook friends and Twitter follows to Medium. Upon import, a relationship is created for the two users and the related columns of twitter_followed_at and facebook_followed_at were updated to the time of import. The user stats were also incremented by the number of imports. At this point in time, the latest_followed_at column did not exist, and Medium relationships were calculated in the backend by grabbing the largest timestamp from twitter_followed_at, facebook_followed_at, and medium_followed_at, and making sure the aggregated timestamp is greater than medium_unfollowed_at. The code to calculate whether or not the user of the given user_id is following the given target_user_id in the relation looks like the following:isFollowingRelation(relation) {     const followedAt = Math.max(relation.medium_followed_at,                                relation.twitter_followed_at,                                relation.facebook_followed_at)                       ||   0     return !relation.medium_followed_at ||           followedAt &gt; relation.medium_unfollowed_at}This function takes in a relation that has all the data for a single Medium relationship in the user relationship table (user_id, target_user_id, twitter_followed_at, facebook_followed_at, twitter_followed_at, medium_unfollowed_at) and returns True if the user in question (user_id) is following the given author (target_user_id) or False if they aren’t. To this day, Medium still uses the code above to aggregate the user relationships that are shown on your follow pages. The top number, indicating number of followers, is taken from the user stats table while the list of followers is taken from the user relationship table.Ev’s Medium FollowersThe new column at the table (circa 2016)Following the removal of Twitter and Facebook imports, the engineering team needed a faster way to keep track of Medium relationships. Thus, latest_followed_at was born and is supposed to be the source of truth for a Medium relationship; the single column would make it easier to ascertain whether or not the relationship between two users exist.With this new column, a backfill was ran to populate it. latest_followed_at was supposed to be populated with the max of the followed_at timestamps (twitter_followed_at, facebook_followed_at, medium_followed_at) or zero. latest_followed_at is zero only if the max of the followed_at timestamp is less than medium_unfollowed_at.Some examples:John imported their following of Jane from Twitter on November 21, 2020, at 5:30PM UTC. This means their twitter_followed_at is set to November 21, 2020, at 5:30PM UTC. Their latest_followed_at would be on November 21, 2020, at 5:30PM UTC.Joe imported their following of John from Facebook on November 20, 2020, at 10:34AM UTC but then unfollows John on November 21, 2020, at 3:12PM UTC. This means their facebook_followed_at is set to November 20, 2020, at 10:34AM UTC, and medium_unfollowed_at is set to November 21, 2020, at 3:12PM UTC. With the new column, latest_followed_at is set to zero since twitter_followed_at and medium_followed_at are zero by default and medium_unfollowed_at is greater than facebook_followed_at.Jane imported their following of Joe from Twitter on November 10, 2020, at 8:45AM UTC, then unfollows Joe on November 20, 2020 at 10:50AM UTC, and finally refollows Joe on November 20, 2020 at 12:00PM UTC. This means their twitter_followed_at is set to November 10, 2020, at 8:45AM UTC; medium_unfollowed_at is set to November 20, 2020, at 10:50AM UTC; and their medium_followed_at is set to November 20, 2020, at 12:00PM UTC. With the new column, latest_followed_at is set to the value of medium_followed_at. The max of the followed_at timestamps is medium_followed_at, and since medium_followed_at is greater than medium_unfollowed_at, latest_followed_at is set to medium_followed_at.The problem that prompted the backfillsWhile developing the new interface, we noticed that “Latest From Following” was missing authors or had authors that shouldn’t be there. With the new relational Medium, we became heavily reliant on the user relationship table and the latest_followed_at field and realized that’s where the problem was. While the relationships existed, the data was not properly consolidated; some of the Twitter, Facebook, and Medium followed_at fields were not ported over to the latest_followed_at field, and some unfollows were not properly reflected. This may have been because the backfill to fill the column was incomplete, there might have been some migration issue where the backfill was run before the social media imports were turned off, and maybe there was a backend code hiccup where the unfollows didn’t zero out the latest_followed_at field. Regardless, with this finding, a backfill was in order to set the column straight.The backfill that conked some numbersTo start the backfill, we needed to query the existing data and find the Medium relationships that needed to be fixed. Given that the data is found within DynamoDB, querying millions of entries on nonindexed columns is next to impossible. As such, we have hourly batch jobs that port the data over into Snowflake, our data warehouse. With the data in Snowflake, querying the data becomes less of a burden.Getting the dataFirst, we queried the database for all user/author pairs that didn’t have the Twitter, Facebook, or Medium follows ported over into the latest_followed_at column. We were looking for any relationships where latest_followed_at is set to zero even though the max of the followed_at timestamps is greater than medium_unfollowed_at. The query looked like the following:with new_follow_updates as (   select      user_id,      target_user_id,      medium_unfollowed_at,      GREATEST(         twitter_followed_at,         facebook_followed_at,          medium_followed_at) as followed_at   from user_relations   where latest_followed_at = 0   and followed_at &gt; 0)select user_id, target_user_id from new_follow_updates where followed_at &gt; medium_unfollowed_at;We ran a second query to find all the user/author pairs that didn’t have the proper unfollow relationships. The erroneous relationship would be one where medium_unfollowed_at is greater than the max of the followed_at timestamps but latest_followed_at is not zero. The query looked like the following:select     user_id,     target_user_id from user_relations where     medium_unfollowed_at &gt; 0     and latest_followed_at &gt; 0     and medium_unfollowed_at &gt; latest_followed_at;The queries returned more than 180M rows combined, and the resulting datasets of the user_id and target_user_id were sent to S3 as comma-separated value (CSV) files that we will then use for backfilling the data.Backfill workflow.Each row in the CSV files is sent to Amazon’s Simple Queuing Service (SQS) as an individual message. The messages are then picked up by our event processors. The event processor takes in theuser_id and target_user_id and grabs the relation from the database. The processor does another computation of the latest_followed_at to make sure what we found in Snowflake is still the case in Dynamo. Based on the value of the calculatedlatest_followed_at, the processor utilized the existing Medium codebase and called the function mentioned above (putUserRelationship) to update the user relationship table appropriately.ProblemThe function we used in the backfill had the following return statement:return this._userSocialData    .putUserRelationships(        relationships,     )    .thenBound(this._updateUserCounts, this) &lt;--- Problem call    .then((): boolean =&gt; {       return true     })This updates the user relationship table and subsequently takes the user_id and target_user_id, which we had just performed on, and updates their follow counts via the function call this._updateUserCounts.This means that for the 180M relations we had updated, we had also accidentally incremented that many follow stats for Medium users.Even though the user relationship table originally had discrepancies in data, the user stats table did not. With the completion of this backfill, we had corrected the errors in user relationship table but introduced errors in the user stats table. We had flipped the tables, so to speak.The backfill that got it rightWith the newly created issue, we had to run another backfill to rectify the data. Since the user relationship table in Snowflake was updated hourly to properly reflect the table in DynamoDB and it had been a couple weeks since the initial backfill, the list of the original 180M relations that we had fixed were lost.So, what did we do? We couldn’t take the number of followers and followees from the user relationship table at face value since they can be ephemeral; it does not account for any possible new follows or unfollows that may happen between the batch job into Snowflake and the time the backfill updated the data in Dynamo. If we had taken the numbers from the user relationship table as is, we would have potentially created more errors in the user stats table. Instead, we took the difference of followers and followees between the user relationship table and user stats table and added the difference to the user stats table in the backfill. By adding the difference to the data to the user stats table, we will undo the accidental increments from the first backfill. To simplify, it would look something like:// This data will go into s3followees_diff = user_stats_followees - user_relationships_followeesfollowers_diff = user_stats_followers - user_relationships_followers// This calculation is done in the processor and goes into the databasefinal_followers = user_stats_followers + followers_differencefinal_followees = user_stats_followees + followees_differenceThe query to find the difference looked something like the following:with followees as (   select      user_id,      count(*) as followees   from user_relations   where      latest_followed_at &gt; 0   group by 1),followers as (   select      target_user_id as user_id,      count(*) as followers   from user_relations   where      latest_followed_at &gt; 0   group by 1),differences as (   select      user_id,      fe.followers as followers,      fr.followees as followees,      fe.followers - us.followers as diff_followers,      fr.followees - us.followees  as diff_followees   from user_stats us   join followers fe using(user_id)   join followees fr using(user_id)   where      abs(diff_followers) &gt; 0      or abs(diff_followees) &gt; 0   order by diff_followers, diff_followees desc)select   user_id,   diff_followers,   diff_followeesfrom differencesEssentially this query finds the true number of followers and followees for a user from the user relationship table based on the fixed latest_followed_at and subtracts the current user stats from the user stats table. The results for a user who had their number of followers increased from the first backfill might look like this:user_id | diff_followers | diff_followees------------------------------------------abcde123|          -10324|              0This means when the user is passed to the second backfill, their number of followers in the user stats table will decrement by 10,324.Similar to the initial backfill, this one goes through the same workflow. For this backfill, the user_id, number_of_followers, andnumber_of_followees are sent as a message, and the processor will take the three fields and update the user stats table. Again, we used existing code that would update the table with the delta. We made sure there were no follow-up callbacks this time around.With the completion of the second backfill, all numbers on a user’s profile now properly reflect their relationship with other users.TakeawaysThe main learnings from this:Thoroughly look through your code to make sure your intent is going to be executed.Save the data prior to the backfill as a keepsake of your adventures (and in case anything goes awry).Double check that your backfill fixed/completed the data as intended.Go through your draft; check your code. Double check that your intent is clear and well executed. If the existing function putUserRelationship was not used the way it did, the false increase in the user stats table would not have happened (beware of callbacks!) and the second backfill would not have been warranted.Save the shitty first draft, the data prior to the backfill. This data is what you worked from and what you’re trying to improve. Keeping it for after the backfill completion allows for data comparison; we want to make sure all the rows in the first draft are modified after the backfill. With our backfill endeavor, the initial data would have been helpful with the second backfill and to calculate the difference without pulling in data from the user stats table.Do a final look through; make sure the backfill has completed/fixed the data as intended. One reason we had to run the backfill that conked the numbers was because the initial backfill to complete the “new column” latest_followed_at was incomplete. There were some relationships where the latest_followed_at column did not properly reflect the aggregation of the other timestamp columns.Treat your backfill like you do your Medium posts: Write, check, save, and check one last time. Make sure to hit the “follow” button and watch the logic in action.Counting your followers was originally published in Medium Engineering on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-11-24 21:28:01","link":"https://medium.engineering/counting-your-followers-facbfafe45d9?source=rss----2817475205d3---4","blog":{"id":"medium","link":"https://medium.com/medium-eng","name":"Medium","rssFeed":"https://medium.engineering/feed","type":"company"},"blogType":"company"},{"id":"9f4961a62446f91900deb78fae727610","publish_timestamp":1603210337,"title":"Engineering Onboarding Processes at Medium","blogName":"Medium","image":"https://miro.medium.com/max/1200/1*umubBx5axzi4ZXk5EacU3Q.jpeg","categories":[],"description":"A couple months into Medium working fully remote, I started thinking about our engineering onboarding processes and looking for any holes that we needed to fill, especially now that we were all off-site. There were some things that, as an organization, we were adjusting to—pairing through Zoom, for instance, or using online tools like Miro for retros—and onboarding seemed like it could use some attention. With the help of Shruthi Adappa, I got to work on what needed to be changed or improved and what else we needed to include.What we found through researchEfficient onboarding is key. As stated in this Deputy blog post, onboarding helps with employee retention rates, clarifies and sets expectations for the new hire’s role, and lowers employee stress. Having things in place like onboarding classes and customized new-hire checklists shows new hires that you care about them and want them to have the resources they need to succeed. These resources also enable new hires to master their roles at a faster rate. While we had some valuable processes in place, we found the following things through research that gave us ideas of what we could improve.Provide a variety of learning optionsPresenting information in different formats increases a person’s chances of retaining this new information. Say, for example, your company has an onboarding class on your front-end stack; it may be helpful to have written documentation for what’s covered in that class so the new hire can reference it in the future.Provide documentation in onboarding handbooks and checklistsHigh-quality onboarding handbooks give new hires a view of what they can expect to learn and provides them with future resources. Customized onboarding checklists give new hires structure and help them see their progress.Pair new hires with an assigned mentor or onboarding buddyThe mentor or onboarding buddy is ideally another engineer on the team the new hire will be joining. This person’s priority is to be available for the new hire’s questions, help them set up, and guide them through their first projects. The amount of assistance the onboarding buddy will provide depends on the level of the new hire.Provide checklists for both the manager and onboarding buddyAll the other available onboarding resources are less helpful if the new hire has to search for them on their own. Both the manager and onboarding buddy should have their own checklists to help them complete necessary tasks before the new hire joins. Some of these tasks include:Getting the new hire into Slack groups and calendar groups.Setting up initial 1:1s with teammates.Assigning an onboarding buddy.Having a set of projects that the new hire could start on.Lean into evaluating your own processesCollecting feedback from everyone involved in the onboarding processes (new hires, onboarding buddies, managers, etc.) is helpful to continue iterating and improving what’s in place so things will be better for the next round of people who join.Improvements we madeModified onboarding class requirements and recorded the classesWe have recurring onboarding classes that happen every two weeks. In the past, the new hire took whichever classes the manager thought they needed within the first two weeks of joining. An improvement we decided to make to these classes was giving the manager and onboarding buddy freedom to have the new hire take these classes at some point within their first 30 days of joining. This gives the new hire more time to dive into parts of the code that these classes cover and attend the classes whenever they’d benefit from them most.We also decided to record all the onboarding classes, which gives new hires another learning option and creates a resource for existing engineers who need a quick refresher.Ensured continuous updates to our engineering handbooksWe have an engineering handbook compiled of multiple resources that we believe all engineers could benefit from (setting up your dev environment, linking out to more specific handbooks, etc.), but this handbook, and others like it, hadn’t been thoroughly edited in about two years. These are living handbooks that we allow anyone to edit if they’d like, but because nobody explicitly owned the handbooks, there was a lot of outdated information.As part of making sure these handbooks stay up to date, we assigned points of contact (POCs) who have the knowledge to update information when necessary. The other benefit of having POCs is that new hires now have someone specific to go to if they have questions about a certain topic. We also encourage new hires to update any outdated or incorrect information in our documentation.Updated and made manager and onboarding buddy checklists remote-friendlyWe updated resources on the checklists and made some adjustments so that it could be as remote-friendly as possible.Updated our feedback loopsWe used to have only one survey for new hires. It contained mostly open-ended questions, and we rarely received any submissions. Now we have a more in-depth new-hire form, and we’re working on surveys for the engineering manager and onboarding buddy to use in future iterations. We are finding new ways to make sure these surveys are filled out in a timely manner.We will also continue to have a dedicated Slack channel for onboarding discussions and suggestions, and we take any feedback we receive and update our onboarding processes accordinglyOnboarding for an organization will never stay the same. Some company-wide practices could change, or engineering could abandon a technology as a whole, and these changes will need to be reflected through the onboarding processes. The last time we revamped our onboarding documentation, it had everything it needed, but that was more than a year ago, and now we’re working fully remote, which is something we didn’t expect! We will have to continue finding ways to learn and adapt to the ever-changing ways that companies are accustomed to, so we can make sure we are providing incoming co-workers with everything they need.Resourceshttps://www.deputy.com/blog/what-is-onboarding-and-why-is-it-importanthttps://blog.rstankov.com/onboarding-software-engineers-remote-at-product-hunt/https://arc.dev/blog/remote-developer-onboarding-experience-7z0iu5fu3h#start-with-a-manager-onboarding-checklist-(not-a-remote-developer-onboarding-checklist!)https://foxbox.com/blog/how-we-onboard-our-remote-engineering-and-product-management-team/https://blog.spendesk.com/en/remote-employee-onboardingEngineering Onboarding Processes at Medium was originally published in Medium Engineering on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-10-20 16:12:17","link":"https://medium.engineering/engineering-onboarding-processes-at-medium-368095116ac3?source=rss----2817475205d3---4","blog":{"id":"medium","link":"https://medium.com/medium-eng","name":"Medium","rssFeed":"https://medium.engineering/feed","type":"company"},"blogType":"company"},{"id":"50836809b6face3002d64667715f4d87","publish_timestamp":1601410088,"title":"Scaling Email Infrastructure for Medium Digest","blogName":"Medium","image":"https://miro.medium.com/max/1200/1*Y3Ej0mmSGqu0BPIbTVoGkg.png","categories":[],"description":"How we increased our email capacity by 3.8xIntroductionThe Medium digest is a daily email containing a personalized list of stories that’s scheduled to arrive between 8 a.m. and 9 a.m. user’s local time. Timely delivery is important to us because we want our readers to start their days with the digest. Kind of like your morning newspaper!We send millions of digests every day, and digest generation creates one of the heaviest loads on our resources. Making sure they continue to arrive on time as our reader base grows has been challenging. If there’s a bottleneck somewhere, digests are normally impacted first.Our email infrastructure, built on top of Amazon’s Simple Queuing Service (SQS), did a commendable job handling the growing volume, but we finally hit its limit toward the end of 2019.In the following months, we worked on a series of improvements to our email infrastructure that enabled us to send 282% more digests. This post describes our journey from discovering the limit, iterating on the improvements, to finally bringing life back into our email infrastructure.📭 Digest generation overviewBefore we dive into the issues we encountered, let’s take a look at the lifecycle of a digest first.In order for digests to arrive at 8 a.m. user’s local time on time, we trigger digest generation for users in a specific time zone six hours before the scheduled send time. For example, we start generating digests for Pacific time at 2 a.m. Pacific time.We check which time zone to generate digests for every 10 minutes. If you’re wondering why we don’t check every hour, which is presumably when we go from one time zone to another, there are two reasons. First, not all time zones are one hour apart! Second and more importantly, we randomly divide users in each time zone into six 10-minute buckets. This way, each bucket contains fewer users, and we lose fewer digests if the generation for that bucket fails to start for whatever reason.With that said, let’s look at the different events involved in generating and sending a digest:Shard events: Every 10 minutes, a Jenkins job triggers the upstream shard event, which is then fanned out into 256 downstream shard events. Each downstream shard event iterates through a shard of the user email table for the current time zone bucket and emits a digest generation event for each user in the shard.Generation events: Generation events do the heavy lifting of looking up the information needed for a digest.Send events: Once an email is generated, it’s passed to Sendgrid, the email delivery service we use, via a send event. Sendgrid takes care of actually delivering the email to the reader’s inbox.All digest events were processed by a single queue so that digest events won’t block other events from being processed.🛑 Digests reached processing limitAround September to October 2019, we saw that the digest delivery time was getting delayed, and it was getting significantly worse every day.We dug into it and found that the queue responsible for handling digest events was approaching its processing limit.Normally, we’d have some downtime throughout the day, during time zones with fewer Medium users, when the queue is processing at below the maximum rate of 500 events per second. As digest send volume grows, the downtime was starting to disappear:In other words, the number of events the queue has to process over one day is approaching the maximum number of events the queue can process at its maximum processing rate.If we go beyond the limit (which we did by the end of October 2019), the queue will remain backed-up forever because we add more events than we can process every day.The most obvious solution was to bump up the processing rate, but due to digest’s scale, that would require us to significantly increase resources for underlying services, which is costly. We wanted to see if we could optimize our email infrastructure before shelling out the big bucks.🛠️ Separating send events onto a separate queueAs described earlier, all digest events were processed by the same queue. The problem with this approach is that when the queue gets backed up, a generated digest can’t get passed onto Sendgrid right away because the send event gets added all the way to the back of the queue. (Although SQS doesn’t guarantee FIFO order, events still get processed roughly in the order they are put in.)The first thing we did was putting send events onto their own queue. Since send events are fast compared to most other email events, this new queue is unlikely to get backed up, and a generated digest can be handed to Sendgrid without further delay.With this change, we got a decent amount of breathing room, as around 20% of the events on the original queue were send events.We were off the hook for now, but this was a Band-Aid rather than a solution, as we were still close to our limit. In fact, with an influx of users in the first half of 2020, we reached the limit again by June 2020.️️🛠 Reduce the number of events processedAn important best practice for email deliverability is to “sunset” inactive recipients, which means to only send emails to users who have been active with your products in the last few months. Following this rule, we only send digests to users who have either opened an email or visited Medium in the last 45 days.Previously, we handled evaluating whether a user has been sunsetted in the generation event itself. This means we were processing one generation event for every user who has ever signed up for a Medium account.However, most of the generation events we process don’t actually translate to send events. In fact, we only send emails to 1/4 of all users we try generating for.We check whether we should send each digest in the very beginning of the generation event, so the generation events that don’t become send events (presumably because those users have been sunsetted) are effectively no-op.Even though these no-op events are relatively fast, they can still cause the queue to get backed up because we rate-limit each queue by number of events per second. For example, even if 100% of events are no-op and we finish processing the whole batch in the first 100ms, it’ll still wait the full one second before picking up the next batch of events.Seeing this, the first thing we decided to do was to process events for active users only. We added a lastActiveAt field to our user email table and updated it every time a user becomes active. Then, when we’re querying the table to decide who to generate digest for, we only query for those who were active in the last 45 days according to the lastActiveAt field.This doesn’t sound too hard! We put in the necessary changes behind a flag and started ramping up percent of events processed using this new strategy.⚠️ Increased errors from bursty trafficAs we ramped to 50%, we started seeing a significant increase of errors from Rex, the recommendations service we rely on for personalizing stories in the digest.Upon closer inspection, we realized that the request pattern from our offline-processing service to Rex had become more bursty with short peaks every 10 minutes, which causes Rex to thrash as it continually scales up and back down within each interval.This is because all the no-op events for sunsetted users effectively served as a buffer and smoothed out the request pattern. Without these no-op events, every generation event resulted in an actual request to Rex, and Rex was doing poorly under this new pattern.🛠 Spread generation events across each 10-min intervalWe were able to spread out the generation events so that the request pattern to Rex is more even, reducing thrashing. The difference for traffic pattern to Rex is shown below:Traffic pattern before and after spreading out generation events; you can see the spikes in the “before” chart on the left, especially during low-traffic times.We accomplished this with two changes…Randomized delaysSQS allows each event to be delayed up to 15 minutes. In order to evenly spread out generation events, we went with a naive approach of:Function ShardEventHandler:  For each user in shard:    emit generation event with randomized delay within the    next ten minutesProcess shard events promptly by putting them on a separate queueWe deployed the randomized delays, but the request pattern was still uneven. This was because this approach assumed we were processing each batch of shard events every 10 minutes, when in fact they often get processed in bunches when the queue is backed up:To resolve this, we separated the shard events onto a dedicated queue so they can be processed promptly.The difference in how new events are added onto the queue is shown below:Number of visible queued up events, before and after: On the left (before), you can see events are added in big chunks, whereas on the right (after), the events are added steadily over time.🛠 Improve Rex scalingEven though the traffic was now smoother, we were still seeing some Rex errors during times when we went from processing almost no generation events to processing at maximum rate (since we have consecutive time zones where one has very few users and a neighboring one has a lot of users).This was because, without the no-op events as buffer, we were still sending Rex a higher number of requests per second. In fact, we were effectively tripling the number of requests to Rex, and Rex simply wasn’t able to scale up fast enough.To alleviate this, we lowered the max processing rate from 500 to 250, since we no longer have to process at 500 now that we have fewer events to process. In addition, we gave Rex more resources and tuned the configuration so it auto-scales better with the new request pattern.🎉 Email infrastructure is healthy againWith all these changes, we were finally able to fully ramp up generating digests for active users only. We ended up reducing the number of generation events by 70%.Even with this change, we noticed around 22.5% of all generation events are still no-op (that is, they don’t become send events). This is likely because email settings were turned off for those users, or the accounts may have been deleted or suspended.At the end, comparing the number of digests we were sending when we hit the limit to the projected maximum number of digests we can send under the improved infrastructure (assuming 22.5% of all generation events continue to be no-op), we can now process around 282% more events than before. We don’t expect to hit the limit again for at least another few years.🧐 TakeawaysSeparation of concerns with queuesWhen working with queues, it’s generally a good idea to separate different types of events onto different queues so they can be managed separately. On the other hand, it’s also good to avoid overengineering and only split events out onto separate queues when a concrete need arises.Predictable and even traffic patternsMaking traffic patterns as even as possible makes autoscaling underlying services easier. We also found that more predictability (in our case, by moving shard events onto a separate queue) with when new events come in help us make better decisions. It’s hard to try to adjust the traffic pattern when you don’t even know when the new events will come in.Scaling services can be trickyWith microservices, you can’t assume services you rely on are ready when you are. Making sure they are configured properly to handle your load can be challenging.The difficulty is compounded if your team doesn’t own those services. It can be hard to diagnose and fix issues in services you don’t own, as they can look wildly different from the services you’re familiar with.Our team doesn’t own Rex, and our attempts at trying to figure out how to scale Rex ourselves did not end up well. We simply weren’t familiar with it enough.In the end, we decided to ask the Recommendations team to review our plan and communicated with them closely throughout our ramp. With enough context, they were able to help us figure out how to scale Rex.We learned that as much as we didn’t want to bother other people, working directly with the teams that own those services is the best way to make sure you don’t break them.🌅 Future ConsiderationsOptimize for the remaining 22.5%As noted above, around 22.5% of the generation events are still effectively no-ops. We can further reduce the number of events we need to process by recording email settings and user status (suspension, deletion) in the User Email table.Increasing processing rateIf we do reach the limit again, increasing processing rate back up to 500 (or even higher) is still an option. We will need to make sure Rex and its underlying services can handle the higher processing rate.Improve digest to Rex connectionPart of the difficulty for bumping up processing rate is that even with spreading out events within each 10-minute chunk, digest to Rex traffic can still go from processing very little to processing a lot as we move across time zones, causing Rex to have to scale up significantly in a short amount of time.We can make the digest to Rex connection more stable either by increasing processing rate slowly (for example, instead of going from 10 to 500 in one minute, go from 10 → 20 → 40 → 80 → … → 500 over a few minutes) or allowing digest requests to be retried safely.Scaling Email Infrastructure for Medium Digest was originally published in Medium Engineering on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-09-29 20:08:08","link":"https://medium.engineering/scaling-email-infrastructure-for-medium-digest-254223c883b8?source=rss----2817475205d3---4","blog":{"id":"medium","link":"https://medium.com/medium-eng","name":"Medium","rssFeed":"https://medium.engineering/feed","type":"company"},"blogType":"company"},{"id":"c2483038798b411c82afa9389c150377","publish_timestamp":1569859423,"title":"The Ghost in the Hashes","blogName":"Medium","image":"https://miro.medium.com/focal/720/379/69/31/1*GmofqghuqY3Wga0BkfKwHQ.png","categories":["softwareengineering","ghosts"],"description":"This was originally published on Medium&#x2019;s internal instance, to explain a persistent bug to other engineers.Continue reading on Medium Engineering »","publish_date":"2019-09-30 16:03:43","link":"https://medium.engineering/the-ghost-in-the-hashes-55ef1e3a86ef?source=rss----2817475205d3---4","blog":{"id":"medium","link":"https://medium.com/medium-eng","name":"Medium","rssFeed":"https://medium.engineering/feed","type":"company"},"blogType":"company"},{"id":"f6be1abf6fbc23052cbca3b44e3cd065","publish_timestamp":1566858564,"title":"CODE2040 2019 Offsite","blogName":"Medium","image":"https://miro.medium.com/max/924/1*mPjT_k-OrcLqek7bKTJvBw.jpeg","categories":["internships","fun","teambuilding","engineering","code2040"],"description":"on our personal transportersIt was a BIG day. Our CODE2040 crew left the office for a day of adventure! Only one of us had ever truly experienced a segway before. Some of us excelled, some of us were terrified, and all of us bonded.We met in Oakland on Skyline Blvd. and took time getting acclimated by cautiously moving about a parking lot. After settling into our new two-wheelers, we cruised around looking at views and dodging branches until we made it to the Chabot Space and Science Center. Some kids at the summer camp yelled some things at us, and we were on our way.staying out of traffic / taking the sharp corners testA couple of hours rolling through the redwoods had us exhausted. Luckily, Miles had booked a ramen+mochi class at Kaori’s Kitchen.engineering hosts + their interns ❤my fingers were too sticky to capture any mochi photos between these two ^It’s the 7th week of our 10-week internship program, and we’ll be sad to see the 2019 internship program end. Misiel, Aleida, and Chris — we’ll remember this summer always!CODE2040 2019 Offsite was originally published in Medium Engineering on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2019-08-26 22:29:24","link":"https://medium.engineering/code2040-2019-offsite-236768037357?source=rss----2817475205d3---4","blog":{"id":"medium","link":"https://medium.com/medium-eng","name":"Medium","rssFeed":"https://medium.engineering/feed","type":"company"},"blogType":"company"},{"id":"0353bef31bbb2b1347f98a19648d3699","publish_timestamp":1562966314,"title":"Code Reviews at Medium","blogName":"Medium","image":"https://miro.medium.com/max/1200/1*N6N9nxKKf7PXht5YgLtjqQ.jpeg","categories":["codereview","softwaredevelopment"],"description":"Small PRs, quick reviews, and working like we&#x2019;re in this togetherContinue reading on Medium Engineering »","publish_date":"2019-07-12 21:18:34","link":"https://medium.engineering/code-reviews-at-medium-bed2c0dce13a?source=rss----2817475205d3---4","blog":{"id":"medium","link":"https://medium.com/medium-eng","name":"Medium","rssFeed":"https://medium.engineering/feed","type":"company"},"blogType":"company"},{"id":"5b709827e229f90e8d4e779520be0325","publish_timestamp":1561987894,"title":"Engineering growth at Medium","blogName":"Medium","image":"https://miro.medium.com/max/1200/1*HFfC7aZbMiudym0E5ayLwA.png","categories":[],"description":"Over the years, Medium Engineering (in partnership with our People Operations team) worked to improve the process to evaluate our engineers growth and their impact on the organization. We’ve shared some of our thinking and approach to this process in the past, and are happy to hear that other companies are using their own adaptations of this model!As we continue to learn, we are now on our third iteration of this process, and are excited to share with this community some key takeaways. At its core, our mission at Medium is to build the best possible product, so while we won’t be updating this post with each new iteration of our growth framework, we are still engaged in conversations about what it means to grow as an engineer, what increasing scope looks like for different kinds of leaders, and how we can be fair in our evaluation strategy.The bullets below give an overview of how we are trying to continuously improve our process so that we can build a robust, flexible, and inclusive team:Focus on describing behaviors you can observe, teach, and evaluate in your rubric criteria.Conversations about growth and impact should be ongoing, and no one should be surprised by information communicated at the time of their assessment.Studies have shown that the positive feelings associated with getting a raise are only temporary, and that long term satisfaction is much more closely tied to personal development. This is another reason why we have chosen to frame our conversations around growth.We value a growth mindset, and are intentional about this being reflected in our rubricTraditional methods of assessing people, and often the language that is used to describe them — ladders, slots, boxes, etc. — are primarily concerned with giving someone a level or categorizing them in some way. We instead craft a framework that centers around the growth of an individual, and supports them in the kind of career that they both want to have and that also benefits the business. Therefore, we talk about what others may call a performance assessment in a way that anchors on growth and uses a tool (the rubric) to define how to progress, and how we measure and reward that progress.A key measure of success for any assessment system is whether it treats employees equitably, and rewards their work appropriately, regardless of race, gender, age, or any other personal identifiable indicator.A strong rubric will incentivize the kinds of behaviors that we want to see in the team, and recognize the different kinds of value that people add.It’s important to connect hiring and growth progression to our company values, so we hire, incentivize, and reward what we value throughout the employee lifecycle.There are many paths to a successful career, and diverse experiences and strengths together make for a robust and flexible team.Titles typically serve three purposes — helping people understand that they are progressing, vesting authority in those people who might not automatically receive it, and communicating an expected competency level to the outside world.Execution: The best idea in the world is worthless without great execution. Delivering great software products in teams requires rallying people behind an idea, strong technical leadership throughout the project, a focus on quality, and excellent communication to keep everyone aligned.Management: Effective, formal people management is crucial to getting the most out of a team, building for the future, and providing stability during organizational change.Ideally, we would have a completely objective rubric, with simple Yes/No decisions made about clear, concrete tasks. This isn’t really possible, but it’s important to get as close as possible. Of course, subjectivity invites the possibility, and likelihood, of bias so an effort needs to be made to ensure the rubric is applied as evenly and fairly as possible.How do you capture communication objectively? How do you capture selflessness?We base our criteria on scopes of influence, which help paint a picture of how a specific skill can grow over time to impact more people or more surfaces.We have learned so much going through this process, and want to thank our community for their questions and conversation on this topic!Engineering growth at Medium was originally published in Medium Engineering on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2019-07-01 13:31:34","link":"https://medium.engineering/engineering-growth-at-medium-4935b3234d25?source=rss----2817475205d3---4","blog":{"id":"medium","link":"https://medium.com/medium-eng","name":"Medium","rssFeed":"https://medium.engineering/feed","type":"company"},"blogType":"company"},{"id":"73e5e1feaa3c22ed9ccd576d4581a000","publish_timestamp":1541187195,"title":"GraphQL Server Design @ Medium","blogName":"Medium","image":"https://miro.medium.com/focal/1200/632/53/48/1*LxzBwQmETizo-ZA_jiBLiQ.png","categories":["infrastructure","microservicearchitecture","softwareengineering","graphql","microservices"],"description":"A while ago, we told the story of how we are migrating to React.js and a service oriented architecture with the help of GraphQL. Now, we want to tell the story of how the structure of our GraphQL server helped make our migration much smoother.We had three things in mind when we began designing our GraphQL server:It should be easy to alter the shape of the dataWe currently use protocol buffers as a schema for data that comes from our backend. However, the way we use our data has changed over time, but our protobufs haven’t caught up. This means that our data isn’t always the shape that the clients need.It should be clear what data is for the clientWithin our GraphQL server, data is being passed around and exists in different stages of “readiness” for the client. Instead of mixing the stages together, we wanted to make the stages of readiness explicit so we know exactly what data is meant for the client.It should be easy to add new data sourcesSince we are moving to a service oriented architecture, we wanted to make sure it was easy to add new data sources to our GraphQL server, and make it explicit where data comes from.With these things in mind, we came up with a server structure that had three distinct roles:Fetchers, Repositories (Repos), and the GraphQL Schema.a layer cake of responsibilityEach layer has it’s own responsibilities, and only interacts with the layer above it. Let’s talk about what each layer does specifically.Fetchersfetch the data from any number of sourcesFetchers are for fetching data from data sources. The data that is fetched by the GraphQL server should already have gone through any business logic additions or changes.Fetchers should correspond to a REST or preferably a gRPC endpoint. Fetchers require a protobuf. This means that any data that is being fetched by a Fetcher must follow the schema defined by the protobuf.Repositoriesshape the data for what the client needsRepos are what the GraphQL schemas will use as a data representation. The repo “stores” the cleaned-up data that originally came from our data sources.In this step, we hoist up and flatten fields and objects, move data around, etc. to change the data shape to be what the client actually needs.This step is necessary for moving from a legacy system because it gives us the freedom to update the data shape for the client without having to update or add endpoints or their corresponding protobufs.Repos only access data retrieved from Fetchers and never actually fetch the data themselves. To put it another way, Repos only create the shape of the data we want, but they don’t “know” where we get the data from.GraphQL Schemaderive the schema for the client from our repo objectsThe GraphQL Schema is the form our data will take when it gets sent to the clients.The GraphQL schema only uses data from Repos and will never access Fetchers directly. This keeps our separation of concerns clear.In addition, our GraphQL schema is completely derived from our Repo objects. The schema doesn’t alter the data at all, nor does it need to: the Repo has already changed the shape of the data to be what we need, so the schema just needs to use it and that’s it. In this way, there isn’t confusion about what the data shape is or where we can manipulate the shape.GraphQL Server Data Flowhow data flows through our GraphQL serverThe data’s shape becomes more like what the client needs as it passes through each of the distinct layers. It’s clear where the data comes from at each step and we know what each piece of the server is responsible for.These abstraction boundaries mean that we can incrementally migrate our legacy system by replacing different data sources, but without rewriting our entire system. This has made our migration path clear and easy to follow and makes it easy to work towards our service oriented architecture without changing everything at once.GraphQL Server Design @ Medium was originally published in Medium Engineering on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2018-11-02 19:33:15","link":"https://medium.engineering/graphql-server-design-medium-34862677b4b8?source=rss----2817475205d3---4","blog":{"id":"medium","link":"https://medium.com/medium-eng","name":"Medium","rssFeed":"https://medium.engineering/feed","type":"company"},"blogType":"company"},{"id":"ea5bb534e3178f80910b60e5386cda77","publish_timestamp":1615990893,"title":"How PayPal moves secure and encrypted data across security zones","blogName":"Paypal","image":"https://miro.medium.com/max/1200/1*p0_O42MPnChopUng1wfdoQ.png","categories":["crosszone","hadoopsecurity","securedatamovement","hadoopencryption"],"description":"PayPal, like other large companies, has many data centers, regions, and zones with different security levels and restrictions to protect data. This makes data movement a not-so-easy task, and sets a high bar for maintaining security and data protection compliance.The Data Movement Platform (DMP) team of Enterprise Data Platform (EDP) needed to build a fast and reliable data movement channel to offer to its customers within and outside EDP. This channel needed to be secure, fully InfoSec compliant. Last year, we started the journey to build our next-gen data movement platform. This blog is specifically about the security aspect of the platform.At PayPal, security is our top priority, and that entails data protection.Traditionally, to move data across zones, various teams have been using either DropZone — a Secure-FTP-based platform that’s built in-house, or Kafka — an easily available service by the Kafka team at PayPal. DropZone provides secure and reliable data storage for offline use-cases, whereas Kafka provides a fast data highway for more real-time use-cases. Both have their limitations when moving large datasets and require additional efforts to develop for the producer and consumer. These platforms are not suitable for batch data movement and suffer from burst data ingestion problems, not to mention an additional hop of (intermediary) data storage. Apache Pulsar specifically tries to solve this by exposing the storage layer interface for direct analytics.What does it mean to move data securely?Providing secure data movement means complying with the following InfoSec rules:All connections must be secured using TLS 1.2.Only a higher security zone can initiate the connection to a lower security zone — not vice-versa.Data at rest must be encrypted, with no unauthorized and unaudited access.All authorizations must be provided via either IAM or Kerberos.All secrets must be managed by KeyMaker — a key-management service.How we incorporated these principles within the DMP:The DMP primarily uses Hadoop as its execution platform to achieve high availability, reliability and resiliency for both computing and storage. Hence, both Apache Gobblin and Hadoop need to support all security measures. Let’s look at the high-level deployment architecture.High-level Hadoop cluster setup in multiple zones with key distribution center (KDC), Key Management Service (KMS), and Apache Ranger.Securing communication — HTTPS and socketsHadoop, in addition to regular socket addresses, provides secure socket addresses for each service to interact. When used, the connection can be encrypted over TLS 1.2. More info on secure ports can be found here.Securing Authorizations — via KerberosSecuring a group of Hadoop clusters is a relatively complex task, involving multiple architectural decisions. It’s much more complex than securing a single Hadoop cluster. There are two choices here:1. Create trust between all different KDC servers located in different zones, so they can trust each others TGT (tickets) and DT (tokens) with the same security standards for communications.2. Use a single KDC for both Hadoop clusters to keep authentication and authorization centralized.We went ahead with the second approach, where we manage a single central KDC which provides all the required security for all Hadoop clusters, instead of managing multiple KDC servers per each Hadoop cluster.We run Apache Gobblin on Yarn mode. Currently, Apache Gobblin does not support in-built token management for the multiple remote Hadoop environment. We added this feature with GOBBLIN-1308.Securing data — using Transparent Data Encryption (TDE)The Hadoop Platform natively supports a data encryption feature called TDE. Once TDE is enabled, all backend services encrypt and decrypt data transparently without requiring modifications to client code. With the right security configurations, it will work seamlessly.TDE brings a lot of challenges, especially in a multi-cluster Hadoop environment across zones protected by firewalls. It adds to the complexity of token management, KMS configurations, and WebHDFS client API calls, etc…Managing tokens— delegation token management for authentication &amp; authorizationTDE is usually enabled with Kerberos. This makes token management even more complex for applications because TDE requires additional token — KMS, a cryptographic key management server that provides tokens for encryption/decryption to read/write data to underlying storage — HDFS. We can also define encryption zones for TDE to be applicable via Apache Ranger policies.KMS setup: Hadoop 2.7.x has several known bugs:HADOOP-14441— The KMS delegation token does not ask all KMS servers. As a result, a renew request to the KMS server fails if it is not the issuer. The expected behavior for the KMS service is to figure this out internally.HADOOP-14445— The KMS does not provide high availability (HA) service, and hence tokens issued by one KMS server cannot be authenticated by another KMS instance of the same HA pool. Since this fix is only available in &gt; Hadoop 2.8.4, we had to renew the KMS token with all KMS servers and ignore the failed operations, assuming we hit the right one from the list of available KMS servers.HADOOP-15997 &amp; HADOOP-16199 — When a token renews, it does not necessarily go to the right issuer. For this to work, we had to include all issuers in the local Hadoop config. This is due to the token class structure where it does not hold and use the right issuer or UGI while performing token.renew() call as described in the jira tickets.An alternative to the WebHDFS API: With this highly secured environment, we have learned that it is not recommended to use the WebHDFS API, especially when both Kerberos and TDE are enabled on Hadoop 2.7.x. Hadoop 2.7.x has an implementation flaw that can possibly misuse HDFS superuser to access those “Encrypted Zones”. WebHDFS also has known issue of possibly stalling the NameNode. These can potentially create breaches or incidents, so we created a replacement WebHDFS-like API to overcome such challenges. Hopefully, Hadoop 3.x is going to take care of these issues.Overall, this setup contributed to the most secure way of moving data across multiple Hadoop clusters between multiple zones. This provides the backbone for the batch DMP and enables multiple internal customer use-cases at PayPal.It was a bit of an effort, but it was well worth learning the internal workings of Hadoop and network security. Kudos to PayPal’s Hadoop SRE and InfoSec team for partnering on this journey.Teams:Data Movement Platform Team: Jay Senjaliya, Radha Ramasubramanian, Sathish Srinivasan, Anisha Nainani, Rahul Kalita, Jyothsna Kullatira, Sanjiv Prabhunandan, Afroz K. (PM)Information Security Team: Skip Hathorne, Greg King, Scott Van SchoyckHadoop SRE: Raghu Agani, Elliott Brown, William WangEngineering Leadership: Sudhir Rao, Bala Natarajan, Prasanna Krishna, Michael Zeltser, Sarah BrydonHow PayPal moves secure and encrypted data across security zones was originally published in Technology at PayPal on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-03-17 14:21:33","link":"https://medium.com/paypal-tech/how-paypal-moves-secure-and-encrypted-data-across-security-zones-10010c1788ce?source=rss----6423323524ba---4","blog":{"id":"paypal","link":"https://medium.com/paypal-engineering","name":"Paypal","rssFeed":"https://medium.com/feed/paypal-engineering","type":"company"},"blogType":"company"},{"id":"72e7897ba11962342407b5d1cb86d5a4","publish_timestamp":1613484944,"title":"Real Load Aware Scheduling in Kubernetes with Trimaran","blogName":"Paypal","image":"https://miro.medium.com/max/1200/1*ZlJsVjnIxxbR4yXaLrgyFw.jpeg","categories":["kubernetesscheduling","trimaran","targetloadpacking","kubescheduler","kubernetes"],"description":"Why is scheduling in Kubernetes inefficient?Native scheduling in Kubernetes is handled by the kube-scheduler service. Resource utilization of pods is defined via a declarative resource model and the kube-scheduler works with a kubelet service to provide pod QoS guarantees. This model can lead to low utilization and wastage of cluster resources, as live node resource utilization is not considered in scheduling decisions. Also, it is hard for users to predict the correct usage values of their pods when they define pod specs.Trimaran SchedulerAs of Kubernetes 1.15, the scheduler has been made flexible for customizations with the Scheduling Framework. Our team at PayPal leveraged this to develop the Trimaran scheduler which works on live node utilization values to efficiently utilize cluster resources and save costs. As part of this, we developed and contributed the TargetLoadPacking plugin and Load Watcher to the open-source community.Trimaran ArchitectureTargetLoadPacking PluginThere are multiple extension points in the Scheduling Framework that we can hook into for customization. TargetLoadPacking Plugin extends the Score extension point, which is responsible for scoring nodes to schedule pods in each scheduling cycle. Our algorithm is a hybrid variant of the standard bin pack algorithm that favors packing pods on nodes around a target utilization, by moving from best fit to least fit. In other words, given a target utilization of x%, the plugin favors nodes that are closer to x%. Currently, CPU Utilization is supported but it can be extended to multiple resources.Deployment TutorialThis tutorial will guide you to deploy the Trimaran scheduler in any K8s setup including Minikube, Kind, etc. Trimaran depends upon the load watcher service to run, which in turn depends on a metrics provider to load metrics from. Currently, supported providers are Kubernetes Metrics Server (default) and SignalFx, with current ongoing work to add support for Prometheus. Make sure to deploy the metrics provider you want to use in your K8s cluster before you deploy the load watcher.Note that your Kubernetes version should be at least v1.19.0 as of this writing.As part of the Trimaran scheduler deployment, we will create Docker images for two services — kube-scheduler configured with TargetLoadPacking plugin (trimaran image) and load watcher service (load-watcher image), and deploy them as a single pod.Instructions to build a load-watcher Docker image can be found here.To build atrimaran image, first build a kube-scheduler image as follows:git clone https://github.com/kubernetes-sigs/scheduler-plugins.gitcd scheduler-pluginsenv GOARCH=amd64 GOOS=linux go build -o kube-scheduler main.goSave the following Trimaran scheduler configuration in scheduler-config.yaml:apiVersion: kubescheduler.config.k8s.io/v1beta1kind: KubeSchedulerConfigurationleaderElection:  leaderElect: falseclientConnection:  kubeconfig: &quot;REPLACE_ME_WITH_KUBE_CONFIG_PATH&quot;profiles:- schedulerName: trimaran  plugins:    score:      disabled:      - name: NodeResourcesBalancedAllocation      - name: NodeResourcesLeastAllocated      enabled:      - name: TargetLoadPacking  pluginConfig:    - name: TargetLoadPacking      args:        watcherAddress: http://127.0.0.1:2020It is strongly recommended to disable the two native plugins above to prevent scoring conflictsCreate a Dockerfile with the following content in the root directory of the cloned repository above:FROM golang:1.5-alpineADD ./kube-scheduler /usr/local/bin/kube-schedulerADD ./scheduler-config.yaml /home/scheduler-config.yamlBuild the Docker image as follows:docker build -t trimaran .docker tag trimaran:latest &lt;your-docker-repo&gt;:latestdocker push &lt;your-docker-repo&gt;The following is a YAML spec for Trimaran K8s deployment:apiVersion: v1kind: ServiceAccountmetadata:  name: trimaran  namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: trimaran-as-kube-schedulersubjects:- kind: ServiceAccount  name: trimaran  namespace: kube-systemroleRef:  kind: ClusterRole  name: system:kube-scheduler  apiGroup: rbac.authorization.k8s.io---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:  name: trimaran-as-volume-schedulersubjects:- kind: ServiceAccount  name: trimaran  namespace: kube-systemroleRef:  kind: ClusterRole  name: system:volume-scheduler  apiGroup: rbac.authorization.k8s.io---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:  name: trimaran-extension-apiserver  namespace: kube-systemsubjects:- kind: ServiceAccount  name: trimaran  namespace: kube-systemroleRef:  kind: Role  name: extension-apiserver-authentication-reader  apiGroup: rbac.authorization.k8s.io---apiVersion: apps/v1kind: Deploymentmetadata:  labels:    component: scheduler    tier: control-plane  name: trimaran  namespace: kube-systemspec:  selector:    matchLabels:      component: scheduler      tier: control-plane  replicas: 1  template:    metadata:      labels:        component: scheduler        tier: control-plane        version: second    spec:      serviceAccountName: trimaran      hostNetwork: true      containers:      - name: trimaran        command:        - /usr/local/bin/kube-scheduler        - --address=0.0.0.0        - --leader-elect=false        - --scheduler-name=trimaran        - --config=/home/scheduler-config.yaml        - -v=6        image: &lt;replace-me&gt;        imagePullPolicy: Always        resources:          requests:            cpu: &#39;0.1&#39;        securityContext:          privileged: false        volumeMounts:         - mountPath: /shared          name: shared      - name: load-watcher        command:        - /usr/local/bin/load-watcher        image: &lt;replace-me&gt;        imagePullPolicy: Always      volumes:      - name: shared        hostPath:          path: /tmp          type: DirectorySave the above content in a file named “trimaran-scheduler.yaml” and deploy it using the command:kubectl apply -f trimaran-scheduler.yamlFor any pods to be scheduled with Trimaran scheduler, schedulerName needs to be modified with value trimaran in a respective pod spec YAML file. An example pod spec is given below:apiVersion: v1kind: Pod...spec:  schedulerName: trimaran  containers:  - name: pod-with-annotation-container    ...Verify that the pod has been scheduled with the following command and status as Running.kubectl describe pod Configuring TargetLoadPackingThere are three configurable parameters other than watcherAddress that can be used to modify the behavior of the plugin according to your requirements:targetUtilization: CPU Utilization % target you would like to achieve in bin packing. It is recommended to keep this value ten less than what you desire. The default is 40.defaultRequests: This configures CPU requests for containers without requests or limits i.e. Best Effort QoS. The default is one core. This is used for utilization prediction when scheduling.defaultRequestsMultiplier: This configures the multiplier for containers without limits i.e. Burstable QoS. The default is 1.5 cores.These can be added in args under pluginConfig in scheduler-config.yaml. More details about the design can be found in KEP.ContributionThere are interesting areas where our work in Trimaran can be extended. For example, multidimensional bin packing with multiple resources (CPU, Memory, Network Bandwidth, etc.), ML/AI models for utilization prediction, etc. Readers are welcome to get in touch and contribute!Real Load Aware Scheduling in Kubernetes with Trimaran was originally published in Technology at PayPal on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-02-16 14:15:44","link":"https://medium.com/paypal-tech/real-load-aware-scheduling-in-kubernetes-with-trimaran-a8efe14d51e2?source=rss----6423323524ba---4","blog":{"id":"paypal","link":"https://medium.com/paypal-engineering","name":"Paypal","rssFeed":"https://medium.com/feed/paypal-engineering","type":"company"},"blogType":"company"},{"id":"779836bc675d2e255d12751af46126e5","publish_timestamp":1612450368,"title":"Next-Gen Data Movement Platform at PayPal","blogName":"Paypal","image":"https://miro.medium.com/max/1200/1*gblZ1ye7A6Uib_fxU-saig.png","categories":["apachegobblin","datapipeline","etl","apacheairflow","datamovement"],"description":"…using Apache Airflow scheduler and Apache Gobblin — a data integration framework open-sourced by LinkedIn.As PayPal grows beyond 300 million users, we generate lots of data, both on our online (site) and offline (analytics) storage platforms. Data movement among those systems plays a critical role in enabling many of PayPal’s business use cases.PayPal hosts a large installation of Hadoop and other analytics systems, holding hundreds of petabytes of data.PayPal is one of the few companies in the software industry where almost every major type of storage system is used, from traditional RDBMS systems like MySQL to analytics platforms like Hadoop and other specialized data stores like Aerospike, Elasticsearch, and Kafka.Data that moves is alive and valuable. At rest, data is dead.Data constantly needs to move around and get processed, analyzed, and organized to realize its value. Moreover, the data producers and data consumers are not always in the same org. while producers optimize to write, consumers look to optimize to read. This inherently creates a challenge to enable data-driven decisions at a rapid pace. The data when created is tiny yet very critical, but when it&#39;s the time to read/analyze it, it usually becomes a big data problem. This dichotomy in the data world is bridged by data movement platforms and teams.A decade ago, data movement at PayPal was seen as an operations problem where system admins or platform service providers built tools and utilities to facilitate data movement in and out of the systems. This blog on the Evolution of data movement platforms provides a good glimpse of how data movement system evolved and where it’s headed.At Paypal, due to many legacies and one-off pointed data movement solutions, we ended up having a complex and unmanageable eco-system which added to the run-the-business (RTB) cost. Moreover, supporting new sources and targets costs more and hinders quick solutions for new business initiatives. We needed a data movement platform that can scale and cover a wide variety of storage ecosystems. The following diagram roughly depicts what we ended up with:No product roadmap plans for this :) and yet it inevitably happened as part of our growth journey.Also, as the amount of data being produced increased and consumers demanded more and more real-time experiences, we needed a much faster (i.e. throughput-wise), efficient, and reliable data movement platform to serve the downstream business use cases. So we embarked on the journey to build our Next-Generation of Data Movement Platform for PayPal.“If a solution is built for the most complex scenario, consider it built for the easier ones” — Enterprise data platform leadership.RADD — The Risk Analytical Dynamic Datasets pipeline is one of the most challenging and business-critical use-cases that enable the PayPal risk platform to make decisions on payment transactions. This was a perfect candidate to be proven on a new platform.RADD data flow requirements:Data flow path of the business use-caseFor us, building on top of open-source technology fits with our belief of “don&#39;t reinvent the wheel”, and “contribute back to the community”. So when we evaluated many OSS frameworks, based on our requirements and proof-of-concept results, we felt that Apache Gobblin offers the most features, and provides flexibility and space for us to build our next-gen data movement platform.An enterprise data movement platform requires many more components than just a reader and writer. The following diagram shows what we built and how it interacts to provide an end-to-end solution at a high level.End-to-End Component Interaction for the Data Movement PlatformOnboarding Service is a set of REST APIs built using PayPal’s internal Java spring framework (we call it Raptor). The onboarding service orchestrates the data pipeline. It interacts with various other services like schema reg., Gobblin, and Airflow APIs to create an end-to-end data pipeline. An onboarding API call results in a DAG and deploying configs on Airflow for executions. The DAG can be triggered based on the chosen mechanism at onboarding time, like an upstream handshake, cron-based interval, or ad-hoc. During every run, DAG also fetches metadata to operate based on the latest changes. A set of APIs to manage data pipeline lifecycles is also provided. Here is what the swagger spec looks like.DAG Service: The DAG service can create Airflow DAGs as per the requested configurations and template. Since Airflow does not provide a stable API interface to manage the Airflow DAGs, we built our own as part of this service. The DAG it builds is primarily responsible for incorporating all the application-specific logic: allowed deviation per dataset, type of movement (ad-hoc or rollback), etc. Once the DAG is deployed, Airflow does the execution.Apache Airflow: Airflow is a well-known workflow management and executor platform. We use Airflow to define and execute the data pipeline DAG. Airflow provides a runtime orchestration layer for end-to-end movement. This also provides the ability to embed data processing and handshaking capabilities, and easier operational management for the data pipeline. Here is how the DAG looks like for one of the datasets.Apache Gobblin is a highly scalable and distributed data integration framework that simplifies common aspects of data movement and integration and can support both streaming and batch movements. We use it as a core data mover component, controlled and managed by Airflow. To achieve this architecture, we developed new components within Gobblin for better service integration — job server to start/stop jobs from Airflow and CRUD APIs to manage jobs by the onboarding service, job metadata persistence over MySQL for better job management, SignalFX integration, etc. These new additions make Apache Gobblin more generic for enterprise use-cases that we also plan to contribute back (ref: Gobblin Improvement Proposal 4).With security being a top priority at PayPal, all platform components communicate via HTTPs (over TLS 1.x) and keep data encrypted at rest. This blog on Secure &amp; encrypted data movement across security zones talks about how we achieved it.Overall, there are many components engaged here to form an end-to-end solution and things can quickly get complex if we don&#39;t define clear roles, responsibility, and design principles. There are many architectural principles we followed but one that really works for us is the following set of guidelines. These made implementation very clear for developers to expedite delivery while minimizing complexity:Each component acts as a micro-service that interacts over REST APIs, operating as a service provider.Each component operates on the latest configuration at runtime.Each component’s responsibilities are clearly defined with boundaries. Airflow does not make any data-movement-specific decisions. Gobblin does not have any visibility into why it moved the data; it simply does it when asked by Airflow.Metadata is centralized and changes are directly visible to all components.All components provide visibility via metric store integration (InfluxDB).All components should support rolling deployments to incur zero downtime while deploying changes.The journey for #OnePlatform4PYPL has been started and we are going to keep solidifying and contributing to the open-source platform. Next in a bucket is to support PayPal’s cloud journey… Exciting!.Data Movement Platform Engineering Team:Jay SenjaliyaRadha RamasubramanianSathish SrinivasanAnisha NainaniChirag TodarkaRahul KalitaAnand MehrotraJyothsna KullatiraSanjiv PrabhunandanAfroz K. (PM)Hadoop Platforms Team: Raghu Agani, Elliott Brown, William WangEnterprise Data Platforms Leadership: Sudhir Rao, Zhenyin Yang, Michael Zeltser, Prasanna Krishna, Toby YuGlobal Data Science Leadership: Allon Borkovski, Smadar GazitNext-Gen Data Movement Platform at PayPal was originally published in Technology at PayPal on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2021-02-04 14:52:48","link":"https://medium.com/paypal-tech/next-gen-data-movement-platform-at-paypal-100f70a7a6b?source=rss----6423323524ba---4","blog":{"id":"paypal","link":"https://medium.com/paypal-engineering","name":"Paypal","rssFeed":"https://medium.com/feed/paypal-engineering","type":"company"},"blogType":"company"},{"id":"e132a73b81f138e96c94feb0b9e68aa6","publish_timestamp":1607440899,"title":"The Journey of metadata at PayPal","blogName":"Paypal","image":"https://miro.medium.com/max/1200/1*n-FcxBuC-zofl481lCHKDA.png","categories":["datagovernance","datacatalog","metadatamanagement","metadata","data"],"description":"The journey of metadata at PayPalWith 2020 ending and many of us looking forward to 2021, it’s a good time to reflect on the last three-and-a-half years of PayPal’s journey with metadata.Metadata is not new to PayPal. Attempts to create a comprehensive metadata system predates PayPal’s Unified Data Catalog (UDC), which today is the Enterprise Data Catalog at PayPal.As an introduction, this picture shows where we are today with UDC.Unified Data Catalog — High Level ArchitectureToday, UDC is a central part of everything that deals with data at PayPal. Here’s how we conceived, incubated, and evolved the enterprise data catalog from scratch:Through the multi-year journey, we’ve made a giant leap forward in terms of making UDC the official enterprise data catalog for PayPal. We’ve assimilated and deferred responsibilities of duplicate products and features that have existed traditionally. We’ve made decisions that may not reflect on the roadmap or the features available on product. Hence, it’s meaningful to walk through how we navigated the ecosystem and organization dynamics to where we are today.Last but not least, our story isn’t complete without details of how the team started and evolved over the last three years.We’ll be following up on metadata at PayPal with more posts in the following areas:UDC architecture and how it fits in PayPal’s larger data landscapeTechnical deep-dive into the mechanics of data catalog and metadata crawling at scaleLessons learned and fundamental problems that we’re striving to solveAuthor :Deepak Chandramouli | Engineering Lead, Unified Data Catalog @ PayPalSpecial ThanksEnterprise Data Governance partners : who are driving the strategy &amp; vision to build a comprehensive metadata solution for the company : Subra Munaganuru, Thomas Castriota, Funmi BalogunEnterprise Data Platforms Leadership : Sudhir Rao, Sudhir Muthuraj, Prasanna KrishnaUDC Team :Dheeraj Rampally | Staff Software Engineer, PayPal [ Lead | UDC foundations stack ]Harsh Bhimani | Staff Software Engineer, PayPal [ Lead | UDC Metadata Discovery stack ]Shraddha Yeole | Software Engineer, PayPal [ Unified Data Catalog Engineering ]Aishwarya Mohan | Software Engineer, PayPal [ Unified Data Catalog Engineering ]Romit Mehta | Lead Product Manager, PayPal [ Product manager | UDC |2018,2019 &amp; early 2020 ]Daniel Silva | Product Manager, PayPal [ Product manager | UDC | 2020 &amp; beyond ]Vladimir Bacvanski | Principal Architect, Strategic Architecture, PayPalThe Journey of metadata at PayPal was originally published in Technology at PayPal on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-12-08 15:21:39","link":"https://medium.com/paypal-tech/the-journey-of-metadata-at-paypal-c374ac66e2e6?source=rss----6423323524ba---4","blog":{"id":"paypal","link":"https://medium.com/paypal-engineering","name":"Paypal","rssFeed":"https://medium.com/feed/paypal-engineering","type":"company"},"blogType":"company"},{"id":"df0524ae6d093abf9921c742a777c8f1","publish_timestamp":1607110941,"title":"A Year (plus a little) on TC39","blogName":"Paypal","image":"https://miro.medium.com/max/1200/0*eXjinrzgkTr-Zzs-","categories":["standards","ecmascript","javascript"],"description":"Dispatches from the future of JavaScriptA snapshot of the May, 2018 TC39 meeting. Courtesy Ross Kirsling.Author’s note: In this post while explaining where a certain programming language comes from, I use the terms “JavaScript” and “ECMAScript®” interchangeably. There is a difference, but it’s pedantic and not worth getting into today.In August 2017, fellow PayPal engineering JavaScripter Kent C. Dodds contacted me with an opportunity: PayPal is an “Ordinary Member” of Ecma International, he told me, and that meant that PayPal could send any delegate(s) they wish to attend meetings of TC39, the committee that writes the spec for JavaScript. He wanted to know if I’d be interested in representing PayPal, Braintree, and Venmo in the committee. I had never thought about where JavaScript came from, but suddenly I was eager to find out. So I said “yes” without giving the commitment much thought, then started researching what, exactly, I had volunteered for.Wait — someone does that?All the ES5 and ES6 features I’d been so excited about a few years ago had to weave their way through the TC39 committee process. Changes to the spec aren’t added at random by a browser engineer anxious to implement a feature in their next release, but go through an actual process with actual experts who know what they’re talking about.The dedicated folks who make up TC39 come from all manner of professional backgrounds. We have developers at every level of interaction with JavaScript: people who work on JavaScript engines, framework authors, web developers, and more. Having such a mixture of experience allows us to put care and thought into every decision we make, and understand how it will impact anyone who deals with JavaScript.Don’t break the webThe number one guiding principle, which the committee is unbending on, is “Don’t Break the Web”. Frequently, JavaScript community members don’t see just how broadly that applies to every single decision we make.We have to be unbending on that principle because, unlike most languages, JavaScript needs to be backward compatible — forever. When contemplating “breaking” changes, we can’t just call it a major version bump and move on — once something’s in, it’s in for good. (Just think of what would happen to all those archived Geocities sites if we removed String.prototype.fontcolor(color)). (Please do not ever, ever use String.prototype.fontcolor(color)) We pay close attention to common usage in the past and take note of any frameworks and conventions that were used regularly at some point, even if they aren’t common now. Naturally, we’re also on a mission to future-proof to the best of our ability. We don’t want to make it harder for ourselves to make additions or changes in the future. (The word “footgun” is thrown about a lot during plenary.)We also have to look outside of the ECMAScript® spec to make sure our work plays nice with anything JavaScript regularly interacts with. Several delegates are directly or indirectly involved with other related standards bodies, like W3C’s TAG, WHATWG, IETF, and others. By collaborating with other standards groups, we make sure that conflicting ideas can be worked out, and that work isn’t unnecessarily duplicated.The naming of things¹Whether in collaboration with other standards bodies or independently, TC39 has to put an absurd amount of thought, research, and planning into the naming of things. Naming is famously regarded as the hardest thing in programming, but it’s triply so when working in a language like JavaScript. In addition to the general challenges that make naming hard, the struggle is compounded by “it can never be changed or deleted” as well as “it can’t be so obvious that people are already using it in a way that would cause a conflict”. (See also, that time a Stage 3 proposal broke flickr).An average meetingMeeting expectations: The JavaScript dark council my coworkers joked about. Courtesy jonobacon under CC-BY 2.0 license.Meeting reality: My first, September 2017, hosted by Bocoup in Boston.When we get together in person for three day bimonthly plenaries, the agenda follows a predictable format even if the content varies dramatically. Agenda items come from the editors of our various publications, members of subcommittees, updates from other standards bodies, security issues, and of course, proposal updates. (I’ll go into the proposal process, which drives the vast majority of feature additions and changes to JavaScript, in a future post).Early in the first day, editors and subcommittee members report on their activities between meetings. The editors of the spec² take a moment to update everyone on any changes and their relevance to ongoing work. Additional editor’s reports include those from the internationalization API and JSON specs, as well as Test262, the canonical ECMAScript® test suite. We also have the Code of Conduct committee and several outreach subcommittees that offer updates of their activities between meetings.TC39’s approach to its work is unique in the standards world; decisions are made by consensus, as opposed to voting like other standards bodies. The primary reason for this difference is the near-immediate and unavoidable burden that stakeholders take on when changes and additions are made to the language. The consequence is that new features mean more work for implementors, who need a way to put a stop (whether temporarily or permanently) to changes that they cannot execute on.Because we need consensus, conversations can break out of their time boxes easily. This can be frustrating from time to time, but it allows for concerns to be heard from all angles. JavaScript is run in such a wide variety of environments — of course browsers, but also native applications, IoT devices and in space — that we discover unexpected constraints in the course of a discussion.During meetings, a small number of praiseworthy scribes take notes almost detailed enough to be considered transcripts. They also make a note of final conclusions of the topic at hand, whatever it may be. About two weeks after everyone goes home, notes are published on GitHub for anyone to read. If you’ve ever seen changes to JavaScript and thought “what on earth were they thinking?”, the notes can be especially illuminating. The story behind every conclusion is buried somewhere in that repo, just waiting to be read.The real work startsThe bulk of the work TC39 does happens between meetings on GitHub and IRC. The lessons learned and conclusions reached in the previous meeting are applied to the code, prose, and tests found in proposal repos. Over in the world of browsers and JavaScript engines, engineers start to implement features that are ready for real-world testing. And of course, bikeshedding³ abounds in the GitHub issues.We don’t fill our time with purely technical work, though. We’re currently developing several nascent outreach groups for specific constituents like educators and framework authors. Our goal is to offer more lines of communication than just GitHub issues and IRC, so we can reach the community where they are and solicit feedback. There’s also a beta website in the works, with information on current proposals, links to meeting notes, and explanations of what we do and how.Back at BraintreeIt’s my firm belief that no one needs to be an expert in every aspect of JavaScript. I work with TC39 as a Braintree software engineer, not an author of a browser or engine. Participating in the standards process has really driven home how to communicate the most important priorities in development for the web. Those priorities take many shapes, but “Don’t break the web” should be a principle not just for this committee, but for anyone who works on SDKs, authors frameworks, or publishes packages of any kind. When it comes to building products people depend on, as my manager recently advised: “Move slow and maintain things.”This post is just an introduction to what TC39 does and how we work together to move JavaScript forward. As co-chair in 2019, I’m looking forward to providing more insight into the committee and making sure new voices are heard. If you’d like me to cover a specific facet of TC39 in another post, drop me a line at aki.braun@getbraintree.com.Yes, this song is stuck in my head. It’s a good title.For ES2019, editor-in-chief Brian Terlson with editors Jordan Harband &amp; Bradley FariasBikeshedding, also known as Parkinson’s law of triviality, is the idea that organizations give disproportionate weight to trivial issues. The fictional example Parkinson gave was a committee analyzing plans for a nuclear power plant, but spending the majority of its time discussing details such as the color and construction of staff bike shed, while neglecting the more complex problem of the design of the plant itself. While this is a common problem in programming in general, TC39 considers it an important part of our process.A Year (plus a little) on TC39 was originally published in Technology at PayPal on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-12-04 19:42:21","link":"https://medium.com/paypal-tech/a-year-plus-a-little-on-tc39-a1acb87eb862?source=rss----6423323524ba---4","blog":{"id":"paypal","link":"https://medium.com/paypal-engineering","name":"Paypal","rssFeed":"https://medium.com/feed/paypal-engineering","type":"company"},"blogType":"company"},{"id":"97367dc36ed6cd6977a16c947e8e99e3","publish_timestamp":1605717249,"title":"Three Things I Learned as a Remote Intern at PayPal","blogName":"Paypal","image":"https://miro.medium.com/max/1200/1*pCTkHxJM2Dewr42IKXD6Qw.jpeg","categories":["paypal","intern","learning","remoteworking"],"description":"When I got an internship offer from Braintree, I was really excited to come and work in Texas. I’d never been in the southern part of the United States, let alone Texas. I was hoping to meet many new people, explore the State Capitol, visit museums, go on bike tours, see the botanical gardens, etc. However, all my excitement came crashing down when I found out that I was going to be working remotely.Although I was assured the PayPal University Recruiting Team was hard at work making sure the transition from in-person to remote would be seamless, I couldn’t help but wonder: will the experience be the same? How am I going to form friendships or have any sort of relationship with my coworkers? Will I be able to learn effectively?Fortunately, my worries were put to rest within a couple of months of working at PayPal. Here’s what I learned about PayPal and its culture working remotely.1) PayPal has a variety of resources for learningMy team, Payment Experiences, was working with Ruby, Vim, Tmux, and Cpairs, which were all unfamiliar tools to me. I had access to LinkedIn Learning, Udemy, and O’Reilly, which helped me learn essential skills for my job. Also, these tools came in handy for open-dev day. Open-dev day is a day dedicated every other week for employees to learn anything they would like.Although I learned a lot with these resources, I often found that I learned more by talking to people. Every week, we could sign up for intern meetups in which we would be paired with another intern to chat with. I learned about more parts of the company by talking to other interns. They often guided me to other resources that I could use for additional assistance.2) Your team wants you to succeedEvery week I had check-ins with both my mentor and manager. I learned valuable skills from both of them on a variety of topics, including how to navigate through code, review PRs, and contribute to discussions. I was also paired with a trail guide dedicated to helping me ramp up on the different apps our team uses and the tools used to debug for the first half of my internship. On top of this, when I had questions, my questions were answered quickly, team members gave me effective feedback, and every single member of my team helped me get closer to my academic goals.3) PayPal is dedicated to listening to your concerns.Every month we have a meeting in which we anonymously write about things that we’re happy, sad, and mad about. After doing this, all of the members can see what’s been written on different notecards, and we all get to vote on which notecards we’d like to talk about.During one of those meetings, someone wrote they wanted to improve the diversity on our team. I was happy to see many people on our team contribute to the discussion. Today I’m proud to say my team is made up of 18 people from over 10 states and 3 continents.I had an experience that exceeded my expectations. I’m so grateful for all the friendships I’ve made, the people I’ve learned from, and the opportunity to work at one of the leading tech companies supporting online payments.Apply here!Three Things I Learned as a Remote Intern at PayPal was originally published in Technology at PayPal on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-11-18 16:34:09","link":"https://medium.com/paypal-tech/three-things-i-learned-working-remotely-c52146e90c85?source=rss----6423323524ba---4","blog":{"id":"paypal","link":"https://medium.com/paypal-engineering","name":"Paypal","rssFeed":"https://medium.com/feed/paypal-engineering","type":"company"},"blogType":"company"},{"id":"f145dd64cc6aec8d9995166c24fb4e56","publish_timestamp":1604502117,"title":"Introducing Pay in 4 from PayPal. Here’s What Devs Need to Know.","blogName":"Paypal","image":"https://miro.medium.com/max/1042/1*DNf6m-lweuaSv8wKVW71lA.png","categories":["credit","payments","javascript","sdk","paypal"],"description":"Introducing Pay in 4 from PayPal: Here’s What Devs Need to KnowAs someone on the developer team who writes documentation for PayPal, I had a front row seat to the design process of PayPal’s new pay later documentation.I’m especially excited for the opportunities PayPal’s new pay later offerings could open for businesses: growing their sales, attracting new customers, and driving customer loyalty. But before I get ahead of myself, I should explain pay later.PayPal pay later options include Pay in 4 and PayPal Credit. Pay in 4 is a new payment offering that enables customers to pay for their purchases in four interest-free payments.PayPal Credit* offers special financing. Both pay later options are included in the latest PayPal Checkout integration. Customers get the flexibility to pay for their purchases over time while businesses get paid up front at no extra cost.Integrate with the PayPal JavaScript SDKTo show pay later options at checkout, make sure you integrate with the PayPal JavaScript SDK. If you’re using a legacy integration, we’ve got you covered with documentation to upgrade to the latest JavaScript SDK.The SDK will show a pay later button if:You’re a US-based seller with a US-facing websiteYou’ve integrated a PayPal payment solution that doesn’t include vaulted payments or billing agreementsAdd pay later messagingIntegrate the messages component of the PayPal JavaScript SDK to add pay later messages across your site. One of my favorite features of pay later messaging — it dynamically displays messaging for the most relevant pay later offer based on what the customer is shopping for. This can help grow online sales and average order values.If you built your site yourselfIf you built your site yourself, you can integrate pay later messaging by adding the PayPal JavaScript SDK to your web page and determining where to render the messages. Here’s the documentation. PayPal recommends lightweight, contextual messaging placed near a product’s price. To maximize sales, put messaging in product and checkout pages.If you’re using a commerce platformIf you’re using a commerce platform, you can turn on pay later messaging on your site — no integration needed. You can use our documentation to enable pay later messaging on these platforms:3dcartBigCommerceCentarroIWDMivaPinnacleCartSalesforceWooCommerceTell us what you thinkWe’d love if you told us what you think of our documentation. Select the Feedback tab on PayPal Developer and hit us with your best shot.To get started with pay later offers and pay later messaging, or contact sales, check out https://www.paypal.com/us/business/buy-now-pay-later.*PayPal Credit is subject to consumer credit approval.Introducing Pay in 4 from PayPal. Here’s What Devs Need to Know. was originally published in Technology at PayPal on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-11-04 15:01:57","link":"https://medium.com/paypal-tech/introducing-pay-in-4-from-paypal-heres-what-devs-need-to-know-12287bd362c1?source=rss----6423323524ba---4","blog":{"id":"paypal","link":"https://medium.com/paypal-engineering","name":"Paypal","rssFeed":"https://medium.com/feed/paypal-engineering","type":"company"},"blogType":"company"},{"id":"549b58ff6bffab731d83e6c4d8525758","publish_timestamp":1603810659,"title":"Decentralizing UI Development with Module Federation","blogName":"Paypal","image":"https://miro.medium.com/max/1200/1*S3kxo7GJ7RG4BHe4AcjL8Q.jpeg","categories":["react","webpack","uidevelopment","modulefederation","paypal"],"description":"“You can always be a little bit better” — Drew BreesComponent distribution is hard, especially in a global company. PayPal alone has thousands of components across its network. These components have many versions in production.To help solve this issue as well as other concerns around UI development, we created Component Explorer. Component Explorer is a UI development gateway that allows our developers to test, share, and consume UI components across our ecosystem. You can read more about it in a previous blog post here: https://medium.com/paypal-engineering/reusing-ui-components-at-enterprise-level-a7df1ea1f8ddBut, as my favorite NFL quarterback Drew Brees states, you can always be a little bit better. We have been striving to make the UI development process better. Some of the problems we have run into are the result of centralized package repositories such as npm.Centralized Distribution: Not a Perfect SolutionNow, I see the crowd gathering pitchforks and torches, but hear me out. I love npm for its many features and its immense development community. However, npm’s centralized architecture has its drawbacks when it comes to UI development.Development Velocity and YouFirst of all, development velocity is one of the key measures of a company’s success and movement forward. Rigid versioning hinders this. With rigid versioning, UI development is held hostage to the cycle of deploy, install, and redeploy that npm package usage requires, causing development delays. When one UI component’s release does not coincide with a consuming application’s release cycle, you can lose up to an entire sprint’s time waiting for the next release cycle to adopt the new version.Consistency is KeyAlong with this release cycle delay, versioning can cause many versions of a UI component to be in use, hindering the consistency of a product’s UI. This UI/UX consistency is arguably one of the most important targets of a company’s UI developers.So how we do address these issues? We have to completely rethink and redesign how we share components within a tech stack. This is where module federation comes in.Module Federation: A New ApproachYou may have heard of module federation. It’s a brand new feature within webpack, a component bundler used all over the modern web. Module federation is a feature of webpack 5, the newest version of webpack.Module federation is the ability to publish and consume UI components without using a centralized package repository like npm. This decentralized, app-to-app approach allows you to use and share React components with just a few lines of code sprinkled into your applications.Webpack’s creator and maintainer, Zach Jackson, wrote a wonderfully in-depth yet easy-to-understand explanation of this technology and I encourage you to read it if you want to know all things module federation: https://medium.com/swlh/webpack-5-module-federation-a-game-changer-to-javascript-architecture-bcdd30e02669Benefits of Module FederationWith module federation, we can solve both the release cycle issues as well as the UI consistency issues common with npm usage. Along with this, module federation comes with a host of benefits includingEasier bug fixesFewer bugs in productionBetter customer experiencesUnified UI by designFaster deploy timesConsistent versioningBecause of all of these benefits, as well as the ability to address the underlying issues with npm usage, we at PayPal have created an experimental proof-of-concept to test out this technology and its benefits.How we tried out module federationFirst of all, we upgraded our sample app to include module federation. This sample app allows our developers to create component libraries that come with Component Explorer, Storybook, and module federation support out of the box, integrating all new component libraries into this cutting edge by default.We could have stopped there, as greenfield development support is the easier of the two tasks involved with new technology integration. However, we wanted to show our developers that not only creating new components with this technology was possible and beneficial, but that onboarding existing component libraries to module federation would reap immense benefits for them as well as our customers.In order to do this, we had to onboard an existing application as a proof-of-concept. We could have picked a simple, non-real component library within our ecosystem to onboard, as this would have been the easier approach. However, we chose to onboard Component Explorer itself to module federation for two reasons.Real Example, Real ResultsFirst, we wanted a real-world example of the power of this technology. Choosing a small, insignificant component library to onboard would not be a powerful demonstration of this tech. However, Component Explorer is a relatively complex application with a multitude of UI components within its library. It has real impact on the development ecosystem our internal developers are aware of and see regularly.Module Federation + Component ExplorerSecondly, module federation within our ecosystem is not meant to exist in a vacuum. As a technology, we wanted to couple the benefits of module federation with the existing benefits of Component Explorer in order to more holistically approach and resolve the many issues and concerns surrounding UI development. That is, Component Explorer acts as a gateway for UI development and we wanted Component Explorer now also act as a gateway for federated UI development.In ConclusionWith these two technologies acting hand-in-hand to help ease the sharing of components within our ecosystem, we hope to create a better developer experience within our company, as well as a better customer experience for our hundreds of millions of loyal PayPal customers worldwide.Decentralizing UI Development with Module Federation was originally published in Technology at PayPal on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-10-27 14:57:39","link":"https://medium.com/paypal-tech/decentralizing-ui-development-with-module-federation-90dbfb6ace29?source=rss----6423323524ba---4","blog":{"id":"paypal","link":"https://medium.com/paypal-engineering","name":"Paypal","rssFeed":"https://medium.com/feed/paypal-engineering","type":"company"},"blogType":"company"},{"id":"b0d053df67627b8a6bc1244da1e4f505","publish_timestamp":1602595564,"title":"Three Key Practices of Successful Product Managers","blogName":"Paypal","image":"https://miro.medium.com/max/1200/1*WsHlIK_aQmyXWmem1DTu8g.jpeg","categories":["productmanagement","leadership","customerempathy","womeninproduct","datadrivendecisions"],"description":"Product management is one of the most sought-after professions in tech. Why? A product manager is often viewed as the individual responsible for driving a product to utter success or complete failure. So how do product managers do it?Photo by Vlad Bagacian on UnsplashWhile the industry has varying views of what a product manager does, at PayPal, what makes one truly successful is a combination of customer empathy, leading without authority, and a data-driven approach to solving real-world problems. I’ve summarized three practices that will help you master being a successful product manager.You may have heard that the product manager is considered the CEO of the product. Not true. A product manager truly has all the responsibility, but none of the authority of a CEO. In other words, a product manager needs to lead without authority.So how does a product manager then lead a product to success?Start with DiscoveryBefore you determine what to build, determine who to build it for.Who is your customer? What are their biggest problems? At PayPal, the journey of product building begins with customer discovery.Customer discovery is quintessential for developing empathy for our customers — one of the key ingredients to being a successful product manager. Product teams that span different organizations come together to learn about our target customer segments through interviews and surveys. After weeks of discovery and hours of interviews, the discovery team identifies all the problem statements. Then we narrow them down to the most common and important problems the customer is facing. Finally, product managers define the top-level requirements to arrive at the distilled product description.It is common to make assumptions about the customer, especially as the product manager of an internal team delivering products for customers within your company. As a product manager for PayPal’s AI &amp; machine learning platform, we started our product journey with several assumptions and even had a product proposal in mind. However, we followed all the principles of discovery and conducted over 50 interviews with our stakeholders in consumer, merchant, corporate strategy, and customer services. At the end of discovery, our product ended up looking different from the initial proposal, and our customers were glad to see their pain points being directly addressed in the product.Create a Safe Environment for Your TeamBeing a product manager is like having all the responsibility of a CEO, but none of the authority. A successful product manager is one who is able to lead a cross-functional team of people, without direct managerial authority.The people, process, and product journey starts with people. As a PM, you have a goal to deliver a product and satisfy key success criterion. To do so, you are required to come up with a coherent plan, which will evolve along the journey. A big part of your plan will include the people you choose to recruit.Think about an analogy of climbing a mountain. In the beginning, when you build, you plan for the journey. You rarely know about the obstacles you will encounter along the way. Similarly, a product manager knows very little about the problems they will encounter. A key engineer may fall ill, deliverables from dependent teams may get delayed, your team size may shrink, etc. To help you navigate these obstacles and recalibrate your plan, you need to ensure you have an amazing group of experts to help you assess the situation and adapt the plan until you achieve your goal.As a product manager and as a leader, one of the most important things you can do is to empower your teammates.This ensures you are not making decisions alone. Create a safe environment in which everyone has a voice, and where diverse opinions are respected. You make decisions collectively, so you progress collectively, and everyone feels more committed every step you take forward.Make Data-Driven DecisionsThroughout the journey of building a product, from concept, to launch, and beyond, there will be decisions that need to be made. During the design and build phase, a product manager has to make decisions on which feature to prioritize, which metrics should they monitor, and what type of instrumentation they need to build into the product to better understand the performance. During launch, product managers need to determine what experiments should run i.e. during A/B testing, what segments they should target, which country to launch in, etc. Post launch, which is the most critical time to learn how your product is performing, they need to study the data, and monitor all KPIs to understand which features resonate with their customers. This will give them cues to investigate and improve the performance of the product going forward.Building new products for emerging markets is especially hard, since you are operating in an evolving, untested market where you are not sure who your segment or customers are yet. You are at the forefront of innovation, so you don’t have data about adoption and competitor success. And yet as a product manager, your entire team is looking at you to determine the market segment, forecast revenue and sales, determine a path for profitability as well as handle operational details. Sound impossible? Not really! This is where you rely on early adopters to help shape your product until it is ready for the total addressable market.In ConclusionMany product managers focus on execution and do not realize the importance of these fundamental practices. However, in addition to sound execution, being successful as a product manager means evolving into a true customer champion, creating safety for your team, and making data-driven decisions. These practices will help both new and seasoned product managers develop a customer-centric mindset and enable them to be true customer champions.Three Key Practices of Successful Product Managers was originally published in Technology at PayPal on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-10-13 13:26:04","link":"https://medium.com/paypal-tech/three-key-practices-of-successful-product-managers-825d42b3cd28?source=rss----6423323524ba---4","blog":{"id":"paypal","link":"https://medium.com/paypal-engineering","name":"Paypal","rssFeed":"https://medium.com/feed/paypal-engineering","type":"company"},"blogType":"company"},{"id":"b09b65f5315f68de297375771d2c8d91","publish_timestamp":1601914649,"title":"A bird’s-eye view on Java concurrency frameworks","blogName":"Paypal","image":"https://miro.medium.com/max/1200/0*58-KZz-qCY4VHYI0.png","categories":["java","javaconcurrency"],"description":"A Bird’s-Eye View on Java Concurrency FrameworksThe Why QuestionA few years ago, when NoSQL was trending, like every other team, our team was also enthusiastic about the new and exciting stuff; we were planning to change the database in one of the applications. But when we got into the finer details of the implementation, we remembered what a wise man once said, “the devil is in the details,” and eventually, we realized that NoSQL is not a silver bullet to fix all problems, and the answer to NoSQL VS RDMS was: “it depends.” Similarly, in the last year, concurrency libraries like RxJava and Spring Reactor were trending with enthusiastic statements, like the asynchronous, non-blocking approach is the way to go, etc. In order to not make the same mistake again, we tried to evaluate how concurrency frameworks like ExecutorService, RxJava, Disruptor, and Akka differ from one another and how to identify the right use case for respective frameworks.Terminologies used in this article are described in greater detail here.Sample Use Case for Analyzing Concurrency FrameworksQuick Refresher on Thread ConfigurationBefore getting into a comparison of concurrency frameworks, let’s have a quick refresher on how to configure the optimal number of threads to increase the performance of parallel tasks. This theory applies to all frameworks, and the same thread configuration has been used in all frameworks to measure performance.For in-memory tasks, the number of threads equals approximately the number of cores that has the best performance, though it can change a bit based on the hyper-threading feature in the respective processor.For example, in an 8-core machine, if each request to an application has to do four in-memory tasks in parallel, then the load on this machine should be maintained @ 2 req/sec with 8 threads in ThreadPool.For I/O tasks, the number of threads configured in the ExecutorService should be based on the latency of an external service.Unlike the in-memory task, the thread involved in the I/O task will be blocked and it will be in a waiting state until an external service responds or times out. So, when I/O tasks are involved, as the threads are blocked, the number of threads should be increased to handle the additional load from concurrent requests.The number of threads for I/O task should be increased in a conservative way, as many threads in the Active state bring the cost of context-switching, which will impact application performance. To avoid that, the exact number of threads and load of this machine should be increased proportionately to the waiting time of the threads involved in I/O task.Reference: http://baddotrobot.com/blog/2013/06/01/optimum-number-of-threads/Performance ResultsPerformance tests ran in GCP -&gt; processor model name: Intel® Xeon® CPU @ 2.30GHz; Architecture: x86_64; No. of cores: 8 (Note: These results are subjective to this use-case and don’t imply one framework is better than another).Parallelize IO Tasks With Executor ServiceWhen to Use?If an application is deployed in multiple nodes and if req/sec in each node is less than the number of cores available, then the ExecutorService can be used to parallelize tasks and execute code faster.When Not to Use?If an application is deployed in multiple nodes and if the req/sec in each node is much higher than the number of cores available, then using ExecutorService to further parallelize can only make things worse.Performance results when the delay of external service is increased to 400 ms (request rate @ 50 req/sec in 8 core machine).Example When all Tasks Are Executed in Sequential Order:// I/O tasks : invoke external servicesString posts = JsonService.getPosts();String comments = JsonService.getComments();String albums = JsonService.getAlbums();String photos = JsonService.getPhotos();// merge the response from external service// (in-memory tasks will be performed as part this operation)int userId = new Random().nextInt(10) + 1;String postsAndCommentsOfRandomUser = ResponseUtil.getPostsAndCommentsOfRandomUser(userId, posts, comments);String albumsAndPhotosOfRandomUser = ResponseUtil.getAlbumsAndPhotosOfRandomUser(userId, albums, photos);// build the final response to send it back to clientString response = postsAndCommentsOfRandomUser + albumsAndPhotosOfRandomUser;return response;Code Example When I/O Tasks Are Executed in Parallel With the ExecutorService// add I/O TasksList&lt;Callable&lt;String&gt;&gt; ioCallableTasks = new ArrayList&lt;&gt;();ioCallableTasks.add(JsonService::getPosts);ioCallableTasks.add(JsonService::getComments);ioCallableTasks.add(JsonService::getAlbums);ioCallableTasks.add(JsonService::getPhotos);// Invoke all parallel tasksExecutorService ioExecutorService = CustomThreads.getExecutorService(ioPoolSize);List&lt;Future&lt;String&gt;&gt; futuresOfIOTasks = ioExecutorService.invokeAll(ioCallableTasks);// get results of I/O operation (blocking call)String posts = futuresOfIOTasks.get(0).get();String comments = futuresOfIOTasks.get(1).get();String albums = futuresOfIOTasks.get(2).get();String photos = futuresOfIOTasks.get(3).get();// merge the response (in-memory tasks will be part of this operation)String postsAndCommentsOfRandomUser = ResponseUtil.getPostsAndCommentsOfRandomUser(userId, posts, comments);String albumsAndPhotosOfRandomUser = ResponseUtil.getAlbumsAndPhotosOfRandomUser(userId, albums, photos);//build the final response to send it back to clientreturn postsAndCommentsOfRandomUser + albumsAndPhotosOfRandomUser;Parallelize IO Tasks With Executor Service (CompletableFuture)This works similar to the above case: the HTTP thread, which handles the incoming request, will be blocked, and CompletableFuture is used to handle the parallel tasksWhen to Use?Without AsyncResponse, performance is the same as the ExecutorService. If multiple API calls have to be async and if it has to be chained, this approach is better (which is similar to Promises in Node.)ExecutorService ioExecutorService = CustomThreads.getExecutorService(ioPoolSize);// I/O tasksCompletableFuture&lt;String&gt; postsFuture = CompletableFuture.supplyAsync(JsonService::getPosts, ioExecutorService);CompletableFuture&lt;String&gt; commentsFuture = CompletableFuture.supplyAsync(JsonService::getComments,    ioExecutorService);CompletableFuture&lt;String&gt; albumsFuture = CompletableFuture.supplyAsync(JsonService::getAlbums,    ioExecutorService);CompletableFuture&lt;String&gt; photosFuture = CompletableFuture.supplyAsync(JsonService::getPhotos,    ioExecutorService);CompletableFuture.allOf(postsFuture, commentsFuture, albumsFuture, photosFuture).get();// get response from I/O tasks (blocking call)String posts = postsFuture.get();String comments = commentsFuture.get();String albums = albumsFuture.get();String photos = photosFuture.get();// merge response (in-memory tasks will be part of this operation)String postsAndCommentsOfRandomUser = ResponseUtil.getPostsAndCommentsOfRandomUser(userId, posts, comments);String albumsAndPhotosOfRandomUser = ResponseUtil.getAlbumsAndPhotosOfRandomUser(userId, albums, photos);// Build final response to send it back to clientreturn postsAndCommentsOfRandomUser + albumsAndPhotosOfRandomUser;Parallelize All Tasks With ExecutorServiceParallelize all tasks with the ExecutorService and use @Suspended AsyncResponse response to send a response in a non-blocking way.[io vs nio]Images from http://tutorials.jenkov.com/java-nio/nio-vs-io.htmlIncoming requests will be handled via an event-pool and the request will be passed to the Executor pool for further processing, and when all tasks are done, another HTTP-thread from event-pool will send the response back to the client. (asynchronous and non-blocking).The reason for a drop in performance:In synchronous communication, though the thread involved in I/O task was blocked, the process will still be in running state as long as it has additional threads to take the load of concurrent requests.So, the benefit that comes from keeping a thread in a non-blocking manner is very less and the cost involved to handle the request in this pattern seems to be high.More often than not, using the asynchronous non-blocking approach for the use case we discussed here will bring down application performance.When to Use?If the use case is like a server-side chat application where a thread need not hold the connection until the client responds, then the async, non-blocking approach can be preferred over synchronous communication; in those use cases, rather than just waiting, system resources can be put to better use with the asynchronous, non-blocking approach.// submit parallel tasks for Async executionExecutorService ioExecutorService = CustomThreads.getExecutorService(ioPoolSize);CompletableFuture&lt;String&gt; postsFuture = CompletableFuture.supplyAsync(JsonService::getPosts, ioExecutorService);CompletableFuture&lt;String&gt; commentsFuture = CompletableFuture.supplyAsync(JsonService::getComments,ioExecutorService);CompletableFuture&lt;String&gt; albumsFuture = CompletableFuture.supplyAsync(JsonService::getAlbums,ioExecutorService);CompletableFuture&lt;String&gt; photosFuture = CompletableFuture.supplyAsync(JsonService::getPhotos,ioExecutorService);// When /posts API returns a response, it will be combined with the response from /comments API// and as part of this operation, some in-memory tasks will be performedCompletableFuture&lt;String&gt; postsAndCommentsFuture = postsFuture.thenCombineAsync(commentsFuture,(posts, comments) -&gt; ResponseUtil.getPostsAndCommentsOfRandomUser(userId, posts, comments),ioExecutorService);// When /albums API returns a response, it will be combined with the response from /photos API// and as part of this operation, some in-memory tasks will be performedCompletableFuture&lt;String&gt; albumsAndPhotosFuture = albumsFuture.thenCombineAsync(photosFuture,(albums, photos) -&gt; ResponseUtil.getAlbumsAndPhotosOfRandomUser(userId, albums, photos),ioExecutorService);// Build the final response and resume the http-connection to send the response back to client.postsAndCommentsFuture.thenAcceptBothAsync(albumsAndPhotosFuture, (s1, s2) -&gt; {LOG.info(&quot;Building Async Response in Thread &quot; + Thread.currentThread().getName());String response = s1 + s2;asyncHttpResponse.resume(response);}, ioExecutorService);RxJava/RxNettyThe main difference of RxJava/RxNetty combination is, it can handle both incoming and outgoing requests with an event-pool there by making the I/O tasks completely non-blocking.Also, RxJava gives better DSL to write code in a fluid manner, which may not be visible with this example.Performance is better than handling parallel tasks with CompletableFutureWhen to Use?If the asynchronous, non-blocking approach suits a use-case, then RxJava or any reactive libraries can be preferred. It has additional capabilities like back-pressure, which can balance the load between producers and consumers.// non blocking API call from Application - getPosts()  HttpClientRequest&lt;ByteBuf, ByteBuf&gt; request = HttpClient.newClient(MOCKY_IO_SERVICE, 80)  .createGet(POSTS_API).addHeader(&quot;content-type&quot;, &quot;application/json; charset=utf-8&quot;);rx.Observable&lt;String&gt; rx1ObservableResponse = request.flatMap(HttpClientResponse::getContent)  .map(buf -&gt; getBytesFromResponse(buf))  .reduce(new byte[0], (acc, bytes) -&gt; reduceAndAccumulateBytes(acc, bytes))  .map(bytes -&gt; getStringResponse(bytes, &quot;getPosts&quot;, startTime));int userId = new Random().nextInt(10) + 1;// Submit parallel I/O tasks for each incoming request.Observable&lt;String&gt; postsObservable = Observable.just(userId).flatMap(o -&gt; NonBlockingJsonService.getPosts());Observable&lt;String&gt; commentsObservable = Observable.just(userId)  .flatMap(o -&gt; NonBlockingJsonService.getComments());Observable&lt;String&gt; albumsObservable = Observable.just(userId).flatMap(o -&gt; NonBlockingJsonService.getAlbums());Observable&lt;String&gt; photosObservable = Observable.just(userId).flatMap(o -&gt; NonBlockingJsonService.getPhotos());// When /posts API returns a response, it will be combined with the response from /comments API// and as part of this operation, some in-memory tasks will be performedObservable&lt;String&gt; postsAndCommentsObservable = Observable.zip(postsObservable, commentsObservable,                                                               (posts, comments) -&gt; ResponseUtil.getPostsAndCommentsOfRandomUser(userId, posts, comments));// When /albums API returns a response, it will be combined with the response from /photos API// and as part of this operation, some in-memory tasks will be performedObservable&lt;String&gt; albumsAndPhotosObservable = Observable.zip(albumsObservable, photosObservable,                                                              (albums, photos) -&gt; ResponseUtil.getAlbumsAndPhotosOfRandomUser(userId, albums, photos));// build final responseObservable.zip(postsAndCommentsObservable, albumsAndPhotosObservable, (r1, r2) -&gt; r1 + r2)  .subscribe((response) -&gt; asyncResponse.resume(response), e -&gt; {    LOG.error(&quot;Error&quot;, e);    asyncResponse.resume(&quot;Error&quot;);  });Disruptor[Queue vs RingBuffer]Image 1: http://tutorials.jenkov.com/java-concurrency/blocking-queues.htmlImage 2: https://www.baeldung.com/lmax-disruptor-concurrencyIn this example, HTTP-thread will be blocked until the disruptor completes the tasks and a CountDownLatch has been used to synchronize the HTTP-thread with the threads from ExecutorService.The main feature of this framework is to handle inter-thread communication without any locks; in ExecutorService, the data between a producer and consumer will be passed via a Queue, and there is a Lock involved during this data transfer between a producer and a consumer. The Disruptor framework handles this Producer-Consumer communication without any Locks with the help of a data-structure called Ring Buffer, which is an extended version of a Circular Array Queue.This library was not meant for use cases like the one we discuss here. It has been added just out of curiosity.When to Use?The Disruptor framework performs better when used with event-driven architectural patterns and when there is a single producer and multiple consumers with the main focus on in-memory tasks.static {int userId = new Random().nextInt(10) + 1;// Sample Event-Handler; count down latch is used to synchronize the thread with http-thread  EventHandler&lt;Event&gt; postsApiHandler = (event, sequence, endOfBatch) -&gt; {  event.posts = JsonService.getPosts();  event.countDownLatch.countDown();  };  // Disruptor set-up to handle events  DISRUPTOR.handleEventsWith(postsApiHandler, commentsApiHandler, albumsApiHandler)  .handleEventsWithWorkerPool(photosApiHandler1, photosApiHandler2)  .thenHandleEventsWithWorkerPool(postsAndCommentsResponseHandler1, postsAndCommentsResponseHandler2)  .handleEventsWithWorkerPool(albumsAndPhotosResponseHandler1, albumsAndPhotosResponseHandler2);  DISRUPTOR.start();}// for each request, publish an event in RingBuffer:Event event = null;RingBuffer&lt;Event&gt; ringBuffer = DISRUPTOR.getRingBuffer();long sequence = ringBuffer.next();CountDownLatch countDownLatch = new CountDownLatch(6);try {event = ringBuffer.get(sequence);event.countDownLatch = countDownLatch;event.startTime = System.currentTimeMillis();} finally {ringBuffer.publish(sequence);}try {event.countDownLatch.await();} catch (InterruptedException e) {e.printStackTrace();}AkkaImage source: https://blog.codecentric.de/en/2015/08/introduction-to-akka-actors/The main advantage of the Akka library is that it has native support to build Distributed Systems.It runs on a system called the Actor System, which abstracts the concept of Threads, and Actors in the Actor System communicate via asynchronous messages, which is similar to the communication between a Producer and Consumer.This extra level of abstraction helps the Actor system provide features like Fault Tolerance, Location Transparency, and more.With the right Actor-to-Thread strategy, this framework can be optimized to perform better than the results shown in the above table. Though it cannot match the performance of a traditional approach on a single node, it can still be preferred for its capabilities to build Distributed and Resilient systems.Sample Code// from controller :Actors.masterActor.tell(new Master.Request(&quot;Get Response&quot;, event, Actors.workerActor), ActorRef.noSender());// handler :public Receive createReceive() {    return receiveBuilder().match(Request.class, request -&gt; {    Event event = request.event; // Ideally, immutable data structures should be used here.    request.worker.tell(new JsonServiceWorker.Request(&quot;posts&quot;, event), getSelf());    request.worker.tell(new JsonServiceWorker.Request(&quot;comments&quot;, event), getSelf());    request.worker.tell(new JsonServiceWorker.Request(&quot;albums&quot;, event), getSelf());    request.worker.tell(new JsonServiceWorker.Request(&quot;photos&quot;, event), getSelf());    }).match(Event.class, e -&gt; {    if (e.posts != null &amp;&amp; e.comments != null &amp; e.albums != null &amp; e.photos != null) {    int userId = new Random().nextInt(10) + 1;    String postsAndCommentsOfRandomUser = ResponseUtil.getPostsAndCommentsOfRandomUser(userId, e.posts,    e.comments);    String albumsAndPhotosOfRandomUser = ResponseUtil.getAlbumsAndPhotosOfRandomUser(userId, e.albums,    e.photos);    String response = postsAndCommentsOfRandomUser + albumsAndPhotosOfRandomUser;    e.response = response;    e.countDownLatch.countDown();    }    }).build();}A special case — when shared memory between threads crosses ~8MB — is discussed further in this example.SummaryDecide the Executor framework’s configuration based on the load of the machine and also check if load balancing can be done based on the number of parallel tasks in the application. If optimum number of threads calculation for I/O tasks is done right, then more often than not, this approach will be the winner in Performance results.Using reactive or any asynchronous libraries decreases the performance for most of the traditional applications. This pattern is useful only when the use case is like a server-side chat application, where the thread need not retain the connection until the client responds.Performance of the Disruptor framework was good when used with event-driven architectural patterns; but when the Disruptor pattern was mixed with traditional architecture and for the use case we discussed here, it was not up to the mark. It is important to note here that the Akka and Disruptor libraries deserve a separate post on using them with event-driven architectural patterns.The source code for this post can be found on GitHub.A bird’s-eye view on Java concurrency frameworks was originally published in Technology at PayPal on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-10-05 16:17:29","link":"https://medium.com/paypal-tech/a-birds-eye-view-on-java-concurrency-frameworks-5072fd3a8759?source=rss----6423323524ba---4","blog":{"id":"paypal","link":"https://medium.com/paypal-engineering","name":"Paypal","rssFeed":"https://medium.com/feed/paypal-engineering","type":"company"},"blogType":"company"},{"id":"f67cb76c07cf6acabe83d42d4b8251d2","publish_timestamp":1615964409,"title":"Using Dekorate to generate Kubernetes manifests for Java applications","blogName":"Redhat","image":"https://developers.redhat.com/blog/wp-content/uploads/2021/03/2021_Dekorate_Java_Manifest_Featured_article_image_B.png","categories":["DevOps","Java","Kubernetes","Microservices","Dekorate","deployment","kubernetes manifest","openshift"],"description":"To deploy an application on Kubernetes or Red Hat OpenShift, you first need to create objects to allow the platform to install an application from a container image. Then, you need to launch the application using a pod and expose it as a service with a static IP address. Doing all of that can be tedious, [&#8230;]\nThe post Using Dekorate to generate Kubernetes manifests for Java applications appeared first on Red Hat Developer.\n","publish_date":"2021-03-17 07:00:09","link":"https://developers.redhat.com/blog/2021/03/17/using-dekorate-to-generate-kubernetes-manifests-for-java-applications/","blog":{"id":"redhat","link":"https://developers.redhat.com/blog/","name":"Redhat","rssFeed":"https://developers.redhat.com/blog/feed/","type":"company"},"blogType":"company"},{"id":"cd6505aec99f4ddc9e92fa447559c53c","publish_timestamp":1615878005,"title":"Three ways to containerize .NET applications on Red Hat OpenShift","blogName":"Redhat","image":"https://developers.redhat.com/blog/wp-content/uploads/2021/03/NET-on-openshift_1x.png","categories":["NET Core","Containers","Kubernetes","Linux","Windows","NET OpenShift","Linux containers","OpenShift CNV","Windows containers"],"description":"When Microsoft announced in November 2014 that the .NET Framework would be open source, the .NET developer&#8217;s world shifted. This was not a slight drift in a new direction; it was a tectonic movement with huge implications. For one thing, it positioned .NET developers to take part in the rapidly-growing world of Linux containers. As [&#8230;]\nThe post Three ways to containerize .NET applications on Red Hat OpenShift appeared first on Red Hat Developer.\n","publish_date":"2021-03-16 07:00:05","link":"https://developers.redhat.com/blog/2021/03/16/three-ways-to-containerize-net-applications-on-red-hat-openshift/","blog":{"id":"redhat","link":"https://developers.redhat.com/blog/","name":"Redhat","rssFeed":"https://developers.redhat.com/blog/feed/","type":"company"},"blogType":"company"},{"id":"7bf48774b45519df07c1834e68abd9f7","publish_timestamp":1615791652,"title":"Planning your containerization strategy on Red Hat OpenShift","blogName":"Redhat","image":"https://developers.redhat.com/blog/wp-content/uploads/2020/11/Screen-Shot-2020-11-11-at-9.45.05-AM.png","categories":["CICD","Containers","Kubernetes","containers","Migration","openshift","scaling"],"description":"There&#8217;s no question about the benefits of containers, including faster application delivery, resilience, and scalability. And with Red Hat OpenShift, there has never been a better time to take advantage of a cloud-native platform to containerize your applications. Transforming your application delivery cycle and shifting from traditional infrastructure to cloud-native can be daunting, however. As [&#8230;]\nThe post Planning your containerization strategy on Red Hat OpenShift appeared first on Red Hat Developer.\n","publish_date":"2021-03-15 07:00:52","link":"https://developers.redhat.com/blog/2021/03/15/planning-your-containerization-strategy-on-red-hat-openshift/","blog":{"id":"redhat","link":"https://developers.redhat.com/blog/","name":"Redhat","rssFeed":"https://developers.redhat.com/blog/feed/","type":"company"},"blogType":"company"},{"id":"15a6d4b3e09e2ae30ba5196c8652d878","publish_timestamp":1615536043,"title":"No more Java in vscode-xml 0.15.0!","blogName":"Redhat","image":"https://developers.redhat.com/blog/wp-content/uploads/2021/03/vscode-xml_1x.png","categories":["Java","Linux","Mac","VS Code","Windows","GraalVM","vscodexml","XML extension for VS Code"],"description":"Among other improvements and bug fixes in the vscode-xml extension 0.15.0 release, you can now run the extension without needing Java. We know the Java requirement discouraged many people from trying the extension. We have included a new setting, Prefer Binary (xml.server.preferBinary) that lets you choose between the Java server and the new binary server. [&#8230;]\nThe post No more Java in vscode-xml 0.15.0! appeared first on Red Hat Developer.\n","publish_date":"2021-03-12 08:00:43","link":"https://developers.redhat.com/blog/2021/03/12/no-more-java-in-vscode-xml-0-15-0/","blog":{"id":"redhat","link":"https://developers.redhat.com/blog/","name":"Redhat","rssFeed":"https://developers.redhat.com/blog/feed/","type":"company"},"blogType":"company"},{"id":"75f10b9cb91fef2b840b1968659c7f0d","publish_timestamp":1615363244,"title":"Write your own Red Hat Ansible Tower inventory plugin","blogName":"Redhat","image":"https://developers.redhat.com/blog/wp-content/uploads/2021/03/2020_Ansible_Inventory_Featured_Article__A-01.png","categories":["Automation","Python","Ansible","Ansible inventories","Ansible inventory plugin","AWX","Python 3"],"description":"Ansible is an engine and language for automating many different IT tasks, such as provisioning a physical device, creating a virtual machine, or configuring an application and its dependencies. Ansible organizes these tasks in playbook files, which run on one or more remote target hosts. Inventory files maintain lists of these hosts and are formatted [&#8230;]\nThe post Write your own Red Hat Ansible Tower inventory plugin appeared first on Red Hat Developer.\n","publish_date":"2021-03-10 08:00:44","link":"https://developers.redhat.com/blog/2021/03/10/write-your-own-red-hat-ansible-tower-inventory-plugin/","blog":{"id":"redhat","link":"https://developers.redhat.com/blog/","name":"Redhat","rssFeed":"https://developers.redhat.com/blog/feed/","type":"company"},"blogType":"company"},{"id":"cfe1ea71449270ce94a0f3973ecb3762","publish_timestamp":1615276850,"title":"An introduction to JavaScript SDK for CloudEvents","blogName":"Redhat","image":"https://developers.redhat.com/blog/wp-content/uploads/2021/03/CloudEventsJs_2x.png","categories":["EventDriven","JavaScript","Kubernetes","Nodejs","CloudEvents","serverless functions","Typescript"],"description":"In today&#8217;s world of serverless functions and microservices, events are everywhere. The problem is that they are described differently depending on the producer technology you use. Without a common standard, the burden is on developers to constantly relearn how to consume events. Not having a standard also makes it more difficult for authors of libraries [&#8230;]\nThe post An introduction to JavaScript SDK for CloudEvents appeared first on Red Hat Developer.\n","publish_date":"2021-03-09 08:00:50","link":"https://developers.redhat.com/blog/2021/03/09/an-introduction-to-javascript-sdk-for-cloudevents/","blog":{"id":"redhat","link":"https://developers.redhat.com/blog/","name":"Redhat","rssFeed":"https://developers.redhat.com/blog/feed/","type":"company"},"blogType":"company"},{"id":"c366886bc27c43139451c125add5d525","publish_timestamp":1615276839,"title":"Deploying Node.js applications to Kubernetes with Nodeshift and Minikube","blogName":"Redhat","image":"https://developers.redhat.com/blog/wp-content/uploads/2021/03/CP-nodejskubernetes_2x-e1615240548598.png","categories":["Developer Tools","JavaScript","Kubernetes","Nodejs","minikube","nodeshift","openshift","S2I"],"description":"In a previous article, I showed how easy it was to deploy a Node.js application during development to Red Hat OpenShift using the Nodeshift command-line interface (CLI). In this article, we will take a look at using Nodeshift to deploy Node.js applications to vanilla Kubernetes—specifically, with Minikube. Getting started If you want to follow along [&#8230;]\nThe post Deploying Node.js applications to Kubernetes with Nodeshift and Minikube appeared first on Red Hat Developer.\n","publish_date":"2021-03-09 08:00:39","link":"https://developers.redhat.com/blog/2021/03/09/deploying-node-js-applications-to-kubernetes-with-nodeshift-and-minikube/","blog":{"id":"redhat","link":"https://developers.redhat.com/blog/","name":"Redhat","rssFeed":"https://developers.redhat.com/blog/feed/","type":"company"},"blogType":"company"},{"id":"f305136ee208e449c531720afe21dbdf","publish_timestamp":1615276838,"title":"A guide to Red Hat OpenShift 4.5 installer-provisioned infrastructure on vSphere","blogName":"Redhat","image":"https://developers.redhat.com/blog/wp-content/uploads/2021/03/2021_VSphere_OS_installer_Featured_Article__A-1024x512.png","categories":["Kubernetes","Linux","Operator","container host provisioning","installer","openshift","RHEL","vSphere"],"description":"With Red Hat OpenShift 4, Red Hat completely re-architected how developers install, upgrade, and manage OpenShift to develop applications on Kubernetes. Under the hood, the installation process uses the OpenShift installer to automate container host provisioning using Red Hat Enterprise Linux (RHEL) CoreOS. It is then easy to initialize the cluster and set up the [&#8230;]\nThe post A guide to Red Hat OpenShift 4.5 installer-provisioned infrastructure on vSphere appeared first on Red Hat Developer.\n","publish_date":"2021-03-09 08:00:38","link":"https://developers.redhat.com/blog/2021/03/09/a-guide-to-red-hat-openshift-4-5-installer-provisioned-infrastructure-on-vsphere/","blog":{"id":"redhat","link":"https://developers.redhat.com/blog/","name":"Redhat","rssFeed":"https://developers.redhat.com/blog/feed/","type":"company"},"blogType":"company"},{"id":"ad4072924d621ac2c184fa99d0f540d6","publish_timestamp":1615190455,"title":"Red Hat Summit Virtual Experience 2021: Register today","blogName":"Redhat","image":"https://developers.redhat.com/blog/wp-content/uploads/2021/03/summit2021-register_2x-1024x512.png","categories":["Uncategorized","Burr Sutter","red hat summit","Summit 2021"],"description":"Automation, application deployment, and how to speed up your journey to the cloud. These and other developer hot topics will take center stage at Red Hat Summit 2021. Join thousands of your peers by registering for our all-new, free, two-part virtual Summit experience. Keynote speaker Burr Sutter will be delving deep into developer technologies as [&#8230;]\nThe post Red Hat Summit Virtual Experience 2021: Register today appeared first on Red Hat Developer.\n","publish_date":"2021-03-08 08:00:55","link":"https://developers.redhat.com/blog/2021/03/08/red-hat-summit-virtual-experience-2021-register-today/","blog":{"id":"redhat","link":"https://developers.redhat.com/blog/","name":"Redhat","rssFeed":"https://developers.redhat.com/blog/feed/","type":"company"},"blogType":"company"},{"id":"1638a0e7bd9f93b5a328dc58400fe655","publish_timestamp":1615190426,"title":"New developer quick starts and more in the Red Hat OpenShift 4.7 web console","blogName":"Redhat","image":"https://developers.redhat.com/blog/wp-content/uploads/2021/03/ocp4.7_1x.png","categories":["CICD","Kubernetes","Operator","Serverless","helm charts","openshift","serverless","Tekton"],"description":"We are continuing to evolve the developer experience in Red Hat OpenShift 4.7. This article highlights what&#8217;s new for developers in the OpenShift 4.7 web console. Keep reading to learn about exciting changes to the topology view, an improved developer catalog experience, new developer quick starts, user interface support for Red Hat OpenShift Pipelines and [&#8230;]\nThe post New developer quick starts and more in the Red Hat OpenShift 4.7 web console appeared first on Red Hat Developer.\n","publish_date":"2021-03-08 08:00:26","link":"https://developers.redhat.com/blog/2021/03/08/new-developer-quick-starts-and-more-in-the-red-hat-openshift-4-7-web-console/","blog":{"id":"redhat","link":"https://developers.redhat.com/blog/","name":"Redhat","rssFeed":"https://developers.redhat.com/blog/feed/","type":"company"},"blogType":"company"},{"id":"ef2129455d7d782e8eb6fc496ad682e1","publish_timestamp":1607583638,"title":"MessageBus – Automated Documentation of our Messages","blogName":"Runtastic","image":"https://d2z0k43lzfi12d.cloudfront.net/blog/vcdn322/wp-content/uploads/2020/12/thumbnail_tech-post-maf_1200x800.jpg","categories":["Tech  Runtastic"],"description":"In the latest adidas Runtastic blog post we explain how our backend team automated messages and topics documentation.","publish_date":"2020-12-10 07:00:38","link":"https://www.runtastic.com/blog/en/messages-documentation/","blog":{"id":"runtastic","link":"https://www.runtastic.com/blog/en/category/tech/","name":"Runtastic","rssFeed":"https://www.runtastic.com/blog/en/category/tech/feed/","type":"company"},"blogType":"company"},{"id":"e13e1299f9abdcbdce1fae1612b3843f","publish_timestamp":1600761714,"title":"How to Keep Your Automated Testing Tools Fast","blogName":"Runtastic","image":"https://d2z0k43lzfi12d.cloudfront.net/blog/vcdn322/wp-content/uploads/2020/09/thumbnail_backend_bad_vs_good_tests_1200x800.jpg","categories":["Tech  Runtastic"],"description":"Want to keep your specs fast? Know the dos and don’ts of automated testing tools and how to deal with slow test suites.","publish_date":"2020-09-22 08:01:54","link":"https://www.runtastic.com/blog/en/fast-specs-with-automated-testing-tools/","blog":{"id":"runtastic","link":"https://www.runtastic.com/blog/en/category/tech/","name":"Runtastic","rssFeed":"https://www.runtastic.com/blog/en/category/tech/feed/","type":"company"},"blogType":"company"},{"id":"b490201bce95d7971b62ac5078df04c1","publish_timestamp":1591084825,"title":"How Small Changes Led to Significant Scalability Improvements","blogName":"Runtastic","image":"https://d2z0k43lzfi12d.cloudfront.net/blog/vcdn322/wp-content/uploads/2020/06/thumbnail_backend_performance_improvements_1200x800.png","categories":["Tech  Runtastic"],"description":"We are always focused on improving the health of our services. Read about what we’re doing to speed up performance in backend.","publish_date":"2020-06-02 08:00:25","link":"https://www.runtastic.com/blog/en/performance-in-backend/","blog":{"id":"runtastic","link":"https://www.runtastic.com/blog/en/category/tech/","name":"Runtastic","rssFeed":"https://www.runtastic.com/blog/en/category/tech/feed/","type":"company"},"blogType":"company"},{"id":"bc295b417cfad9c285a373f09a5be19b","publish_timestamp":1582189210,"title":"How we improved our sprints when we stopped estimating stories","blogName":"Runtastic","image":"https://d2z0k43lzfi12d.cloudfront.net/blog/vcdn322/wp-content/uploads/2020/02/thumbnail_PAY_no_estimates_800px.jpg","categories":["Tech  Runtastic"],"description":"Our estimations did not support our sprint and release planning. The solution: No more estimates.","publish_date":"2020-02-20 09:00:10","link":"https://www.runtastic.com/blog/en/no-more-estimates/","blog":{"id":"runtastic","link":"https://www.runtastic.com/blog/en/category/tech/","name":"Runtastic","rssFeed":"https://www.runtastic.com/blog/en/category/tech/feed/","type":"company"},"blogType":"company"},{"id":"98bfbdc4914d5ff214214a73f3d27c28","publish_timestamp":1567065603,"title":"The Hidden Cost of Core Data Managed Relationships","blogName":"Runtastic","image":"https://d2z0k43lzfi12d.cloudfront.net/blog/vcdn322/wp-content/uploads/2019/08/01_thumbnail_core-data_thumbnail_1200x800.jpg","categories":["Tech  Runtastic"],"description":"Read all about how we maintain database performance across devices by measuring critical operations and optimizing them.","publish_date":"2019-08-29 08:00:03","link":"https://www.runtastic.com/blog/en/core-data-optimization/","blog":{"id":"runtastic","link":"https://www.runtastic.com/blog/en/category/tech/","name":"Runtastic","rssFeed":"https://www.runtastic.com/blog/en/category/tech/feed/","type":"company"},"blogType":"company"},{"id":"fde6b79489c62cc1293239b6e006d289","publish_timestamp":1565251206,"title":"How We Reduce Regression Testing to 48 Hours","blogName":"Runtastic","image":"https://d2z0k43lzfi12d.cloudfront.net/blog/vcdn322/wp-content/uploads/2019/08/01_thumbnail_regression-testing_1200x800.jpg","categories":["Tech  Runtastic"],"description":"With a new setup for regression testing, our apps we are ready to release updates within 48 hours.","publish_date":"2019-08-08 08:00:06","link":"https://www.runtastic.com/blog/en/regression-testing/","blog":{"id":"runtastic","link":"https://www.runtastic.com/blog/en/category/tech/","name":"Runtastic","rssFeed":"https://www.runtastic.com/blog/en/category/tech/feed/","type":"company"},"blogType":"company"},{"id":"4a64c47d5b449d6d20dbc09e6a329a6f","publish_timestamp":1556775003,"title":"How Runtastic Developers Chose the Proper Architecture for Our Android Apps","blogName":"Runtastic","image":"https://d2z0k43lzfi12d.cloudfront.net/blog/vcdn322/wp-content/uploads/2019/05/android-architecture-thumbnail_blog_1200x800.jpg","categories":["Tech  Runtastic"],"description":"There are a lot of different architectures and implementations available for Android apps. Read about how Runtastic developers chose the right Android architecture for our apps.","publish_date":"2019-05-02 05:30:03","link":"https://www.runtastic.com/blog/en/how-runtastic-developers-chose-the-proper-architecture-for-our-android-apps/","blog":{"id":"runtastic","link":"https://www.runtastic.com/blog/en/category/tech/","name":"Runtastic","rssFeed":"https://www.runtastic.com/blog/en/category/tech/feed/","type":"company"},"blogType":"company"},{"id":"abbd02a28c882e163d74ac1bbc77cd99","publish_timestamp":1555392649,"title":"Refactoring a Multiple State View Using the UIStackView","blogName":"Runtastic","image":"https://d2z0k43lzfi12d.cloudfront.net/blog/vcdn322/wp-content/uploads/2019/04/ios-refactoring-thumbnail_blog_1200x800-5.jpg","categories":["Tech  Runtastic"],"description":"An interesting way to improve your multistate UIView code using UIStackView, making your code simpler and easier to understand.","publish_date":"2019-04-16 05:30:49","link":"https://www.runtastic.com/blog/en/ui-stack-view/","blog":{"id":"runtastic","link":"https://www.runtastic.com/blog/en/category/tech/","name":"Runtastic","rssFeed":"https://www.runtastic.com/blog/en/category/tech/feed/","type":"company"},"blogType":"company"},{"id":"540c9c2897146ba6ea2749318b4b4569","publish_timestamp":1552977042,"title":"MessageBus – How We Improved Managing Our Dead Letter Queue","blogName":"Runtastic","image":"https://d2z0k43lzfi12d.cloudfront.net/blog/vcdn322/wp-content/uploads/2019/03/thumbnail_blog_1200x800-1.jpg","categories":["Tech  Runtastic"],"description":"How implementing another consumer and storing messages in a database improved our automated dead letter handling.","publish_date":"2019-03-19 06:30:42","link":"https://www.runtastic.com/blog/en/message-bus-automated-dead-letter-handling/","blog":{"id":"runtastic","link":"https://www.runtastic.com/blog/en/category/tech/","name":"Runtastic","rssFeed":"https://www.runtastic.com/blog/en/category/tech/feed/","type":"company"},"blogType":"company"},{"id":"93a49dba2e0e9ea7699510e7f451a238","publish_timestamp":1550039449,"title":"Opening Portals to the Database: Making Core Data Easy","blogName":"Runtastic","image":"https://d2z0k43lzfi12d.cloudfront.net/blog/vcdn322/wp-content/uploads/2019/01/thumbnail_blog_1200x800-3.jpg","categories":["Tech  Runtastic"],"description":"Architectural approach to abstract away database specifics in iOS apps.","publish_date":"2019-02-13 06:30:49","link":"https://www.runtastic.com/blog/en/database-layer-on-ios/","blog":{"id":"runtastic","link":"https://www.runtastic.com/blog/en/category/tech/","name":"Runtastic","rssFeed":"https://www.runtastic.com/blog/en/category/tech/feed/","type":"company"},"blogType":"company"},{"id":"92ecee16513e95fd177966258f8aaf61","publish_timestamp":1615910869,"title":"Uber’s Journey Toward Better Data Culture From First Principles","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2021/03/pasted-image-0-1-e1615910779906.png","categories":["Uber Data"],"description":"Data powers Uber\nUber has revolutionized how the world moves by powering billions of rides and deliveries connecting millions of riders, businesses, restaurants, drivers, and couriers. At the heart of this massive transportation platform is Big Data and Data Science &#8230;\nThe post Uber&#8217;s Journey Toward Better Data Culture From First Principles appeared first on Uber Engineering Blog.\n","publish_date":"2021-03-16 16:07:49","link":"https://eng.uber.com/ubers-journey-toward-better-data-culture-from-first-principles/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"e9b54677f33adbced4010a172ef7d642","publish_timestamp":1615568046,"title":"Navigating to the Technical Program Management and Learning Team","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2021/03/UberIM_004279.jpg","categories":["Team Profile"],"description":"Spread across 4 continents, the Technical Strategy, Program Management, and Learning team is composed of Technical Program Managers (TPMs), Technical Writers, Technical Strategists, and Technical Training Program Managers.\nUber TPMs play a critical role in executing high-impact, company-wide initiatives and &#8230;\nThe post Navigating to the Technical Program Management and Learning Team appeared first on Uber Engineering Blog.\n","publish_date":"2021-03-12 16:54:06","link":"https://eng.uber.com/tpm-and-learning-team/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"62ae5f9abbde4ddfb702de1a3f3b09a7","publish_timestamp":1615222819,"title":"Elastic Deep Learning with Horovod on Ray","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2021/03/horovod-ray.jpg","categories":["Machine Learning","Open Source"],"description":"Introduction\nIn 2017, we introduced Horovod, an open source framework for scaling deep learning training across hundreds of GPUs in parallel.  At the time, most of the deep learning use cases at Uber were related to the research and &#8230;\nThe post Elastic Deep Learning with Horovod on Ray appeared first on Uber Engineering Blog.\n","publish_date":"2021-03-08 17:00:19","link":"https://eng.uber.com/horovod-ray/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"7d95fc9e7efb4131ce989a5c777fb216","publish_timestamp":1614704309,"title":"Applying Machine Learning in Internal Audit with Sparsely Labeled Data","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2021/03/8671003870_b4b6a0b44f_o-e1614703956722.jpg","categories":["Artificial Intelligence  Machine Learning"],"description":"As machine learning continues to evolve, transforming the various industries it touches, it has only begun to inform the world of audit. As a data scientist and former CPA Auditor, I can understand why this is the case. By nature, &#8230;\nThe post Applying Machine Learning in Internal Audit with Sparsely Labeled Data appeared first on Uber Engineering Blog.\n","publish_date":"2021-03-02 16:58:29","link":"https://eng.uber.com/ml-internal-audit/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"e445c211c6a0cd346c65cdc1a9955e1d","publish_timestamp":1614357022,"title":"How Uber Deals with Large iOS App Size","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2021/02/6792412657_367fd41fc7_b-1-e1614663184388.jpg","categories":["General Engineering"],"description":"The App Size Problem\nUber’s iOS mobile Apps for Rider, Driver, and Eats are large in size. The choice of Swift as our primary programming language, our fast-paced development environment and feature additions, layered software and its dependencies, and statically &#8230;\nThe post How Uber Deals with Large iOS App Size appeared first on Uber Engineering Blog.\n","publish_date":"2021-02-26 16:30:22","link":"https://eng.uber.com/how-uber-deals-with-large-ios-app-size/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"1d10b0f6d0590116ef73ad9bcdaad001","publish_timestamp":1614101897,"title":"Evolving Schemaless into a Distributed SQL Database","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2021/02/Docstore-Features.png","categories":["General Engineering"],"description":"Introduction\nIn 2016 we published blog posts (I, II) about Schemaless &#8211; Uber Engineering’s Scalable Datastore. We went over the design of Schemaless as well as explained the reasoning behind developing it. In this post today we &#8230;\nThe post Evolving Schemaless into a Distributed SQL Database appeared first on Uber Engineering Blog.\n","publish_date":"2021-02-23 17:38:17","link":"https://eng.uber.com/schemaless-sql-database/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"7f9a73bed3fa69a819a57e0400880648","publish_timestamp":1613754022,"title":"Fast and Reliable Schema-Agnostic Log Analytics Platform","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2021/02/unnamed-1.png","categories":["General Engineering"],"description":"At Uber, we provide a centralized, reliable, and interactive logging platform that empowers engineers to work quickly and confidently at scale. The logs are tagged with a rich set of contextual key value pairs, with which engineers can slice and &#8230;\nThe post Fast and Reliable Schema-Agnostic Log Analytics Platform appeared first on Uber Engineering Blog.\n","publish_date":"2021-02-19 17:00:22","link":"https://eng.uber.com/logging/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"8650366259e7b16949dbc5b466c54061","publish_timestamp":1611075708,"title":"Uber’s Real-time Data Intelligence Platform At Scale: Improving Gairos Scalability/Reliability","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2021/01/Screen-Shot-2019-03-05-at-10.29.31-PM-1.png","categories":["General Engineering"],"description":"Background\nReal-time data (# of ride requests, # of drivers available, weather, game) enables operations teams to make informed decisions like surge pricing, maximum dispatch ETA calculating, and demand/supply forecasting about our services that improve user experiences on the &#8230;\nThe post Uber&#8217;s Real-time Data Intelligence Platform At Scale: Improving Gairos Scalability/Reliability appeared first on Uber Engineering Blog.\n","publish_date":"2021-01-19 17:01:48","link":"https://eng.uber.com/gairos-scalability/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"610ce5788586bcfa2bbf0cbd7ba29a63","publish_timestamp":1610471640,"title":"The Journey Towards Metric Standardization","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2021/01/0_0-1.png","categories":["General Engineering"],"description":"At Uber, business metrics are vital for discovering insights about how we perform, gauging the impact of new products, and optimizing the decision making process. The use cases for metrics can range from an operations member diagnosing a fares issue &#8230;\nThe post The Journey Towards Metric Standardization appeared first on Uber Engineering Blog.\n","publish_date":"2021-01-12 17:14:00","link":"https://eng.uber.com/umetric/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"cd5bf382c3bc706fe240710930495caa","publish_timestamp":1608570007,"title":"Disaster Recovery for Multi-Region Kafka at Uber","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/12/pasted-image-0-14.png","categories":["Uncategorized"],"description":"Apache Kafka at Uber\nUber has one of the largest deployments of Apache Kafka in the world, processing trillions of messages and multiple petabytes of data per day. As Figure 1 shows, today we position Apache Kafka as a cornerstone &#8230;\nThe post Disaster Recovery for Multi-Region Kafka at Uber appeared first on Uber Engineering Blog.\n","publish_date":"2020-12-21 17:00:07","link":"https://eng.uber.com/kafka/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"4be0f5ac09aa0c4b05242ae3d17f88ed","publish_timestamp":1608310877,"title":"Uber’s Real-Time Push Platform","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/12/unnamed-4.png","categories":["General Engineering","Uncategorized"],"description":"Uber builds multi-sided marketplaces handling millions of trips every day across the globe. We strive to build real-time experiences for all our users.\nThe nature of real time marketplaces make them very lively. Over the course of a trip, there &#8230;\nThe post Uber’s Real-Time Push Platform appeared first on Uber Engineering Blog.\n","publish_date":"2020-12-18 17:01:17","link":"https://eng.uber.com/real-time-push-platform/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"89cacf0be0150e84a1f81f112e8b2934","publish_timestamp":1607706036,"title":"No Code Workflow Orchestrator for Building Batch & Streaming Pipelines at Scale","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/12/image-e1607702779896.png","categories":["Uncategorized"],"description":"Data-In-Motion @ Uber\nAt Uber, several petabytes of data move across and within various platforms every day. We power this data movement by a strong backbone of data pipelines. Whether it’s ingesting the data from millions of Uber trips or &#8230;\nThe post No Code Workflow Orchestrator for Building Batch &#038; Streaming Pipelines at Scale appeared first on Uber Engineering Blog.\n","publish_date":"2020-12-11 17:00:36","link":"https://eng.uber.com/no-code-workflow-orchestrator/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"a290ce0e85837cb09d6bcec6706efd9d","publish_timestamp":1606237206,"title":"Horovod v0.21: Optimizing Network Utilization with Local Gradient Aggregation and Grouped Allreduce","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2018/12/Header-1.png","categories":["Machine Learning","Open Source"],"description":"We originally open-sourced Horovod in 2017, and since then it has grown to become the standard solution in industry for scaling deep learning training to hundreds of GPUs.  With Horovod, you can reduce training times from days or weeks to &#8230;\nThe post Horovod v0.21: Optimizing Network Utilization with Local Gradient Aggregation and Grouped Allreduce appeared first on Uber Engineering Blog.\n","publish_date":"2020-11-24 17:00:06","link":"https://eng.uber.com/horovod-v0-21/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"462813daedbfa700ec525c8f9895ea37","publish_timestamp":1605027604,"title":"Turning Metadata Into Insights with Databook","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/11/pasted-image-0-3.png","categories":["General Engineering","Uber Data","Uncategorized"],"description":"Every day in over 10,000 cities around the world, millions of people rely on Uber to travel, order food, and ship cargo. Our apps and services are available in over 69 countries and run 24 hours a day. At our &#8230;\nThe post Turning Metadata Into Insights with Databook appeared first on Uber Engineering Blog.\n","publish_date":"2020-11-10 17:00:04","link":"https://eng.uber.com/metadata-insights-databook/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"8d9c6d5a1c4b1caf9ced8676f4441d40","publish_timestamp":1603987204,"title":"Meet the 2020 Safety Engineering Interns: COVID Edition","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/10/interns.png","categories":["Culture","General Engineering","Team Profile","Uber Engineering Internship"],"description":"About the Safety team &#38; What we do\nUber is dedicated to keeping people safe on the road. The Safety and Insurance Engineering team is at the core of Uber’s business. We work to redefine what it takes to be &#8230;\nThe post Meet the 2020 Safety Engineering Interns: COVID Edition appeared first on Uber Engineering Blog.\n","publish_date":"2020-10-29 16:00:04","link":"https://eng.uber.com/safety-engineering-interns-2020/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"00d3af64c9939955f586eef1d810d3db","publish_timestamp":1603209676,"title":"Operating Apache Pinot @ Uber Scale","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/10/Screen-Shot-2020-10-20-at-8.59.53-AM.png","categories":["Open Source","Uber Data"],"description":"Introduction\nUber has a complex marketplace consisting of riders, drivers, eaters, restaurants and so on. Operating that marketplace at a global scale requires real-time intelligence and decision making. For instance, identifying delayed Uber Eats orders or abandoned carts helps to &#8230;\nThe post Operating Apache Pinot @ Uber Scale appeared first on Uber Engineering Blog.\n","publish_date":"2020-10-20 16:01:16","link":"https://eng.uber.com/operating-apache-pinot/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"45636dec9d29d22971e6469909ca390f","publish_timestamp":1602741646,"title":"Building from the Baltics: Meet the Uber Engineering Team in Vilnius, Lithuania","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/03/Header.jpg","categories":["Culture","Team Profile","Core Infrastructure","Developer Productivity","Developer Tools","Infrastructure","Lithuania","Production Engineering","site reliability engineering","SRE","Storage Platform","Uber Vilnius"],"description":"When the site launched in 2014, the team at Uber Engineering’s Vilnius, Lithuania office consisted of two people working out of a co-working space. Six years later, the team has grown to 46 people, representing Uber&#8217;s Production Engineering, Core Platform, &#8230;\nThe post Building from the Baltics: Meet the Uber Engineering Team in Vilnius, Lithuania appeared first on Uber Engineering Blog.\n","publish_date":"2020-10-15 06:00:46","link":"https://eng.uber.com/building-from-the-baltics-uber-vilnius/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"714d95d7b1a2f3d33154ec4653e88269","publish_timestamp":1602000197,"title":"Ludwig v0.3 Introduces Hyperparameter Optimization, Transformers and TensorFlow 2 support","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/10/unnamed-4-2.png","categories":["Artificial Intelligence  Machine Learning","Open Source"],"description":"In February 2019, Uber released Ludwig, an open source, code-free deep learning (DL) toolbox that gives non-programmers and advanced machine learning (ML) practitioners alike the power to develop models for a variety of DL tasks. With use cases spanning text &#8230;\nThe post Ludwig v0.3 Introduces Hyperparameter Optimization, Transformers and TensorFlow 2 support appeared first on Uber Engineering Blog.\n","publish_date":"2020-10-06 16:03:17","link":"https://eng.uber.com/ludwig-v0-3/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"55102e6b71ae274ada8cb84ce47b0880","publish_timestamp":1601654422,"title":"Revolutionizing Money Movements  at Scale with Strong Data Consistency","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/10/unnamed.png","categories":["Data","General Engineering"],"description":"Uber as a platform invites its users to leverage it, earn from it, and be delighted by it. Serving more than 18 million requests per day, in 10,000+ cities, has enabled people to move freely and to think broadly &#8230;\nThe post Revolutionizing Money Movements  at Scale with Strong Data Consistency appeared first on Uber Engineering Blog.\n","publish_date":"2020-10-02 16:00:22","link":"https://eng.uber.com/money-scale-strong-data/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"b84e8ed6b7137657d3f7c636f1d61078","publish_timestamp":1600964310,"title":"Spearheading Open Source: A Conversation with Jim Jagielski, Staff Technical Program Manager with the Uber Open Source Program Office","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/09/jimjag-hack.jpg","categories":["Open Source","Team Profile","opensource"],"description":"Jim Jagielski&#8217;s fascination with open source software began out of necessity. He was working at NASA Goddard in the 1980s, and the agency had just received fancy new Macintosh computers loaded with Apple&#8217;s new A/UX operating system. There was only &#8230;\nThe post Spearheading Open Source: A Conversation with Jim Jagielski, Staff Technical Program Manager with the Uber Open Source Program Office appeared first on Uber Engineering Blog.\n","publish_date":"2020-09-24 16:18:30","link":"https://eng.uber.com/spearheading-open-source-jim-jagielski/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"43336d822bf782775f4a1659f7c7df48","publish_timestamp":1597766433,"title":"Designing Edge Gateway, Uber’s API Lifecycle Management Platform","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/08/unnamed-3.png","categories":["Architecture","Developers","General Engineering"],"description":"The making of Edge Gateway, the highly-available and scalable self-serve gateway to configure, manage, and monitor APIs of every business domain at Uber.\nEvolution of Uber&#8217;s API gateway\nIn October 2014, Uber had started its journey of scale in what &#8230;\nThe post Designing Edge Gateway, Uber’s API Lifecycle Management Platform appeared first on Uber Engineering Blog.\n","publish_date":"2020-08-18 16:00:33","link":"https://eng.uber.com/gatewayuberapi/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"6f2127f24b12f09097137b7f617f2599","publish_timestamp":1595941280,"title":"Standing for Safety: Meet the Uber Sao Paulo Tech Team","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/07/pasted-image-0.png","categories":["General Engineering","Team Profile"],"description":"Located in the heart of Latin America’s largest city, the Uber Sao Paulo Tech Center was founded in late 2018 as a company-wide hub for Safety Tech. The team is composed of product managers, UX designers, engineers and data scientists. &#8230;\nThe post Standing for Safety: Meet the Uber Sao Paulo Tech Team appeared first on Uber Engineering Blog.\n","publish_date":"2020-07-28 13:01:20","link":"https://eng.uber.com/meet-sao-paulo-tech/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"1d011ab00ac897ae3c41f5d0f0061ffb","publish_timestamp":1595520084,"title":"Introducing Domain-Oriented Microservice Architecture","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/07/unnamed.png","categories":["Architecture","General Engineering"],"description":"Introduction\nRecently there has been substantial discussion around the downsides of service oriented architectures and microservice architectures in particular. While only a few years ago, many people readily adopted microservice architectures due to the numerous benefits they provide such as &#8230;\nThe post Introducing Domain-Oriented Microservice Architecture appeared first on Uber Engineering Blog.\n","publish_date":"2020-07-23 16:01:24","link":"https://eng.uber.com/microservice-architecture/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"21d9efc715d11526ea0112ef459a6c43","publish_timestamp":1594742412,"title":"Engineering Failover Handling in Uber’s Mobile Networking Infrastructure","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/07/first-.png","categories":["General Engineering","Mobile"],"description":"&#160;\nMillions of users use Uber’s applications everyday across the globe, accessing seamless transportation or meal delivery at the push of a button. To achieve this accessibility at scale, our mobile apps require low-latency and highly reliable network communication, regardless &#8230;\nThe post Engineering Failover Handling in Uber&#8217;s Mobile Networking Infrastructure appeared first on Uber Engineering Blog.\n","publish_date":"2020-07-14 16:00:12","link":"https://eng.uber.com/eng-failover-handling/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"2e703abc83cb322d474c673baa9f1f3c","publish_timestamp":1593533750,"title":"Fiber: Distributed Computing for AI Made  Simple","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/06/Screen-Shot-2020-06-29-at-10.47.05-PM.png","categories":["AI","Artificial Intelligence  Machine Learning","Open Source","artificial intelligence","Machine Learning"],"description":"Project Homepage: GitHub\nOver the past several years, increasing processing power of computing machines has led to an increase in machine learning advances. More and more, algorithms exploit parallelism and rely on distributed training to process an enormous amount of &#8230;\nThe post Fiber: Distributed Computing for AI Made  Simple appeared first on Uber Engineering Blog.\n","publish_date":"2020-06-30 16:15:50","link":"https://eng.uber.com/fiberdistributed/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"aa48911755ff646f58fae23a695670f1","publish_timestamp":1592928003,"title":"Editing Massive Geospatial Data Sets with nebula.gl","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/06/Headerr-copy.png","categories":["General Engineering","Open Source","Data Visualization","DataViz","deckgl","ETAs","FrontEnd Dev","Frontend","geospatial visualization","Nebulagl","OSS","Uber Engineering","Visgl","Web","Web Dev"],"description":"Geospatial data, or data tied to a specific, real-world location, is integral to Uber’s ability to better understand the cities we serve. From predicting the most accurate estimated times of arrival (ETAs) to determining the best driving routes, geospatial &#8230;\nThe post Editing Massive Geospatial Data Sets with nebula.gl appeared first on Uber Engineering Blog.\n","publish_date":"2020-06-23 16:00:03","link":"https://eng.uber.com/nebulagl/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"66e2afb27c61e82405e1f1077f28dc3b","publish_timestamp":1592323243,"title":"Profiles in Coding: Diana Yanakiev, Uber ATG, Pittsburgh","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/06/Header-ATG.jpg","categories":["Culture","AI","Electrical Engineering","Engineering Manager","Machine Learning","ML","Profiles in Coding","Sensors","Uber Advanced Technologies Group","Uber ATG","Uber Engineering","Women in Tech"],"description":"Self-driving cars have long been considered the future of transportation, but they’re becoming more present everyday. Uber ATG (Advanced Technologies Group) is at the forefront of this technology, helping bring safe, reliable self-driving vehicles to the streets. Of course, &#8230;\nThe post Profiles in Coding: Diana Yanakiev, Uber ATG, Pittsburgh appeared first on Uber Engineering Blog.\n","publish_date":"2020-06-16 16:00:43","link":"https://eng.uber.com/profiles-in-coding-diana-yanakiev/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"b555490fedd95cb7f032dfe20ea84640","publish_timestamp":1591718451,"title":"Building a Large-scale Transactional Data Lake at Uber Using Apache Hudi","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/06/image4-2.png","categories":["Announcement","Open Source","Apache Hudi","Big Data","Data Processing","HDFS","OSS","The Apache Software Foundation","Uber Open Source"],"description":"From ensuring accurate ETAs to predicting optimal traffic routes, providing safe, seamless transportation and delivery experiences on the Uber platform requires reliable, performant large-scale data storage and analysis. In 2016, Uber developed Apache Hudi, an incremental processing framework, to power &#8230;\nThe post Building a Large-scale Transactional Data Lake at Uber Using Apache Hudi appeared first on Uber Engineering Blog.\n","publish_date":"2020-06-09 16:00:51","link":"https://eng.uber.com/apache-hudi-graduation/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"3be156c37c0401c3e63df441d9e2dd8f","publish_timestamp":1591632130,"title":"Introducing Neuropod, Uber ATG’s Open Source Deep Learning Inference Engine","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/06/Header-image-copy.png","categories":["AI","Announcement","Open Source","artificial intelligence","Deep Learning","Deep Learning Inference","Ludwig","Machine Learning","Neuropod","OSS","Uber Advanced Technologies Group","Uber ATG","Uber Engineering","Uber Open Source"],"description":"At Uber Advanced Technologies Group (ATG), we leverage deep learning to provide safe and reliable self-driving technology. Using deep learning, we can build and train models to handle tasks such as processing sensor input, identifying objects, and predicting where &#8230;\nThe post Introducing Neuropod, Uber ATG’s Open Source Deep Learning Inference Engine appeared first on Uber Engineering Blog.\n","publish_date":"2020-06-08 16:02:10","link":"https://eng.uber.com/introducing-neuropod/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"e71352c47f58b723ae4a369584480b38","publish_timestamp":1591113621,"title":"Inside Uber ATG’s Data Mining Operation: Identifying Real Road Scenarios at Scale for Machine Learning","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/06/header-image.1400x630.png","categories":["AI","Uber Data","Data Mining","Labeling","Machine Learning","Motion Planning","Perception","Safety Engineering","SDV","SelfDriving Vehicles","Simulation","Uber Advanced Technologies Group","Uber ATG"],"description":"How did the pedestrian cross the road?\nContrary to popular belief, sometimes the answer isn&#8217;t as simple as &#8220;to get to the other side.&#8221; To bring safe, reliable self-driving vehicles (SDVs) to the streets at Uber Advanced Technologies Group (ATG)&#8230;\nThe post Inside Uber ATG’s Data Mining Operation: Identifying Real Road Scenarios at Scale for Machine Learning appeared first on Uber Engineering Blog.\n","publish_date":"2020-06-02 16:00:21","link":"https://eng.uber.com/uber-atg-data-mining/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"3bafe413bde3a326f87b86aca0bada10","publish_timestamp":1590764411,"title":"Meta-Graph: Few-Shot Link Prediction Using Meta-Learning","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/05/Feature.png","categories":["AI","artificial intelligence","Graph Learning","Link Prediction","Machine Learning","MetaGraph","ML","Uber AI","Uber AI Labs","Uber Engineering"],"description":"This article is based on the paper “Meta-Graph: Few Shot Link Prediction via Meta Learning” by Joey Bose, Ankit Jain, Piero Molino, and William L. Hamilton\nMany real-world data sets are structured as graphs, and as such, machine &#8230;\nThe post Meta-Graph: Few-Shot Link Prediction Using Meta-Learning appeared first on Uber Engineering Blog.\n","publish_date":"2020-05-29 15:00:11","link":"https://eng.uber.com/meta-graph/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"2024e4bf9f2fb086ff13b376e4651290","publish_timestamp":1590681632,"title":"Profiles in Coding: Christabelle Bosson, Uber Elevate, San Francisco","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/05/Feature-copy.png","categories":["Culture","Aerial Ridesharing","Aerospace","Christabelle Bosson","NASA","Profiles in Coding","STEM","Uber Air","Uber Copter","Uber Elevate","WiT","Women in Tech"],"description":"Uber Elevate takes Uber’s pioneering transportation technology to new heights with urban aerial ridesharing. In its mission to make aviation a part of everyday transportation, Uber Elevate is building an all-electric aerial mobility ecosystem that will bypass congestion on &#8230;\nThe post Profiles in Coding: Christabelle Bosson, Uber Elevate, San Francisco appeared first on Uber Engineering Blog.\n","publish_date":"2020-05-28 16:00:32","link":"https://eng.uber.com/profiles-in-coding-christabelle-bosson-uber-elevate/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"effe1caea2e1ebae52502346480fd995","publish_timestamp":1590163250,"title":"Developing the Next Generation of Coders with the Dev/Mission  Uber Coding Fellowship","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2019/12/DevMission-Header.jpg","categories":["Culture","Open Source","Code for San Francisco","DevMission","Golang","Uber Career Prep","Uber Gives Back"],"description":"Uber is dedicated to furthering global citizenship, whether by connecting people to more reliable transportation or giving back to cities through social impact work. \nSince 2017, Uber Engineering has partnered with Dev/Mission, a San Francisco-based nonprofit that trains &#8230;\nThe post Developing the Next Generation of Coders with the Dev/Mission &lt;&gt; Uber Coding Fellowship appeared first on Uber Engineering Blog.\n","publish_date":"2020-05-22 16:00:50","link":"https://eng.uber.com/uber-open-source-coding-fellowship/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"d7f7c9925ea8e7dd824bec3f546453f3","publish_timestamp":1589988619,"title":"Introducing Athenadriver: An Open Source Amazon Athena Database Driver for Go","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/05/Facebook-2.png","categories":["Announcement","Open Source","Athena","Athenadriver","AWS","Go","Golang","OSS","Uber ATG"],"description":"Data analytics play a critical part in Uber&#8217;s decision making, driving and shaping all aspects of the company, from improving our products to generating insights that inform our business. To ensure timely and accurate analytics, the aggregated, anonymous data that &#8230;\nThe post Introducing Athenadriver: An Open Source Amazon Athena Database Driver for Go appeared first on Uber Engineering Blog.\n","publish_date":"2020-05-20 15:30:19","link":"https://eng.uber.com/introducing-athenadriver/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"4377b2161ee172f080bf57023f215380","publish_timestamp":1589461227,"title":"Building Uber’s Go Monorepo with Bazel","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/05/Bazel_Go_Monorepo-feature-image.png","categories":["Open Source","Bazel","Go","Golang","monorepo","Uber Open Source"],"description":"In traditional industries such as automobile or aerospace, engineers first design the products and the manufacturing facilities produce the cars or aircrafts according to the design. In software development, a build system is similar to the manufacturing facilities that take &#8230;\nThe post Building Uber’s Go Monorepo with Bazel appeared first on Uber Engineering Blog.\n","publish_date":"2020-05-14 13:00:27","link":"https://eng.uber.com/go-monorepo-bazel/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"1cc96cc139b6cc4b2c8b142494d70c52","publish_timestamp":1589299215,"title":"Announcing a New Framework for Designing Optimal Experiments with Pyro","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/05/image13.png","categories":["AI","Open Source","artificial intelligence","Bayesian Inference","Bayesian Modeling","Design","OED","Optimal Experimental Design","Pyro","Sequential Experiment","Uber AI","Uber Engineering"],"description":"Experimentation is one of humanity’s principal tools for learning about our complex world. Advances in knowledge from medicine to psychology require a rigorous, iterative process in which we formulate hypotheses and test them by collecting and analyzing new evidence. At &#8230;\nThe post Announcing a New Framework for Designing Optimal Experiments with Pyro appeared first on Uber Engineering Blog.\n","publish_date":"2020-05-12 16:00:15","link":"https://eng.uber.com/oed-pyro-release/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"ac13b9bf4423ad0f1fe9e15db81dd66c","publish_timestamp":1588867229,"title":"Monitoring Data Quality at Scale with Statistical Modeling","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/05/Monitoring_Data_Quality_feature-image.png","categories":["Uber Data","Big Data","Data Architecture","Data Quality","data science","Statistics"],"description":"Good business decisions cannot be made with bad data. At Uber, we use aggregated and anonymized data to guide decision-making, ranging from financial planning to letting drivers know the best location for ride requests at a given time. \nBut how &#8230;\nThe post Monitoring Data Quality at Scale with Statistical Modeling appeared first on Uber Engineering Blog.\n","publish_date":"2020-05-07 16:00:29","link":"https://eng.uber.com/monitoring-data-quality-at-scale/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"78d19d9dceed924e22f8e797d264928e","publish_timestamp":1588780841,"title":"Enhanced POET: Open-Ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/05/Header-Image.png","categories":["AI","AIGenerating Algorithms","ANNECS","artificial intelligence","CPPNs","Deep Learning","Enhanced POET","Openended Deep Learning","OpenEndedness","Original POET","PairedOpenended Trailblazer","Reinforcement Learning","Uber AI"],"description":"Jeff Clune and Kenneth Stanley were co-senior authors on this work and our associated research paper.\n\nMachine learning (ML) powers many technologies and services that underpin Uber&#8217;s platforms, and we invest in advancing fundamental ML research and engaging with &#8230;\nThe post Enhanced POET: Open-Ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions appeared first on Uber Engineering Blog.\n","publish_date":"2020-05-06 16:00:41","link":"https://eng.uber.com/enhanced-poet-machine-learning/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"1e51b343b6101c63afa45c7d1afa3c27","publish_timestamp":1584459025,"title":"Introducing Piranha: An Open Source Tool to Automatically Delete Stale Code","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/03/Header-Piranha.jpg","categories":["General Engineering","Open Source","Codebase","Developer Productivity","Feature Flags","OSS","Piranha","Programming Systems","Stale Code","Technical Debt","Uber Open Source"],"description":"At Uber, we use feature flags to customize our mobile app execution, serving different features to different sets of users. These flags allow us to, for example, localize the user’s experience in different regions where we operate and, more importantly, &#8230;\nThe post Introducing Piranha: An Open Source Tool to Automatically Delete Stale Code appeared first on Uber Engineering Blog.\n","publish_date":"2020-03-17 15:30:25","link":"https://eng.uber.com/piranha/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"0003c761ba58a7579fe7127936ae9c50","publish_timestamp":1584028833,"title":"Fostering a Culture of Sponsorship: Introducing Uber’s Engineering and Sponsorship Development Program","blogName":"Uber","image":"https://eng.uber.com/wp-content/uploads/2020/03/Header-image.jpg","categories":["Culture","General Engineering","CTO","DI","Diversity and Inclusion","Engineering Sponsorship and Development Program","ESDP","Mentorship","Office of the CTO","Sarah Bowman","Thuan Pham"],"description":"Being a strong engineer requires more than just top-notch coding. The ability to navigate interpersonal relationships with colleagues and leverage leadership skills is fundamental to your success, regardless of whether you’re an individual contributor or a manager. \nMentorship and sponsorship &#8230;\nThe post Fostering a Culture of Sponsorship: Introducing Uber’s Engineering and Sponsorship Development Program appeared first on Uber Engineering Blog.\n","publish_date":"2020-03-12 16:00:33","link":"https://eng.uber.com/engineering-sponsorship-development-program/","blog":{"id":"uber","link":"https://eng.uber.com","name":"Uber","rssFeed":"https://eng.uber.com/feed/","type":"company"},"blogType":"company"},{"id":"d136de95934be34bae833d284abfadc6","publish_timestamp":1593557960,"title":"The Corner Round-up June 2020","blogName":"Square","image":"https://miro.medium.com/max/1200/0*8tPJ-hZqW1ygCpzx","categories":[],"description":"In order to keep some of the followers still here on Medium, we wanted to provide a quick recap on some of the things that have been published over at The Corner Blog’s new home https://developer.squareup.com/blog (in case you didn’t know that already). We’ll just be doing a quick summary of each new thing, but you can check out each topic in detail over at our new home.Manage Team Data from Any Platform with Square Team APIWe released a new Team API to allow managing Team data from any platform. You can track and edit large volumes of team member data automatically and synchronize data with any third-party platform in real time!Introducing HephaestusIn order to make dependency injection with Dagger 2 easier, we had created Hephaestus. It is an open source compiler plugin that automatically merges Dagger modules and component interfaces to make development more enjoyable.API Explorer Moves to General AvailabilityThe Square Developer team also transitioned the API Explorer from Beta to GA, bringing with it a lot of improvements to make testing Square APIs much more enjoyable and easy.Kubernetes - Pod Security PoliciesThe cloud team at Square also did a write-up that provides some useful guidance and examples for handling Pod Security in Kubernetes.Reward Customers Wherever They Shop with Loyalty API and Customers APIAdditionally, Square Loyalty now has an API to make providing rewards to customers on other platforms. Using the Customers API and Loyalty API in combination can allow any developer to create a great rewards system, no matter where their customers are.Announcing Square Terminal API BetaFinally, we also released Terminal API, which lets you programmatically take payments using Square Terminal. This API allows developers to connect Square Terminal, an all-in-one card payments device, to their custom-built POS systems, no matter which platforms or operating systems they’re developed on.Now that isn’t everything that we’ve published since you’ve last heard from us, but should give some good highlights of what has been going on at Square. If you want to learn more or stay up-to-date on everything we’re working on, make sure to check out the blog at https://developer.squareup.com/blog or follow up on twitter at https://twitter.com/squaredev!The Corner Round-up June 2020 was originally published in Square Corner Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2020-06-30 22:59:20","link":"https://medium.com/square-corner-blog/the-corner-round-up-june-2020-5f0eaa1d341e?source=rss----3650599ae4e2---4","blog":{"id":"square","link":"https://corner.squareup.com/","name":"Square","rssFeed":"https://medium.com/feed/square-corner-blog","type":"company"},"blogType":"company"},{"id":"a9a6697b07e0839a286f76774daaf0cd","publish_timestamp":1555632381,"title":"We moved to https://developer.squareup.com/blog","blogName":"Square","image":"https://miro.medium.com/max/414/1*M17oykWIK6luqF1rBCLqrw.png","categories":[],"description":"Heads up, we’ve moved! If you’d like to continue keeping up with the latest technical content from Square please visit us at our new home https://developer.squareup.com/blogWe moved to https://developer.squareup.com/blog was originally published in Square Corner Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2019-04-19 00:06:21","link":"https://medium.com/square-corner-blog/we-moved-to-https-developer-squareup-com-blog-1fd4211cd78f?source=rss----3650599ae4e2---4","blog":{"id":"square","link":"https://corner.squareup.com/","name":"Square","rssFeed":"https://medium.com/feed/square-corner-blog","type":"company"},"blogType":"company"},{"id":"1625ceb4fdf2cb239ebaedc6e809916c","publish_timestamp":1555103181,"title":"Introducing PySurvival","blogName":"Square","image":"https://miro.medium.com/max/1200/1*7q5UV-_a0hLNwvdgelWMTg.jpeg","categories":["python","survivalanalysis","datascience","deeplearning","machinelearning"],"description":"Heads up, we’ve moved! If you’d like to continue keeping up with the latest technical content from Square please visit us at our new home https://developer.squareup.com/blogPySurvival is an open source python package for Survival Analysis modeling.Today, we’re excited to introduce PySurvival, a python package for Survival Analysis modeling.This article is the first installment in a four part series, which will include tutorials designed to demonstrate how to easily make the most of the package. You can also find these tutorials on the official website:Churn modelingPredictive maintenanceCredit riskPySurvival LogoWhat is PySurvival ?PySurvival is an open source python package for Survival Analysis modeling — the modeling concept used to analyze or predict when an event is likely to happen. It is built on top the most commonly used machine learning packages: NumPy, SciPy, and PyTorch.PySurvival provides a very easy way to navigate between theoretical knowledge on Survival Analysis and detailed tutorials on how to conduct a full analysis, as well as build and use a model. The package contains:10+ models ranging from the Cox Proportional Hazard model and the Neural Multi-Task Logistic Regression, to Random Survival ForestSummaries of the theory behind each model as well as API descriptions and examplesDetailed tutorials on how to perform exploratory data analysis, survival modeling, cross-validation and prediction, for churn modeling and credit risk, for examplePerformance metrics to assess the models’ abilities like c-index or brier scoreSimple ways to load and save modelsInstallationIf you have already installed a working version of gcc, the easiest way to install Pysurvival is using pip.pip install pysurvivalThe complete installation steps can be found here.Introduction to Survival analysisWhat is Survival Analysis ?Survival analysis is used to analyze or predict when an event is likely to happen. It originated in medical research, but its use has greatly expanded to many different fields. For instance:Banks, lenders and other financial institutions use it to predict the speed of repayment of loansBusinesses use it to predict when a client will churnCompanies use it to predict when employees will decide to leaveEngineers/manufacturers use it to predict when a machine will breakCensoring: why regression models cannot be used?The real strength of Survival Analysis is its capacity to handle situations when the event has not happened yet. To illustrate this, let’s take the example of two customers of a company and follow their active/churn status between January 2018 and April 2018:Figure 1 — Example of censoringcustomer A started doing business prior to the time window, and as of April 2018, is still a client of the companycustomer B also started doing business before January 2018, but churned in March 2018Here, we have an explicit depiction of the event for customer B. However, we have no information about customer A, except that he/she hasn’t churned yet at the end of the January 2018 to April 2018 time window. This situation is called censoring.One might be tempted to use a regression model to predict when events are likely to happen. But to do that, one would need to disregard censored samples, which would result in a loss of important information. Fortunately, Survival models are able to take censoring into account and incorporate the uncertainty, so that instead of predicting the time of an event, we predict the probability that an event happens at a particular time.PySurvival LogoIntroducing PySurvival was originally published in Square Corner Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2019-04-12 21:06:21","link":"https://medium.com/square-corner-blog/introducing-pysurvival-5a274b072381?source=rss----3650599ae4e2---4","blog":{"id":"square","link":"https://corner.squareup.com/","name":"Square","rssFeed":"https://medium.com/feed/square-corner-blog","type":"company"},"blogType":"company"},{"id":"876ddb0b9114aadb7d25c2a662c81b73","publish_timestamp":1555003490,"title":"Zooming Out From Engineering","blogName":"Square","image":"https://miro.medium.com/max/1200/1*wqObLwqQBMiYeMjp6sc0iQ.jpeg","categories":["engineeringmangement","leadership","management","engineering"],"description":"As originally seen on Greylock Partners’ blog, here are some insights on leadership, career development, and building teams at scale that Square’s Head of Seller, Alyssa Henry, shared during the 2019 SFELC Summit.Square’s Head of Seller &amp; Developer Business Units &amp; Infrastructure Engineering, Alyssa HenryHeads up, we’ve moved! If you’d like to continue keeping up with the latest technical content from Square please visit us at our new home https://developer.squareup.com/blogLeading an organization of 10 employees is considerably different than leading an organization of 10,000+ employees. As teams increase and communication becomes more challenging, a lot can break down. In this episode of Greymatter, we share a fireside discussion from the 2019 SFELC Summit on building and managing scaling teams. NodeSource VP of Engineering, Chanda Dharap talks with Square’s Head of Seller &amp; Developer Business Units &amp; Infrastructure Engineering, Alyssa Henry. Alyssa shares lessons from scaling companies at hyper-scale including Square and AWS, how to bring purpose to your employee’s work, and advice for building company culture.Alyssa leads Square’s cross functional teams with P&amp;L responsibility and is working to shift Square from an app focused on payment processing to a broad financial services platform and commerce ecosystem. Prior, she led AWS Storage Services including Amazon Simple Storage Service and Amazon Glacier where she was responsible for software development and operations. Earlier in her career, Alyssa held several roles in program and product management at Microsoft.SFELC is a curated community of 2500+ engineering leaders evolving the way leadership is implemented in the tech industry. Greylock Partners is the VC partner for SFELC, and sponsored the annual SFELC Summit which hosted top industry leaders to share management best practices and leadership advice.Below is an edited transcript of key points from Dan and Aditya’s fireside discussion.https://medium.com/media/4077660d3bfde3df278a4570b1c1bc25/hrefBring Purpose To Your Team’s Work“People want to work on something that matters, something that allows them to grow and learn. As a leader and as a manager, you should constantly determine how to ensure the people on your team have sufficient autonomy, mastery, and purpose in their work. You want to make sure that they’re engaged and love what they’re doing and sometimes that involves pushing your employees to grow further. It’s understanding how to align the required work with the employee’s interests to bring greater purpose to their work.”- Alyssa HenryBuild Company Culture Through Collaboration“Ultimately the best culture comes from hiring managers and employees that want to be involved with all aspects of building your company’s culture. How do you enable people within the team to help you build not only the product and the code, but also help build the team from a culture perspective? A hiring bar needs to be established and adhered to across the company.At Square, we brought employees together from across different teams within the company to define what culture should look like. From there, we internally established what the engineering career development and ladders looks like. I believe that as much as you can get a broad range of people with a broad range of perspectives to collaborate together, you can build something great where you’re building both the company and the team, not just building the code.”- Alyssa HenryLeaders Are Constantly Learning“When I first started my career, I worked with teams that had been doing the same thing for 20 years and they never understood that they were rapidly becoming obsolete. I learned early on that in the technology industry especially, bad things can happen if you don’t stay current. Constantly learning is critical. You can learn from everyone around you. I always encourage my team to seek out the people who have the skills in the areas they want to learn and meet regularly. As a manager, hire people that can help teach you and then provide them the space to do that and respect the skill sets that they bring.”- Alyssa HenryAt What Level of Scale Should Product And Engineering Be in the Same Organization?“At some level, they’re always in the same organization, because it’s the same company. It’s just a matter of determining at what point they meet within the hierarchy of a company, and I think it depends. If your company has one and only one product and everyone’s working on the same thing, most likely they’ll meet at the C-level underneath the CEO, CTO or a CPO.Because everyone’s working on one thing and the CEO is, in effect, the general manager for the product overall. In a business like Square or AWS where there’s a large amount of products and individual businesses product and engineering were organized with more of a general management structure where you push that down the organization so you get product and engineering coming together lowered down the organization which just helps you move faster and reduces the organizational distance between those two functions for the product.”- Alyssa HenryLessons From Hyper-scale“Growing is always fun, but challenging. The way to grow an organization is similar to the way to grow a code base. Whether it’s a monolith organization or monolith code base, you constantly figure out how to do cell divide, how to decompose or break apart so that you can get stronger boundaries and the ability to iterate in a way that you can make changes that don’t ripple through everything else so that you can reason about them independently” - Alyssa HenryZooming Out From Engineering was originally published in Square Corner Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2019-04-11 17:24:50","link":"https://medium.com/square-corner-blog/zooming-out-from-engineering-dc75c1509fa2?source=rss----3650599ae4e2---4","blog":{"id":"square","link":"https://corner.squareup.com/","name":"Square","rssFeed":"https://medium.com/feed/square-corner-blog","type":"company"},"blogType":"company"},{"id":"30f70e96bd54988f8125d1470e5662ff","publish_timestamp":1554833818,"title":"Spin Cycle — Automating the Tedious","blogName":"Square","image":"https://miro.medium.com/max/1200/1*cW3xdUGgEp8m4rAT4e8sWg.png","categories":["database","automation","opensource","engineering"],"description":"Spin Cycle — Automating the TediousAn orchestration framework to automate anythingHeads up, we’ve moved! If you’d like to continue keeping up with the latest technical content from Square please visit us at our new home https://developer.squareup.com/blogSpin Cycle makes it easy to automate complex infrastructure tasks. The database team at Square uses it for most of our day-to-day operations — provisioning new clusters and decommissioning old ones, upgrading MySQL and Docker, stopping and starting database hosts, and more. We’ve been developing and using Spin Cycle for over two years now, and today we’re happy to announce its GA release.Scripts Don’t ScaleBefore Spin Cycle, databases at Square were managed by hand. This worked, but it wasn’t ideal. For example, we had a script to provision a new database cluster. This script had been used for a few years, and for the most part it served its purpose. The original engineer who wrote it designed it well enough — it was a fully fledged program with functions and classes and unit tests.The script became a problem when other engineers needed to update it. It was difficult to add new tests when not familiar with all the code, so inevitably a lot of untested changes accrued. When the script broke it was hard to fix and even harder to make sure your fix didn’t break any of the untested bits. The script was slow, running its steps serially instead of concurrently, because that was easier to develop and debug. Over time, an important business process became embedded in this tangled ball of code. Provisioning a new database wasn’t a series of orderly steps anymore.Even if our script had been perfect — tests for every new change, each step totally encapsulated — we would have had problems. To provision 10 databases, you had to run the script 10 times in 10 windows — hopefully an error didn’t get lost in the noise. You couldn’t see the arguments someone else passed in and what output they got back, let alone look at past runs. One person would run the script and it would fail, another would run it and it would succeed — but did they really pass in the same arguments, run the same version from the same environment? Results weren’t reproducible, because it was practically impossible to ensure all starting conditions were the same.Scripts don’t scale. They’re difficult to maintain and collaborate on. They’re a pain to run in bulk. They can’t be used for service-to-service automation, and they don’t expose functionality to engineers who don’t have access to the hosts they run on. We needed a new way to automate our database management tasks, and Spin Cycle was our solution.Spec and SpinIn Spin Cycle, those scripts are replaced by requests, each made up of a series of jobs. Each job is very small and does just one thing (think powering off a host, or starting a MySQL instance) but when executed in sequence they accomplish a large task.Jobs are provided by you, written in Go, implementing a small, well-defined interface — a job can do anything you can code it to do. Requests are spec’d out in YAML using a simple syntax. Once you’ve given Spin Cycle its jobs and requests, it’s ready to go.Not Just Another Job SchedulerSpin Cycle’s got a lot going for it:No Dark CornersEnd-to-end status capability is a first-class component of Spin Cycle. You can see the real-time status of every job in a running request. And, you can look back at a request at any time and tell precisely what happened when it was run, because Spin Cycle saves a log entry for each completed job.Reusable JobsWhen starting out with Spin Cycle, you’re going to be writing all of the jobs in your new requests from scratch. This can be a large up-front commitment, especially if you’ve already got a script lying around that does the same thing. However, when you write new requests in the future, you’ll be able to reuse all of the jobs you’ve already written — no copy-pasting required. This makes it faster to create more requests later, and easy to keep all of them up to date. That’s also why it’s important to make jobs distinct units of work — you can reuse a job that makes MySQL read-only, but it’s more difficult to reuse one that makes it read-only and disconnects all clients and shuts it down, in one fell swoop.Requests at RuntimeSpin Cycle parses a request into its actual sequence of jobs when you run it, not when you write it. It’s possible to vary the jobs in a request based on conditions at runtime, so you can write a request ahead of time without knowing all the details of how it will be used. For us, this means things like writing a single request that can upgrade any MySQL cluster, regardless of the number of nodes in that cluster.Fully API DrivenSpin Cycle is designed to work as one part of a larger automation system, so it plays nice with other services. Everything is done via the API—starting and stopping requests, checking progress, looking at logs. Even the CLI is really just a wrapper around an API client. That means it’s easy to kick off a new request or check the status of one that’s running from other code.Highly Available + ScalableSpin Cycle automatically moves in-progress requests to other instances when it shuts down. This means you can upgrade versions and deploy new jobs and requests without downtime, as well as add and remove Spin Cycle instances as needed to scale your request capacity.Get Started!If you’ve got something you’d like to automate, check out the Spin Cycle documentation for a more detailed overview of how it all works. We’ve provided ready-to-go Docker containers in the Github repo, so you can test-drive Spin Cycle in a dev environment with a single command — go here for instructions.Spin Cycle — Automating the Tedious was originally published in Square Corner Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2019-04-09 18:16:58","link":"https://medium.com/square-corner-blog/spin-cycle-automating-the-tedious-3f569106fd3c?source=rss----3650599ae4e2---4","blog":{"id":"square","link":"https://corner.squareup.com/","name":"Square","rssFeed":"https://medium.com/feed/square-corner-blog","type":"company"},"blogType":"company"},{"id":"842906f867acac5f2a6f44ece6b55d4c","publish_timestamp":1554413933,"title":"Announcing Our Connect v2 Labor API","blogName":"Square","image":"https://miro.medium.com/max/1200/1*eas7qSgtic4uFrKWrAs--w.jpeg","categories":["webdevelopment","scheduling","business","api","employment"],"description":"Capture employee working hours with breaks and hourly pay rateHeads up, we’ve moved! If you’d like to continue keeping up with the latest technical content from Square please visit us at our new home https://developer.squareup.com/blogWe’re very excited to announce the release of the Square Connect v2 Labor API. Having visibility into daily employee operations and labor costs is a critical part of managing a business. It helps business owners better understand the performance of their business, employees, and ultimately make more informed &amp; cost-effective staffing decisions. With the Labor API, partners can build stronger, more accurate, and more comprehensive labor management integrations on behalf of our shared customers.To highlight the changes and additional functionality recently implemented (i.e. break tracking, multiple wages, etc…), we updated the name from Timecard API to Labor API. The new name better aligns with the breadth of labor management functionalities our partners can power with the Labor API.You can more easily manage employee’s hours, breaks, shift wages, and import/export labor data.How to create a Shift (including break and wage information):https://medium.com/media/281fb8863abf67cb66c58e404143445e/hrefHow to query a set of closed shifts from a workweek:https://medium.com/media/b15dbea66bdaaf0f8f8a3a6a6cf48d29/hrefYou will find below a list of functionalities we implemented with this release:Employee shifts: View/create shifts worked by any employee, including the business location where the shift was worked, shift start/end times, and regular, overtime, and double time hours worked during the shift.Employee break tracking: Track employee breaks over the course of a worked shift, including break start/end times, durations, and whether they were paid/unpaid.Employee job tracking: View/edit the job (and wage) worked by an employee for any given shift, including the ability for one employee to have multiple jobs (and wages).Search: Search shifts by employee, business location, workday, start/end time, and status (open/closed shifts).If you want to keep up to date with the rest of our content, be sure to follow this blog &amp; our Twitter account, and sign up for our developer newsletter! We also have a Slack community for connecting with and talking to other developers implementing Square APIs.Announcing Our Connect v2 Labor API was originally published in Square Corner Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2019-04-04 21:38:53","link":"https://medium.com/square-corner-blog/announcing-our-connect-v2-labor-api-1e1f26c99f7f?source=rss----3650599ae4e2---4","blog":{"id":"square","link":"https://corner.squareup.com/","name":"Square","rssFeed":"https://medium.com/feed/square-corner-blog","type":"company"},"blogType":"company"},{"id":"9a114cd0975ca390986b0ba01e1db9e9","publish_timestamp":1553873811,"title":"The Road to an Envoy Service Mesh","blogName":"Square","image":"https://miro.medium.com/max/960/0*lZMibbQnakNatVsR","categories":["servicemesh","envoyproxy","engineering"],"description":"Heads up, we’ve moved! If you’d like to continue keeping up with the latest technical content from Square please visit us at our new home https://developer.squareup.com/blogAt Square we’ve been running a microservices architecture for years, primarily using three different languages: Java, Ruby and Go. Running such a diverse stack can make interoperability between the different languages/frameworks challenging, and in this post I’ll talk about how Square has handled this in the past and where we’re at today — actively migrating towards a full service mesh.The Old WorldSquare started off, like many other companies, just running one large monolith. For Square this was a big Ruby on Rails service, that handled everything that Square did. A monolith makes service to service communication completely unnecessary: everything runs in the same code base on a single database, so everything can be implemented without making any outbound network calls (except to third parties outside of Square).After a while, Square decided to move more towards a service oriented architecture to reduce the reliance on a single, large application. To do this Square decided to build their own Protobuf based RPC framework, inspired by Google’s Stubby. This was years before gRPC was created, so there weren’t any open source options at the time. This RPC framework, called Sake, was implemented for Java and Go as these were the languages used for higher availability services, while regular REST HTTP was used between services that didn’t understand Sake. (Incidentally, Sake became a part of gRPC’s DNA)Over time this framework became fairly sophisticated, with many features not generally available:Smart retries with automatic failoverPrioritized routing based on upstream healthTraffic shaping and service discoveryThis ended up working well, but relied on fat client/server libraries, so extending support to other languages was hard. Attempts were made to bring feature parity between languages in a variety of ways:Sidecar process that bridges HTTP to SakeCentralized L4 proxy coupled with smart client libraries for L7 featuresNeither of these fully solved the problem, as while both mitigated the feature gap, they weren’t an exact match. App owners were still forced to be aware of which mechanism was used when debugging issues, leading to a lot of confusion.A migration from Sake to gRPC was attempted, but the large feature set provided by Sake proved challenging to migrate to gRPC, requiring implementing custom load balancing and name resolvers to work with grpc-go and grpc-java. After partially implementing the necessary features, the migration was put on ice due to various reasons, in part because being a drop-in replacement for Sake meant little incentive for app owners to migrate.On top of this, we were leveraging internal hardware load balancers to load balance service to service calls. While we had partial support for client side load balancing in Sake, a lot of Sake traffic and all of HTTP traffic still relied on these load balancers. As the company kept growing, so did the amount of load on the hardware load balancers.Rethinking service to serviceAfter a series of issues with our hardware load balancers, we decided that we needed to revamp our service to service setup. Our existing service to service setup required creating new virtual IPs on the hardware load balancers for each new service, so the load was only going to increase as the number of services grew, so we had to either add more hardware load balancers or move to a more scaleable infrastructure.This proved to be a great opportunity to simplify: instead of having Sake and lots of other infrastructure running, we’d centralize on a single service to service mechanism. This has the added benefit of simplifying the things for app owners: no longer do they need to worry about multiple load balancers, connection pools, etc. They can focus on understanding the semantics of a single implementation, making everything from day to day operations to disaster recovery easier to reason about.Looking around, Envoy stood out as a great candidate, due to theSimilarity between the Envoy load balancer data structures and Sake’sStreaming configuration APIFirst class support for gRPCOut of process architectureIn retrospect, mapping the load balancer implementations required quite a bit of additional upstream work (smart retries, degraded health checks and better per try timeouts to name a few) before we could reconcile Sake and Envoy. Thankfully the Envoy maintainers were extremely helpful and receptive to feature additions, allowing us to build in everything we needed.The configuration API meant that we could easily build this on top of our existing Zookeeper based service discovery system: a centralized control plane would listen for Zookeeper changes and push those to Envoy using the configuration API.Support for gRPC meant that we could pick up the gRPC work that had been partially rolled out, but rely on Envoy for the load balancer logic. Even more important for us, this meant that we’d be able use the exact same load balancer for RPC and HTTP traffic. This is also why the out of process architecture is so great: the implementation is the same no matter what application is using it, because the same binary is used everywhere.Architecting the service meshTo move to Envoy we ended up building out a control plane based on https://github.com/envoyproxy/java-control-plane which integrated with our existing service discovery infrastructure. This let us consume the same data that was used for shaping Sake traffic, allowing app owners to not have to worry about where traffic was coming from.Envoy processes were deployed next to each app, and a very thin client library was provided for each language to communicate with this process over a unix socket. The library was intentionally kept thin to ensure that we kept as much of the logic centralized in the control plane or Envoy, as this ensures that the behavior is consistent between languages.We decided to use a unix socket due to our apps being deployed on multitenant machines, with access to TLS certs protected by file permissions. Using a unix socket allowed us to use similar permissioning to restrict access to a given app’s Envoy instance, which let us use plain HTTP without opening up access to secrets to all other apps on the same host. Further, this provided a performance optimization in not having to encrypt and decrypt traffic multiple times. The primary form of routing relies simply on the Host header, which indicates what service the request should be routed to.How Envoy selects a backend host based on the Host headerSince we kept the client very thin, it then became extremely easy for anyone to build an Envoy client. If nothing else, a simple curl command would give you all the same traffic shaping, retry behavior, etc. that a “full” client implementation would:curl --unix-socket egress.sock production.web.gns.squareThis has allowed developers to prototype new languages at Square in a way that integrated with our service mesh within minutes instead of months.MigrationOnce this was all set up, the migration work could start. Our primary focus has been to get rid of alternative service to service infrastructure, to deprecate and move apps away from other proxies. This is a very slow process, as it relies on coordinating config changes and app deploys with various teams. This is in large part due to the manual routing that our clients are doing to Envoy: Unlike systems like Istio which can assume control of the network and act as a transparent proxy, we have to make code changes to make clients route to Envoy. Unfortunately due to app multi tenancy and lack of network namespacing the approaches used by such transparent proxies were not an option for us.To help facilitate the migration, we released an internal CLI tool named tcli(short for traffic CLI), to simplify the operations necessary for an app to make the switch to Envoy.For an app wanting to move its client calls to go through Envoy, the steps would look something like this:To allocate ports, add Envoy to the deployment and request access to existing service to service dependencies, run a single command from a developer laptop:tcli add-envoy -a &lt;app&gt; -d &lt;datacenter&gt; -e &lt;environment&gt;Once access requests are approved, update client config in Git to use Envoy:userService:  url: user.global.square # CNAME to a hardware load balancer VIPwould need to be changed touserService:  envoy: { app: user }With these two simple changes, traffic would start to be routed through the Envoy sidecar. Making this as simple as possible for app owners has been key in facilitating the rollout: most app owners are usually not interested in dealing with the details of the migration, so abstracting this away and making it self-serve has been incredibly helpful.Current StatusThe rollout is still far from done, but in general it’s been very smooth, with most of the issues stemming from subtle differences in how Envoy works compared to legacy systems. We’re excited to keep working on rolling out Envoy for use at Square, hopefully fully deprecating Sake in favor of gRPC over Envoy in 2019.We’re also looking forward to exploring using Envoy for other parts of our traffic stack and to leverage the increasing presence of Envoy in our service mesh to improve security at Square or to make the job for app owners at Square easier.The Road to an Envoy Service Mesh was originally published in Square Corner Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2019-03-29 15:36:51","link":"https://medium.com/square-corner-blog/the-road-to-an-envoy-service-mesh-d1a51cbd31dd?source=rss----3650599ae4e2---4","blog":{"id":"square","link":"https://corner.squareup.com/","name":"Square","rssFeed":"https://medium.com/feed/square-corner-blog","type":"company"},"blogType":"company"},{"id":"651053560ae3d0d71a283e78bfd3dc46","publish_timestamp":1553813209,"title":"Mobile Web Performance @ Caviar","blogName":"Square","image":"https://miro.medium.com/max/1200/1*2UY_sG5ZY3xpGFzvuEEwEQ.png","categories":["javascript","caviar","engineering","webdevelopment","programming"],"description":"Heads up, we’ve moved! If you’d like to continue keeping up with the latest technical content from Square please visit us at our new home https://developer.squareup.com/blogAcross any e-commerce website, page load times are directly correlated with conversion rate. Faced with a page that takes longer than 3 seconds to load, 53% of users drop off, perhaps never to return. A 1 second delay in load time results in a 7% loss in conversions, 11% fewer page views, and 16% decrease in customer satisfaction. In terms of SEO, slow page speeds also impact indexing algorithms, which hurt crawl efficiency and leads to fewer pages being indexed.For consumers on mobile, slow speeds are even more painful. As more and more consumers move towards a mobile first experience, we need to optimize for speeds that don’t necessitate blazingly fast internet or even 4G speeds. Typically, improvements in this area means managing your requests and what you load better and more efficiently.This mobile first perspective is top of mind for Caviar as we expand to new markets this year. During our adoption of this methodology, we started noticing that trying out new features on mobile web seemed to take unusually long. We realized that our mobile web experience was far from ideal as the Caviar website was originally built for desktop. The shift in perspective from desktop to mobile was not a clean switch from a developer standpoint as we naturally have more resources for web and are less likely to test for mobile.We mapped out the core webpages on our user’s critical path and decided to focus on the starting point: our home page. This is the page most visitors of trycaviar.com start off on and remains one of our most trafficked pages. We used Google Lighthouse to audit our home page as a baseline and full disclosure — it was not pretty.Our initial Lighthouse audit.Setting Up MetricsAt Caviar, our stack includes React/Rails on our web interface bundled with Webpack, a build tool commonly used in conjunction with React. Webpack is a module bundler that builds a dependency graph starting from specified entry points and spits out a bundle for the browser to load. It’s primarily used for JavaScript but is capable of managing any type of front-end asset including HTML/CSS and even images, and is ultimately responsible for a lot of the JavaScript mangling we ended up doing. A neat tool we use in conjunction with this is Webpack Bundle Analyzer, which allows us to see what and where the dependencies we were pulling in and how much space it takes up relative to others. Later on, this helped us identify duplicate dependencies and dependencies to split into smaller chunks to be dynamically imported when we needed them rather than loading it all at once.This allowed us to quickly visualize what was going in to our bundles.We concluded early on that any quick wins would likely not result in noticeable improvement as our senses aren’t capable of detecting microsecond changes. Lucky for us, there are a variety of page performance tracking tools at our disposal. The first step in measuring our success throughout this body of work constituted building out the necessary infrastructure to track our progress and nail down our pain points.We wanted a simple, systematic way to run the Lighthouse CLI so that it accurately tracked our progress. So we built it into our CI pipeline. We found that Lighthouse scores varied depending on which computer we ran it on so this reduced the amount of variance since it would always be ran on our build machine. We started closely monitoring our JavaScript and installed the bundlesize package to better track the sizes of our application and vendor bundles. Based on this, we added a “budget” test to ensure our bundles were within a specified amount. These changes allowed us to track our metrics based on each commit and determine how new features or code changes affected our performance score with the same level of fidelity. This didn’t mean that it would block any deploys as we could simply increase the budget size by updating the test, but rather raise clarity around the performance impact and have us explicitly confirm that we were (or not) okay with it. For added visibility, we logged the times from browser performance object on each page load, which can be accessed via window.performance, so we could see our progress using data visualization tools.Now that we had the necessary infrastructure in place to track our progress, we could better visualize the performance implications of our code changes.Lighthouse suggestions based on our audit.DOM Node and Image OptimizationThrough Lighthouse recommendations and our analytics tools, we started to identify where and how we could make the most impact. Caviar is an image heavy site, so we deduced that optimizations to image loading could result in massive performance improvements by the power of scale. We found that in some cases, we were rendering image sizes upwards of 3000 x 2000 pixels into a small 260 x 100 pixels space! Here at Caviar, we value high quality images but suffice to say, this was not necessary and negatively impacted our performance with minimal gains in image quality.We ran our images through an imaging service called Thumbor and replaced all of them with a wrapper that loaded responsive images in a WebP format. This ensured that our users on mobile wouldn’t load larger images than necessary with the added benefit of guaranteeing consistency among our uploaded images, which is incredibly important as we roll out self-onboarding for our restaurant partners this year. This moved our First Contentful Paint metric from 9.45 seconds to 3.5 seconds, an improvement of nearly 6 seconds!Can you see the difference? Neither can we.With the momentum on image-loading in full effect, we continued to scrutinize our requests. We made small changes such as converting our png sprites to svgs so that they load inline instead of performing another network request. We made big UI changes such as rethinking our content on the page. Our home page used to load all restaurants upon landing. After limiting the number of DOM nodes we rendered on the initial page load and requiring the user to click on a CTA to load the rest, we improved our Time to Interactive speed by 4 seconds. Due to the amount of dynamic information on this page, it will almost always have a large number of DOM nodes, but we cut down almost 50% of the nodes on the initial load for one of our most visited pages.These were quick wins for us, but unfortunately, that was about the extent of what we could accomplish by just focusing on limiting network requests. Now it was time to tackle our JavaScript code execution times.Downsizing &amp; Chunking BundlesWe started off with enormous bundle sizes by any measure. After going down the list of the biggest packages we had using the visual provided by Webpack Bundle Analyzer as a guiding mechanism, we crossed off ones we absolutely could not remove such as React and unfortunately, jQuery. The remaining biggest offenders were moment and lodash, both huge and accordingly so, two of npm’s most depended upon packages. We found that we were actually loading both moment and moment-timezone. Moment-timezone covered all of our use cases so we were able to remove moment. Webpack by default includes all locale files for moment/moment-timezone. Since we weren’t using most of them, we removed all locale files with the IgnorePlugin and only loaded the ones we needed through a custom configuration file. In a similar string, lodash (CommonJS) was replaced with lodash-es, which is exported as ES modules and is tree-shakable.We were already bundle splitting our JavaScript code into application and vendor bundles using Webpack through the CommonsChunkPlugin. Briefly, this allows the browser to cache the vendor bundle so that if just the application changes, the client doesn’t need to load the vendor file again. Since the page is making a few more requests, there’s slightly more overhead for first time visitors. Client level caching mitigates this cost for recurring visitors, who should see a decrease in page load speed.For the remainder of the bundles we were serving up, we ran an audit using Chrome DevTool’s code coverage panel to find out just how much JavaScript was used on the page. Not surprisingly, a large percentage of it was being loaded and unused.Disclosure: This was after the improvements we made so just imagine it being much worse!At the time, we were still on Webpack 3 and code splitting was limited by the CommonsChunkPlugin. The plugin only allowed application chunking and sometimes resulted in more code being loaded than necessary due to its parent-child chunking methodology. Webpack 4 has since deprecated this plugin and is now using a more efficientoptimization.SplitChunks plugin, which can handle vendor splitting as well. Code splitting loads code using dynamic imports, so that users only download the code they need for the part of the site that they’re viewing.// Applicationimport(&#39;./Modal&#39;).then(({default: Modal }) =&gt; {  this.openComponent(Modal);})// Vendorimport(&#39;package&#39;).then(({ default: Package }) =&gt; {  Package.doStuff();})This syntax automatically tells Webpack to start code splitting at this point and create a separate chunk. Code splitting allowed us to trim down the base bundles we serve up on each page and dynamically load the rest when necessary.So many colors and chunks!Instead of directly mounting individual components, we were now dynamically loading the components using react-loadable. However, the improvements in performance were minimal when we were on HTTP/1.1 due to the overhead for the increased number of requests. We started seeing a drastic change after migrating to HTTP/2 thanks to its ability to multiplex parallel requests/responses. This allowed us to quickly reduce the initial JavaScript execution time on the main thread without a large refactor effort and resulted in a material improvement in our Lighthouse scores.A few days later, our budget test actually ended up catching a code change that pushed our application bundle over the limit! We were able to look through past commits and track down a large increase in our bundle size caused by a change to babel in our Webpack upgrade. By the end of this exercise, we cut down our base bundle sizes significantly:vendor.js: 436kb → 233kbapplication.js: 186kb → 46kbFinal ResultsTo recap:Resizing/compressing to WebP format and using responsive images reduced our First Contentful Paint by 6 seconds.Limiting our DOM nodes reduced our Time to Interactive by 4 seconds.Optimizing our package usage resulted in a decrease of about 145kb in our vendor bundle.Code splitting and dynamically importing packages and components on HTTP/2 reduced our base bundle sizes by over 50% cumulatively, which further reduced our TTI by 10 seconds and brought it down to about 13 seconds.In the green!What’s NextWith the right infrastructure in place and code improvements we’ve made this past quarter, we feel confident in our ability to continue to improve our mobile web performance. We’re taking this to heart as we tackle our next performance frontier: legacy code in the form of jQuery, fluxxor, and CSS. Until next time!Mobile Web Performance @ Caviar was originally published in Square Corner Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2019-03-28 22:46:49","link":"https://medium.com/square-corner-blog/mobile-web-performance-caviar-1a57da0cc233?source=rss----3650599ae4e2---4","blog":{"id":"square","link":"https://corner.squareup.com/","name":"Square","rssFeed":"https://medium.com/feed/square-corner-blog","type":"company"},"blogType":"company"},{"id":"63d301e1c07453e2218cad0f53025cdd","publish_timestamp":1553796770,"title":"Streamline Checkout with Shipping Options in Apple Pay","blogName":"Square","image":"https://miro.medium.com/max/1200/1*ZLUPpxvIZNw5Hsf5d3DJdg.png","categories":["ecommerce","webdevelopment","api","payments","applepay"],"description":"Use Apple Pay to Handle Selecting Shipping OptionsHeads up, we’ve moved! If you’d like to continue keeping up with the latest technical content from Square please visit us at our new home https://developer.squareup.com/blogA little over a year ago, we added support for Apple Pay on the Web in our Online Payments APIs to help developers easily integrate Apple Pay into their websites. However, we know eCommerce checkouts don’t stop with payments — you also need to collect additional details for fulfilling and shipping orders.Specifically, you need to collect a shipping address and offer different shipping options based on that address. You might provide a free shipping option and a paid shipping option, for those who need a faster delivery. Even though our Online Payments APIs supported collecting a shipping address from within Apple Pay, you had to handle shipping options outside the Apple Pay experience.With today’s update, Square’s API now supports the entire checkout experience for Apple Pay on the Web, including support for shipping options. You can take payments, ask for and validate the shipping address, and provide various shipping options, all within Apple Pay’s web experience. This speeds up checkout, which in turn increases conversions, with easy to use checkout flows via Apple Pay on web.How Apple Pay Shipping Options Work:Add shipping optionsYou can define any number of shipping options in the PaymentRequest object. The first shipping option will be the default value shown.https://medium.com/media/78471b15e4db3e80ebda3c7748597d30/hrefValidate a shipping addressWhen your customer chooses a shipping address, the shippingContactChanged callback will be invoked. You can use this callback to validate the shipping address, add new fees and/or taxes based on the shipping address, and update the shipping options.Errors shown when a shipping address isn’t supported by that merchantShipping options and taxes are updated when they choose a Canadian addresshttps://medium.com/media/60d5aaa66760c0320bd63f5e4911f05d/hrefOnce a shipping option has been selected, you will also get a chance to update the line items and the total through the shippingOptionChanged callback. Finally, once the payment has been authorized, you will get the final shipping address and the final shipping option in the cardNonceResponseReceived callback.https://medium.com/media/6768d7e86cab205e7b1cfa38660f4982/hrefWe are excited with this release and we look forward to seeing you implement this in your web applications. You can read more about our support for shipping options in Apple Pay on our documentation. To learn more about Square’s developer platform visit https://squareup.com/developers or join our community at squ.re/slack.Note: Square supports Apple Pay only for USD transactions.Additional Reading:Integrate with SqPaymentFormIntegrate with Checkout APIIf you want to keep up to date with the rest of our content, be sure to follow this blog &amp; our Twitter account, and sign up for our developer newsletter! We also have a Slack community for connecting with and talking to other developers implementing Square APIs.Streamline Checkout with Shipping Options in Apple Pay was originally published in Square Corner Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2019-03-28 18:12:50","link":"https://medium.com/square-corner-blog/streamline-checkout-with-shipping-options-in-apple-pay-7f3836245950?source=rss----3650599ae4e2---4","blog":{"id":"square","link":"https://corner.squareup.com/","name":"Square","rssFeed":"https://medium.com/feed/square-corner-blog","type":"company"},"blogType":"company"},{"id":"2876733143ba26b27a7507f7bad67960","publish_timestamp":1552667790,"title":"Square In-App Payments SDK for React Native","blogName":"Square","image":"https://miro.medium.com/max/1200/0*c8MmIMXKv_T3j5hE","categories":["ecommerce","api","mobiledevelopment","softwareengineering","reactnative"],"description":"A React Native Plugin for the Square In-App Payments SDKHeads up, we’ve moved! If you’d like to continue keeping up with the latest technical content from Square please visit us at our new home https://developer.squareup.com/blogSince the release of our In-App Payments SDK, we’ve been getting a lot of requests for when this would be available for React Native. It is officially here! You can simply npm install — save react-native-square-in-app-payments inside your React Native project and follow the setup guide over here to start accepting payments in your React Native app.If you’re not already familiar with the In-App Payments SDK, it enables developers to accept Square-powered payments from within their own mobile apps.Now, it would be too easy to say just install the SDK and move on, so we’ll dig into a React Native app that I built to show how this works.Our Order Ahead React Native App for buying Square Legos and demoed at ShopTalk.Getting Your Developer Environment SetupPrerequisites:Android Studio (follow link to download and install)Xcode (can be installed via App Store on macOS)Square Account (sign-up here)React Native CLI (follow guide for “Building Projects with Native Code”)To be clear, you only need either Android Studio or Xcode if you plan on having your app work on their respective platforms and want to use their simulators for development.Step 1: Get React Native CLI installed and set-upnpm install -g react-native-cliMake sure to follow the React Native setup guide for “Building Projects with Native Code”. Using the react-native-square-in-app-payments plugin requires the In-App Payments SDK, which is native code for iOS and Android. Also, part of following that guide has you install the React Native CLI (command seen above), which helps facilitate linking libraries and running the simulator while developing.Step 2: Add In-App Payments React Native Plugin to your ProjectAfter you’ve setup React Native, you’ll want to follow the Square guide for adding In-App Payments into your React Native project. If you’re starting from scratch, you might want to take a look at the quick-start example application that shows an example app for that allows a user to buy a cookie. You can also just download that example app and modify from there.Quick-start App for React Native In-App Payments Plugin.Things to Understand for React Native Development with In-App Payments SDKReact Native Interfaces for the In-App Payments SDKSQIPCore — Used to initialize the In-App Payments SDK in your React Native application.SQIPCardEntry — Handles the standard credit card form capture. It is worth noting that if you’re wanting to store a Card on File for your user, then you’d want to only use this interface since you cannot store card details using digital wallets.SQIPApplePay — Although fairly straightforward in the name, this interface is used for handling Apple Pay flow.SQIPGooglePay — Same thing as the Apply Pay interface, but for handling Google Pay.Each interface has some methods for initiating the flow, handling errors or the user closing the form, and completing authorization to get a nonce (a one-time use token). You are still required to have a backend implementation to use the nonce for either storing a card on a customer profile, or processing a transaction. You can find more on how this flow works in the Square documentation on.Routing / NavigationAlthough this can vary depending on which library you’re using, its worth explaining the one we use in our example. React Navigation is a commonly used library for routing and navigation in React Native apps.You can add it by running:npm install — save react-navigation react-native-gesture-handlerreact-native link react-native-gesture-handlerThe basic premise of the navigation library is to create a central hub at the root of your React Native app that can control which “screen” should be displayed at any given time. There are a few different types of navigation you can have with this library, but we’re just sticking with the stack navigator. It works exactly like a stack data structure that has each screen go “on” to the stack and when a user goes back it just pops them off the stack.An Order Ahead Example ApplicationIn order (so punny) to show what can be done with the React Native In-App Payments Plugin, we created an app to let people pick their own Square Lego person at conferences and also demonstrate how the new Orders Push Beta can push that into a Square Point of Sale (POS).At the root of our app, we use the createAppContainer and createStackNavigator from React Navigation for wrapping our React app and handling all of our routing and navigation. This is also where we will initialize the In-App Payments SDK using SQIPCore in the componentDidMount() lifecycle method.https://medium.com/media/083d1331551465468d1f2f9cb9d1b760/hrefWe kept this really simple by having only two screens. The main screen displays all of our products (in this case, lego people) and the other screen is our checkout.A lot of the code in the application is dedicated to styling the components, which could be its own blog post. The key part to take away from this is how to interact with the In-App Payments SDK.Next, we’ll dig into our Checkout screen and look inside our componentWillMount() method of our CheckoutScreen component. This is where we set our iOS card entry theme (you need to set these in a styles.xml in Android).https://medium.com/media/b57c9e34e0bba3ca6925c5da3c5907d7/hrefThen, we have to create a few lifecycle methods for handling events after starting the credit card form flow and handle getting our nonce for processing the card details.https://medium.com/media/2a3e0f03b9630fc7c32b3bd7f32e01cd/hrefTo break this down, at our base method for starting the card flow is the onStartCardEntry() method. We then have our onCardNonceRequestSuccess, onCardEntryCancel, and onCardEntryComplete for handling the different events in our flow.onCardNonceRequestSuccess — handles when we’ve successfully requested a nonce using the In-App Payments SDK, so we can send it to our backend for additional processing.onCardEntryCancel — should be used to handle if a user closes out the card entry form before filling it out and triggering a card nonce response.onCardEntryComplete — is used to close out the form, but can also be used for handling any state updates to your application.The React Native Order Ahead App in action.Now, as far as our front-end is concerned(in the case our React Native App), that is all we really need for processing a payment. The app should only be concerned with using the In-App Payments SDK for securely capturing those card details, getting the nonce, passing it to the backend for further processing, then react-ing (again, so punny) to the results of what was processed.Also, to be clear, this is only one way to implement the In-App Payments SDK Plugin in your React Native application. You could certainly also add in digital wallet support for Google Pay and/or Apple Pay, this was just focused on demonstrating the card flow.The rest of our capabilities for creating and pushing orders into a Square POS, charging a transaction (taking a payment), and/or storing customer card details will happen in your backend. You can read more about our Orders Push Beta and our Card on File transactions by following the links if you’re interested in building your own app for that, or join our Slack community and ask for help.If you’re plan on building something on Square using our React Native In-App Payments Plugin and want to write about it (or anything else Square related), please hop into our Slack community and let us know (you can join just to say hi too), we’re always happy to chat about whatever you’re working on.If you want to keep up to date with the rest of our content, be sure to follow this blog &amp; our Twitter account, and sign up for our developer newsletter! We also have a Slack community for connecting with and talking to other developers implementing Square APIs.Square In-App Payments SDK for React Native was originally published in Square Corner Blog on Medium, where people are continuing the conversation by highlighting and responding to this story.","publish_date":"2019-03-15 16:36:30","link":"https://medium.com/square-corner-blog/square-in-app-payments-sdk-for-react-native-499520539d9a?source=rss----3650599ae4e2---4","blog":{"id":"square","link":"https://corner.squareup.com/","name":"Square","rssFeed":"https://medium.com/feed/square-corner-blog","type":"company"},"blogType":"company"},{"id":"05f9f19ddb7fd823b65622f05c1e966b","publish_timestamp":1615449600,"title":"How Creating an Agile Code Base Helped eBay Pivot for Apple Silicon","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/210204-Apple-Silicon-blog-image-v1-01-hub-547x365-C-M2.jpg","categories":["article"],"description":"eBay’s native code base has grown and evolved for over 13 years, but is still nimble enough to quickly incorporate the latest features into the app.","publish_date":"2021-03-11 08:00:00","link":"https://tech.ebayinc.com/engineering/how-creating-an-agile-code-base-helped-ebay-pivot-for-apple-silicon/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"c5c49dfd84053ac39aef498f41389a0c","publish_timestamp":1614758400,"title":"How eBay’s Distributed Architecture Surfaces More Item Listings for Buyers","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/Helping-Sellers-Surface-Items4.jpg","categories":["article"],"description":"With Buyer Demand Data, eBay sellers can access information on the item specifics buyers are using in their search queries. This insight increases sellers’ completion of item specifics, which in turn further enhances listings’ quality, visibility and sales.","publish_date":"2021-03-03 08:00:00","link":"https://tech.ebayinc.com/engineering/how-ebays-distributed-architecture-surfaces-more-item-listings-for-buyers/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"0b5efade55dae004a1fe5420cbb02b26","publish_timestamp":1614153600,"title":"How eBay’s Buy APIs Hit $5 Billion in Gross Merchandise Bought","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/210125-Buy-API-infogfx-v1-03-social-5Billion-1.png","categories":["article"],"description":"eBay’s Buy APIs enable third-party developers to surface eBay inventory in their shopping experience, allowing consumers to purchase items without visiting an eBay site.","publish_date":"2021-02-24 08:00:00","link":"https://tech.ebayinc.com/engineering/how-ebays-buy-apis-hit-5-billion-in-gross-merchandise-bought/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"d8af620b9f978e2a89aa67e5fa653d21","publish_timestamp":1613030400,"title":"eBay Launches Marko 5","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/210202-ProductBlog-Marko-5-image-v1-02-inc-16x9-A2.jpg","categories":["article"],"description":"eBay’s open source JavaScript UI framework modernizes universal web development.","publish_date":"2021-02-11 08:00:00","link":"https://tech.ebayinc.com/engineering/ebay-launches-marko-5/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"8a6eeec1f5a2e70f0160f8a7145a7a74","publish_timestamp":1611561600,"title":"Mathesis: Elements of Learning and Intelligence","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/200817-e4C-eBay-logo-hub-image-16x9-v1-01-B2-B6-1.jpg","categories":["article"],"description":"A formal and interdisciplinary theory of learning and intelligence that combines biology, neuroscience, computer science, engineering and various branches of mathematics to provide a unifying framework, direction and a broader horizon for neural network and machine learning research.","publish_date":"2021-01-25 08:00:00","link":"https://tech.ebayinc.com/research/mathesis-elements-of-learning-and-intelligence/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"16585b457fb1aebed9c2ab353e0789e6","publish_timestamp":1610006400,"title":"Consumer Sellers: Getting Paid on eBay Just Got Easier","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/Paymens-January-7-Lead-Image2.jpg","categories":["article"],"description":"eBay is expanding its management of payments to consumer sellers over the next year. ","publish_date":"2021-01-07 08:00:00","link":"https://tech.ebayinc.com/product/consumer-sellers-getting-paid-on-ebay-just-got-easier/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"252480267502ebca3ec346a2d221888e","publish_timestamp":1609747200,"title":"Customized Ad-Rate Recommendations Now Available for Promoted Listings on eBay","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/201218-AdsRates-blog-hub-547x365-v1-05-T2.jpg-FINAL.jpg","categories":["article"],"description":"A new machine-learning tool helps sellers set competitive ad rates for their Promoted Listings, optimizing between cost and performance. ","publish_date":"2021-01-04 08:00:00","link":"https://tech.ebayinc.com/engineering/customized-ad-rate-recommendations-now-available-for-promoted-listings-on-ebay/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"cdc30e3b8de558b2df3d3a9daa5b0c0e","publish_timestamp":1606896000,"title":"eBay’s Offers to Buyers Feature Hits $1B GMV","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/image2.jpg","categories":["article"],"description":"Offers to Buyers reaches new milestone this month.","publish_date":"2020-12-02 08:00:00","link":"https://tech.ebayinc.com/product/ebays-offers-to-buyers-feature-hits-1b-gmv/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"a9e26ec8e989c673b15bada2d4b8cd44","publish_timestamp":1606723200,"title":"Introducing QR Codes for eBay Shipping Labels","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/image1.jpg","categories":["article"],"description":"With our new QR codes feature, sellers will no longer need a printer to use eBay shipping labels. ","publish_date":"2020-11-30 08:00:00","link":"https://tech.ebayinc.com/product/introducing-qr-codes-for-ebay-shipping-labelsnew-blog-post/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"0eb3591adcefa65913bc49c0e45d8c88","publish_timestamp":1606118400,"title":"Expanding the eBay Stores Experience to Our Mobile App","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/201119-stores-tech-blog-inc-16x9-v1-01-iOS-A.jpg","categories":["article"],"description":"The eBay Stores experience is now available on our mobile app for Android and iOS.","publish_date":"2020-11-23 08:00:00","link":"https://tech.ebayinc.com/product/expanding-the-ebay-stores-experience-to-our-mobile-app/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"a7d72f7293585ab6849865bc5bda064d","publish_timestamp":1605859200,"title":"An Introduction to Apache Flink","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/image5-copy2.jpg","categories":["article"],"description":"How we’re using Apache Flink to handle big data streaming in real-time.","publish_date":"2020-11-20 08:00:00","link":"https://tech.ebayinc.com/engineering/an-introduction-to-apache-flink/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"01633d49caccf02c67712d818cf2405b","publish_timestamp":1605600000,"title":"eBay’s Technology Transformation is Made for Evolving Customer Needs ","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/201111-EdgeComputing-lead-inc-16x9-v1-02-M-C.jpg","categories":["article"],"description":"How our advances in edge computing are empowering both buyers and sellers in times of great demand with ease and agility.","publish_date":"2020-11-17 08:00:00","link":"https://tech.ebayinc.com/engineering/ebays-technology-transformation-is-made-for-evolving-customer-needs/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"5047d0822f6290a4760ebf0e9aaaa7d6","publish_timestamp":1602831600,"title":"Introducing eBay’s New Time Away Feature to Help Sellers Manage Selling on eBay While on a Break","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/201014-TimeAway-lead-16x9-v1-01-M-M.jpg","categories":["article"],"description":"Time Away is now available to all Sellers and automatically updates the shipping date to make the experience more seamless. ","publish_date":"2020-10-16 07:00:00","link":"https://tech.ebayinc.com/product/introducing-ebays-new-time-away-feature-to-help-sellers-manage-selling-on-ebay-while-on-a-break/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"330d3f5bea71bd907489ea0376ff3342","publish_timestamp":1602226800,"title":"eBay Partners with Google Assistant to Bring Voice Control to eBay’s Android App ","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Video/Thumbs/_resampled/ScaleWidthWyIxMDI0Il0/466384721.jpg","categories":["article"],"description":"A new partnership launches voice control capabilities on our marketplace to meet the needs of our customers by creating a more seamless, modern experience. ","publish_date":"2020-10-09 07:00:00","link":"https://tech.ebayinc.com/product/ebay-partners-with-google-assistant-to-bring-voice-control-to-ebays-android-app/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"09c745bcb6949f17be52034a80b5d023","publish_timestamp":1600930800,"title":"An Automatic Mirror Testing Solution for Messaging Systems in eBay","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/Alead.jpg","categories":["article"],"description":"A look at how we developed a solution for automatic mirror testing to overcome the challenges of migrating eBay’s proprietary messaging applications to a more modern technical stack.","publish_date":"2020-09-24 07:00:00","link":"https://tech.ebayinc.com/engineering/an-automatic-mirror-testing-solution-for-messaging-systems-in-ebay/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"dc62a86288b02a172e3597083b6523f2","publish_timestamp":1600758000,"title":"Our Online Analytical Processing Journey with ClickHouse on Kubernetes","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/Kube2.jpg","categories":["article"],"description":"Learn about the latest evolution of online analytical processing (OLAP) data, now with ClickHouse on Kubernetes.","publish_date":"2020-09-22 07:00:00","link":"https://tech.ebayinc.com/engineering/ou-online-analytical-processing/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"15d1ccf0fb15d39568e582520602489b","publish_timestamp":1600239600,"title":"High Efficiency Tool Platform for Framework Migration","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/techlead2.jpg","categories":["article"],"description":"The eBay framework team has implemented a brand new tooling platform to speed up framework level migration. It supports highly efficient migration of applications running on legacy stacks to new platforms. This has been proven by the success stories migrating Batch, SOA, Trading/Shopping API, and messaging application migration.","publish_date":"2020-09-16 07:00:00","link":"https://tech.ebayinc.com/engineering/high-efficiency-tool-platform-for-framework-migration/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"32c8700b0397165d7b3ec05a2553255f","publish_timestamp":1600066800,"title":"Software Quality: Elevating the Game","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/leadstar.jpg","categories":["article"],"description":"Learn how software product quality goes well beyond simply avoiding bugs.","publish_date":"2020-09-14 07:00:00","link":"https://tech.ebayinc.com/engineering/software-quality-elevating-the-game/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"6adc4745bf402c86b30ac054c9b12a8d","publish_timestamp":1599807600,"title":"eBay Helps Buyers More Easily Discover Competitively Priced Items","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/GreatPrice.jpg","categories":["article"],"description":"eBay recently launched a new feature that helps buyers find things easily on the Search page by highlighting competitively priced items from trusted sellers.","publish_date":"2020-09-11 07:00:00","link":"https://tech.ebayinc.com/product/ebay-helps-buyers-more-easily-discover-competitively-priced-items/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"f58af6d72277cdd38169729daa8598e0","publish_timestamp":1599548400,"title":"eBay Motors: Accelerating With Flutter™","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/MotorsLead.jpg","categories":["article"],"description":"The UI software development kit enables a consistent user experience across iOS and Android.","publish_date":"2020-09-08 07:00:00","link":"https://tech.ebayinc.com/product/ebay-motors-accelerating-with-fluttertm/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"d6ef20a8f790114afff31db772bc9682","publish_timestamp":1599116400,"title":"eBay Virtual Tracking Number Now Live in the UK and Australia","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/eVTN-updated.jpg","categories":["article"],"description":"eBay’s unique code flows through Royal Mail and Australia Post networks, automating provision of eligible tracking and event data for buyers and sellers.","publish_date":"2020-09-03 07:00:00","link":"https://tech.ebayinc.com/product/ebay-virtual-tracking-number-now-live-in-the-u-k-and-australia/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"e3ab0229a81119e87ced82c5fec97285","publish_timestamp":1598338800,"title":"Building a Product Catalog: eBay's 2nd Annual University Machine Learning Competition","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/iStock-127825105.jpg","categories":["article"],"description":"Participating universities will structure listing data to help solve a real-world ecommerce challenge. ","publish_date":"2020-08-25 07:00:00","link":"https://tech.ebayinc.com/research/building-a-product-catalog-ebays-2nd-annual-university-machine-learning-competition/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"379b73dc963e8db96c6a7c2979189653","publish_timestamp":1597647600,"title":"An eBay Charity Perspective for Developers","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/200807-e4C-tech-lead-16x9-v1-02-D.jpg","categories":["article"],"description":"This article explores charity support in Public APIs and hypothetical integration scenario walkthroughs for developers who wish to integrate with eBay for Charity in their eBay-powered experiences.","publish_date":"2020-08-17 07:00:00","link":"https://tech.ebayinc.com/product/an-ebay-charity-perspective-for-developers/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"1da75c1a596210e233899b0be07a20d8","publish_timestamp":1597129200,"title":"The Journey to Integrating Android App Bundles","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/lead1.jpg","categories":["article"],"description":"Over the last few years, our Native Mobile Architecture team has been reshaping our native app experience to better align with evolving mobile standards.","publish_date":"2020-08-11 07:00:00","link":"https://tech.ebayinc.com/engineering/the-journey-to-integrating-android-app-bundles/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"ed8e09373186f4e9d784c15953e55ab8","publish_timestamp":1596697200,"title":"Terapeak Research 2.0 - Making the Data Processing Pipeline Robust","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/Terapeak-Research-2.0-Making-the-Data-Processing-Pipeline-Robust-3.jpg","categories":["article"],"description":"How the data processing pipeline functions in Terapeak and what factors make it fault tolerant, robust and highly available.","publish_date":"2020-08-06 07:00:00","link":"https://tech.ebayinc.com/engineering/terapeak-research-2-0-making-the-data-processing-pipeline-robust/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"9f934d22ecbfe867ec234d8c793fa47f","publish_timestamp":1595833200,"title":"Kubernetes Secrets: A Secure Credential Store for Jenkins","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/Kubernetes-Lead-3-copy.jpg","categories":["article"],"description":"At eBay, we containerized Jenkins to provide a continuous build infrastructure on Kubernetes Clusters to power the ecommerce marketplace experience. Our goal was to leverage the capability of Kubernetes secrets, for managing the Jenkins credentials. ","publish_date":"2020-07-27 07:00:00","link":"https://tech.ebayinc.com/research/kubernetes-secrets-a-secure-credential-store-for-jenkins/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"b9f375f68a9a1d0c832d17cd594a134b","publish_timestamp":1595314800,"title":"eBay’s Image Clean-Up Feature Brings the Power of Image Processing Algorithms to Android","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/Screen-Shot-2020-07-20-at-11.49.38-AM-copy.png","categories":["article"],"description":"This feature enables our sellers to create cleaner listings.","publish_date":"2020-07-21 07:00:00","link":"https://tech.ebayinc.com/product/ebays-image-clean-up-feature-brings-the-power-of-image-processing-algorithms-to-android/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"324d8f46294545391b5e8e4cea65ca77","publish_timestamp":1594882800,"title":"eBay Makes Promoted Listings in Search Results More Relevant and Dynamic","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/UR-lead-image.jpg","categories":["article"],"description":"The Promoted Listing algorithm continues to build revenue for the marketplace and makes sellers happy.","publish_date":"2020-07-16 07:00:00","link":"https://tech.ebayinc.com/product/ebay-makes-promoted-listings-in-search-results-more-relevant-and-dynamic/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"6a1eb2510dd52291d633bc846c1f1e48","publish_timestamp":1594623600,"title":"Celebrating 20 Years: eBay’s New APIs Enable Developers to Create Modern Buying and Selling Experiences ","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Video/Thumbs/_resampled/ScaleWidthWyIxMDI0Il0/mpFu25qPNXU.jpg","categories":["article"],"description":"eBay launches APIs for Managed Payments, Seller Initiated Offers, Charity and more for developers to help their businesses thrive.","publish_date":"2020-07-13 07:00:00","link":"https://tech.ebayinc.com/engineering/celebrating-20-years-ebays-new-apis-enable-developers-to-create-modern-buying-and-selling-experiences/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"},{"id":"5856d14c39301e8d7d80d1c830bad965","publish_timestamp":1594278000,"title":"How We Used Our Buy APIs, Catch Platform to Build a New Portal for the NHS","blogName":"Ebay","image":"https://static.ebayinc.com/static/assets/Uploads/Blog/Posts/_resampled/ScaleWidthWyIxMDI0Il0/Screen-Shot-2020-07-08-at-11.33.20-AM-copy.jpg","categories":["article"],"description":"In partnership with the U.K. National Health Services, we leveraged our Buy APIs and Catch platform to build a new portal to deliver personal protective equipment to frontline health care workers. ","publish_date":"2020-07-09 07:00:00","link":"https://tech.ebayinc.com/engineering/how-we-used-our-buy-apis-catch-platform-to-build-a-new-portal-for-the-nhs/","blog":{"id":"ebay","link":"https://tech.ebayinc.com/engineering","name":"Ebay","rssFeed":"https://tech.ebayinc.com/rss/engineering","type":"company"},"blogType":"company"}]